# Detailed per-sample scores (JSONL)
{"id": "line_1", "truth_parsed_k": 285.35, "prediction_parsed_k_with_c_conv": 284.87, "prediction_parsed_k_strict": 284.87, "absolute_error_k_vs_c_conv_pred": 0.4800000000000182, "score_vs_c_conv_pred": 57.53644489985519, "absolute_error_k_vs_strict_pred": 0.4800000000000182, "score_vs_pred_strict": 57.53644489985519, "truth_raw_assistant_text": "The value is 285.35", "prediction_raw_text": "The value is 284.87\u0e1e\u0e31\u0e01\u0e1c"}
{"id": "line_2", "truth_parsed_k": 285.52, "prediction_parsed_k_with_c_conv": 284.8, "prediction_parsed_k_strict": 284.8, "absolute_error_k_vs_c_conv_pred": 0.7199999999999704, "score_vs_c_conv_pred": 48.539496319757234, "absolute_error_k_vs_strict_pred": 0.7199999999999704, "score_vs_pred_strict": 48.539496319757234, "truth_raw_assistant_text": "The value is 285.52", "prediction_raw_text": "The value is 284.80"}
{"id": "line_3", "truth_parsed_k": 286.12, "prediction_parsed_k_with_c_conv": 285.95, "prediction_parsed_k_strict": 285.95, "absolute_error_k_vs_c_conv_pred": 0.17000000000001592, "score_vs_c_conv_pred": 76.87774456469319, "absolute_error_k_vs_strict_pred": 0.17000000000001592, "score_vs_pred_strict": 76.87774456469319, "truth_raw_assistant_text": "The value is 286.12", "prediction_raw_text": "The value is 285.95\u0e27\u0e31\u0e19\u0e17\u0e35\u0e48"}
{"id": "line_4", "truth_parsed_k": 287.15, "prediction_parsed_k_with_c_conv": 285.67, "prediction_parsed_k_strict": 285.67, "absolute_error_k_vs_c_conv_pred": 1.4799999999999613, "score_vs_c_conv_pred": 31.263881448593455, "absolute_error_k_vs_strict_pred": 1.4799999999999613, "score_vs_pred_strict": 31.263881448593455, "truth_raw_assistant_text": "The value is 287.15", "prediction_raw_text": "The value is 285.67."}
{"id": "line_5", "truth_parsed_k": 288.02, "prediction_parsed_k_with_c_conv": 285.32, "prediction_parsed_k_strict": 285.32, "absolute_error_k_vs_c_conv_pred": 2.6999999999999886, "score_vs_c_conv_pred": 16.039062921826364, "absolute_error_k_vs_strict_pred": 2.6999999999999886, "score_vs_pred_strict": 16.039062921826364, "truth_raw_assistant_text": "The value is 288.02", "prediction_raw_text": "The value is 285.32\u0e2a\u0e19\u0e31\u0e1a\u0e2a\u0e19\u0e38"}
{"id": "line_6", "truth_parsed_k": 288.62, "prediction_parsed_k_with_c_conv": 285.77, "prediction_parsed_k_strict": 285.77, "absolute_error_k_vs_c_conv_pred": 2.8500000000000227, "score_vs_c_conv_pred": 14.645911705964421, "absolute_error_k_vs_strict_pred": 2.8500000000000227, "score_vs_pred_strict": 14.645911705964421, "truth_raw_assistant_text": "The value is 288.62", "prediction_raw_text": "The value is 285.77\u0e08\u0e31\u0e07"}
{"id": "line_7", "truth_parsed_k": 288.8, "prediction_parsed_k_with_c_conv": 285.29, "prediction_parsed_k_strict": 285.29, "absolute_error_k_vs_c_conv_pred": 3.509999999999991, "score_vs_c_conv_pred": 9.250430794624586, "absolute_error_k_vs_strict_pred": 3.509999999999991, "score_vs_pred_strict": 9.250430794624586, "truth_raw_assistant_text": "The value is 288.80", "prediction_raw_text": "The value is 285.29."}
{"id": "line_8", "truth_parsed_k": 288.57, "prediction_parsed_k_with_c_conv": 285.77, "prediction_parsed_k_strict": 285.77, "absolute_error_k_vs_c_conv_pred": 2.8000000000000114, "score_vs_c_conv_pred": 15.102333663296175, "absolute_error_k_vs_strict_pred": 2.8000000000000114, "score_vs_pred_strict": 15.102333663296175, "truth_raw_assistant_text": "The value is 288.57", "prediction_raw_text": "The value is 285.77"}
{"id": "line_9", "truth_parsed_k": 288.04, "prediction_parsed_k_with_c_conv": 285.52, "prediction_parsed_k_strict": 285.52, "absolute_error_k_vs_c_conv_pred": 2.5200000000000387, "score_vs_c_conv_pred": 17.811946229475527, "absolute_error_k_vs_strict_pred": 2.5200000000000387, "score_vs_pred_strict": 17.811946229475527, "truth_raw_assistant_text": "The value is 288.04", "prediction_raw_text": "The value is 285.52\u0645\u064e"}
{"id": "line_10", "truth_parsed_k": 287.28, "prediction_parsed_k_with_c_conv": 284.93, "prediction_parsed_k_strict": 284.93, "absolute_error_k_vs_c_conv_pred": 2.349999999999966, "score_vs_c_conv_pred": 19.6008079533465, "absolute_error_k_vs_strict_pred": 2.349999999999966, "score_vs_pred_strict": 19.6008079533465, "truth_raw_assistant_text": "The value is 287.28", "prediction_raw_text": "The value is 284.93."}
{"id": "line_11", "truth_parsed_k": 286.45, "prediction_parsed_k_with_c_conv": 285.45, "prediction_parsed_k_strict": 285.45, "absolute_error_k_vs_c_conv_pred": 1.0, "score_vs_c_conv_pred": 40.83258550341813, "absolute_error_k_vs_strict_pred": 1.0, "score_vs_pred_strict": 40.83258550341813, "truth_raw_assistant_text": "The value is 286.45", "prediction_raw_text": "The value is 285.45."}
{"id": "line_12", "truth_parsed_k": 285.71, "prediction_parsed_k_with_c_conv": 284.79, "prediction_parsed_k_strict": 284.79, "absolute_error_k_vs_c_conv_pred": 0.9199999999999591, "score_vs_c_conv_pred": 42.81897889809685, "absolute_error_k_vs_strict_pred": 0.9199999999999591, "score_vs_pred_strict": 42.81897889809685, "truth_raw_assistant_text": "The value is 285.71", "prediction_raw_text": "The value is 284.79\u0e1a\u0e38\u0e0d"}
{"id": "line_13", "truth_parsed_k": 285.46, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 0.48999999999995225, "score_vs_c_conv_pred": 57.09498827854562, "absolute_error_k_vs_strict_pred": 0.48999999999995225, "score_vs_pred_strict": 57.09498827854562, "truth_raw_assistant_text": "The value is 285.46", "prediction_raw_text": "The value is 284.97 \u0e0b\u0e36\u0e48\u0e07\u0e40\u0e1b\u0e47\u0e19"}
{"id": "line_14", "truth_parsed_k": 285.5, "prediction_parsed_k_with_c_conv": 285.46, "prediction_parsed_k_strict": 285.46, "absolute_error_k_vs_c_conv_pred": 0.040000000000020464, "score_vs_c_conv_pred": 92.52386296505095, "absolute_error_k_vs_strict_pred": 0.040000000000020464, "score_vs_pred_strict": 92.52386296505095, "truth_raw_assistant_text": "The value is 285.50", "prediction_raw_text": "The value is 285.46\u0e2b\u0e23\u0e37\u0e2d\u0e40\u0e1b\u0e25\u0e48\u0e32"}
{"id": "line_15", "truth_parsed_k": 286.09, "prediction_parsed_k_with_c_conv": 285.65, "prediction_parsed_k_strict": 285.65, "absolute_error_k_vs_c_conv_pred": 0.4399999999999977, "score_vs_c_conv_pred": 59.37841068524359, "absolute_error_k_vs_strict_pred": 0.4399999999999977, "score_vs_pred_strict": 59.37841068524359, "truth_raw_assistant_text": "The value is 286.09", "prediction_raw_text": "The value is 285.65\u0e01\u0e32\u0e23\u0e40\u0e07\u0e34\u0e19"}
{"id": "line_16", "truth_parsed_k": 287.18, "prediction_parsed_k_with_c_conv": 285.91, "prediction_parsed_k_strict": 285.91, "absolute_error_k_vs_c_conv_pred": 1.2699999999999818, "score_vs_c_conv_pred": 35.04001232177425, "absolute_error_k_vs_strict_pred": 1.2699999999999818, "score_vs_pred_strict": 35.04001232177425, "truth_raw_assistant_text": "The value is 287.18", "prediction_raw_text": "The value is 285.91."}
{"id": "line_17", "truth_parsed_k": 288.03, "prediction_parsed_k_with_c_conv": 285.84, "prediction_parsed_k_strict": 285.84, "absolute_error_k_vs_c_conv_pred": 2.1899999999999977, "score_vs_c_conv_pred": 21.400442032933764, "absolute_error_k_vs_strict_pred": 2.1899999999999977, "score_vs_pred_strict": 21.400442032933764, "truth_raw_assistant_text": "The value is 288.03", "prediction_raw_text": "The value is 285.84."}
{"id": "line_18", "truth_parsed_k": 288.68, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 3.8600000000000136, "score_vs_c_conv_pred": 6.774965134999311, "absolute_error_k_vs_strict_pred": 3.8600000000000136, "score_vs_pred_strict": 6.774965134999311, "truth_raw_assistant_text": "The value is 288.68", "prediction_raw_text": "The value is 284.82\u1fbe"}
{"id": "line_19", "truth_parsed_k": 288.79, "prediction_parsed_k_with_c_conv": 285.86, "prediction_parsed_k_strict": 285.86, "absolute_error_k_vs_c_conv_pred": 2.930000000000007, "score_vs_c_conv_pred": 13.931355662636857, "absolute_error_k_vs_strict_pred": 2.930000000000007, "score_vs_pred_strict": 13.931355662636857, "truth_raw_assistant_text": "The value is 288.79", "prediction_raw_text": "The value is 285.86."}
{"id": "line_20", "truth_parsed_k": 288.5, "prediction_parsed_k_with_c_conv": 284.85, "prediction_parsed_k_strict": 284.85, "absolute_error_k_vs_c_conv_pred": 3.6499999999999773, "score_vs_c_conv_pred": 8.232777951575255, "absolute_error_k_vs_strict_pred": 3.6499999999999773, "score_vs_pred_strict": 8.232777951575255, "truth_raw_assistant_text": "The value is 288.50", "prediction_raw_text": "The value is 284.85."}
{"id": "line_21", "truth_parsed_k": 287.99, "prediction_parsed_k_with_c_conv": 285.68, "prediction_parsed_k_strict": 285.68, "absolute_error_k_vs_c_conv_pred": 2.3100000000000023, "score_vs_c_conv_pred": 20.039567163977523, "absolute_error_k_vs_strict_pred": 2.3100000000000023, "score_vs_pred_strict": 20.039567163977523, "truth_raw_assistant_text": "The value is 287.99", "prediction_raw_text": "The value is 285.68\u0e02\u0e48\u0e32\u0e27\u0e2a\u0e32\u0e23"}
{"id": "line_22", "truth_parsed_k": 287.09, "prediction_parsed_k_with_c_conv": 284.89, "prediction_parsed_k_strict": 284.89, "absolute_error_k_vs_c_conv_pred": 2.1999999999999886, "score_vs_c_conv_pred": 21.284371806647727, "absolute_error_k_vs_strict_pred": 2.1999999999999886, "score_vs_pred_strict": 21.284371806647727, "truth_raw_assistant_text": "The value is 287.09", "prediction_raw_text": "The value is 284.89\u09df"}
{"id": "line_23", "truth_parsed_k": 286.09, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 1.1199999999999477, "score_vs_c_conv_pred": 38.103340118120066, "absolute_error_k_vs_strict_pred": 1.1199999999999477, "score_vs_pred_strict": 38.103340118120066, "truth_raw_assistant_text": "The value is 286.09", "prediction_raw_text": "The value is 284.97\u0e40\u0e21\u0e37\u0e48\u0e2d\u0e27\u0e31\u0e19\u0e17\u0e35\u0e48"}
{"id": "line_24", "truth_parsed_k": 285.31, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 0.37000000000000455, "score_vs_c_conv_pred": 62.940155716763115, "absolute_error_k_vs_strict_pred": 0.37000000000000455, "score_vs_pred_strict": 62.940155716763115, "truth_raw_assistant_text": "The value is 285.31", "prediction_raw_text": "The value is 284.94."}
{"id": "line_25", "truth_parsed_k": 285.24, "prediction_parsed_k_with_c_conv": 285.46, "prediction_parsed_k_strict": 285.46, "absolute_error_k_vs_c_conv_pred": 0.21999999999997044, "score_vs_c_conv_pred": 72.66161362986657, "absolute_error_k_vs_strict_pred": 0.21999999999997044, "score_vs_pred_strict": 72.66161362986657, "truth_raw_assistant_text": "The value is 285.24", "prediction_raw_text": "The value is 285.46\u0e42\u0e17\u0e23\u0e28\u0e31"}
{"id": "line_26", "truth_parsed_k": 285.31, "prediction_parsed_k_with_c_conv": 285.37, "prediction_parsed_k_strict": 285.37, "absolute_error_k_vs_c_conv_pred": 0.060000000000002274, "score_vs_c_conv_pred": 89.44299233368689, "absolute_error_k_vs_strict_pred": 0.060000000000002274, "score_vs_pred_strict": 89.44299233368689, "truth_raw_assistant_text": "The value is 285.31", "prediction_raw_text": "The value is 285.37"}
{"id": "line_27", "truth_parsed_k": 285.9, "prediction_parsed_k_with_c_conv": 285.66, "prediction_parsed_k_strict": 285.66, "absolute_error_k_vs_c_conv_pred": 0.23999999999995225, "score_vs_c_conv_pred": 71.14412762073957, "absolute_error_k_vs_strict_pred": 0.23999999999995225, "score_vs_pred_strict": 71.14412762073957, "truth_raw_assistant_text": "The value is 285.90", "prediction_raw_text": "The value is 285.66."}
{"id": "line_28", "truth_parsed_k": 286.93, "prediction_parsed_k_with_c_conv": 286.88, "prediction_parsed_k_strict": 286.88, "absolute_error_k_vs_c_conv_pred": 0.05000000000001137, "score_vs_c_conv_pred": 90.9393912228587, "absolute_error_k_vs_strict_pred": 0.05000000000001137, "score_vs_pred_strict": 90.9393912228587, "truth_raw_assistant_text": "The value is 286.93", "prediction_raw_text": "The value is 286.88"}
{"id": "line_29", "truth_parsed_k": 288.14, "prediction_parsed_k_with_c_conv": 285.66, "prediction_parsed_k_strict": 285.66, "absolute_error_k_vs_c_conv_pred": 2.4799999999999613, "score_vs_c_conv_pred": 18.222289420108318, "absolute_error_k_vs_strict_pred": 2.4799999999999613, "score_vs_pred_strict": 18.222289420108318, "truth_raw_assistant_text": "The value is 288.14", "prediction_raw_text": "The value is 285.66"}
{"id": "line_30", "truth_parsed_k": 288.37, "prediction_parsed_k_with_c_conv": 285.7, "prediction_parsed_k_strict": 285.7, "absolute_error_k_vs_c_conv_pred": 2.670000000000016, "score_vs_c_conv_pred": 16.326555874691216, "absolute_error_k_vs_strict_pred": 2.670000000000016, "score_vs_pred_strict": 16.326555874691216, "truth_raw_assistant_text": "The value is 288.37", "prediction_raw_text": "The value is 285.70"}
{"id": "line_31", "truth_parsed_k": 288.84, "prediction_parsed_k_with_c_conv": 285.82, "prediction_parsed_k_strict": 285.82, "absolute_error_k_vs_c_conv_pred": 3.019999999999982, "score_vs_c_conv_pred": 13.149512816791553, "absolute_error_k_vs_strict_pred": 3.019999999999982, "score_vs_pred_strict": 13.149512816791553, "truth_raw_assistant_text": "The value is 288.84", "prediction_raw_text": "The value is 285.82\u0e40\u0e0a\u0e48\u0e19\u0e01\u0e31\u0e19"}
{"id": "line_32", "truth_parsed_k": 288.62, "prediction_parsed_k_with_c_conv": 285.8, "prediction_parsed_k_strict": 285.8, "absolute_error_k_vs_c_conv_pred": 2.819999999999993, "score_vs_c_conv_pred": 14.918835503106397, "absolute_error_k_vs_strict_pred": 2.819999999999993, "score_vs_pred_strict": 14.918835503106397, "truth_raw_assistant_text": "The value is 288.62", "prediction_raw_text": "The value is 285.80\u0e2a\u0e48\u0e27\u0e19\u0e25\u0e14"}
{"id": "line_33", "truth_parsed_k": 288.0, "prediction_parsed_k_with_c_conv": 284.78, "prediction_parsed_k_strict": 284.78, "absolute_error_k_vs_c_conv_pred": 3.2200000000000273, "score_vs_c_conv_pred": 11.489310663167618, "absolute_error_k_vs_strict_pred": 3.2200000000000273, "score_vs_pred_strict": 11.489310663167618, "truth_raw_assistant_text": "The value is 288.00", "prediction_raw_text": "The value is 284.78\u0e17\u0e48\u0e2d\u0e07\u0e40\u0e17\u0e35\u0e48\u0e22\u0e27"}
{"id": "line_34", "truth_parsed_k": 287.23, "prediction_parsed_k_with_c_conv": 285.8, "prediction_parsed_k_strict": 285.8, "absolute_error_k_vs_c_conv_pred": 1.4300000000000068, "score_vs_c_conv_pred": 32.11611241065043, "absolute_error_k_vs_strict_pred": 1.4300000000000068, "score_vs_pred_strict": 32.11611241065043, "truth_raw_assistant_text": "The value is 287.23", "prediction_raw_text": "The value is 285.80\u0e22\u0e48\u0e2d\u0e21"}
{"id": "line_35", "truth_parsed_k": 286.38, "prediction_parsed_k_with_c_conv": 286.35, "prediction_parsed_k_strict": 286.35, "absolute_error_k_vs_c_conv_pred": 0.029999999999972715, "score_vs_c_conv_pred": 94.20742681836049, "absolute_error_k_vs_strict_pred": 0.029999999999972715, "score_vs_pred_strict": 94.20742681836049, "truth_raw_assistant_text": "The value is 286.38", "prediction_raw_text": "The value is 286.35."}
{"id": "line_36", "truth_parsed_k": 285.37, "prediction_parsed_k_with_c_conv": 285.64, "prediction_parsed_k_strict": 285.64, "absolute_error_k_vs_c_conv_pred": 0.2699999999999818, "score_vs_c_conv_pred": 69.01710786994701, "absolute_error_k_vs_strict_pred": 0.2699999999999818, "score_vs_pred_strict": 69.01710786994701, "truth_raw_assistant_text": "The value is 285.37", "prediction_raw_text": "The value is 285.64."}
{"id": "line_37", "truth_parsed_k": 285.2, "prediction_parsed_k_with_c_conv": 286.2, "prediction_parsed_k_strict": 286.2, "absolute_error_k_vs_c_conv_pred": 1.0, "score_vs_c_conv_pred": 40.83258550341813, "absolute_error_k_vs_strict_pred": 1.0, "score_vs_pred_strict": 40.83258550341813, "truth_raw_assistant_text": "The value is 285.20", "prediction_raw_text": "The value is 286.20\u0e40\u0e0a\u0e37\u0e48\u0e2d\u0e27\u0e48\u0e32"}
{"id": "line_38", "truth_parsed_k": 285.29, "prediction_parsed_k_with_c_conv": 286.67, "prediction_parsed_k_strict": 286.67, "absolute_error_k_vs_c_conv_pred": 1.3799999999999955, "score_vs_c_conv_pred": 32.996198937673746, "absolute_error_k_vs_strict_pred": 1.3799999999999955, "score_vs_pred_strict": 32.996198937673746, "truth_raw_assistant_text": "The value is 285.29", "prediction_raw_text": "The value is 286.67\u0e28\u0e39\u0e19\u0e22\u0e4c"}
{"id": "line_39", "truth_parsed_k": 286.15, "prediction_parsed_k_with_c_conv": 285.66, "prediction_parsed_k_strict": 285.66, "absolute_error_k_vs_c_conv_pred": 0.48999999999995225, "score_vs_c_conv_pred": 57.09498827854562, "absolute_error_k_vs_strict_pred": 0.48999999999995225, "score_vs_pred_strict": 57.09498827854562, "truth_raw_assistant_text": "The value is 286.15", "prediction_raw_text": "The value is 285.66."}
{"id": "line_40", "truth_parsed_k": 287.22, "prediction_parsed_k_with_c_conv": 285.99, "prediction_parsed_k_strict": 285.99, "absolute_error_k_vs_c_conv_pred": 1.2300000000000182, "score_vs_c_conv_pred": 35.82343487071261, "absolute_error_k_vs_strict_pred": 1.2300000000000182, "score_vs_pred_strict": 35.82343487071261, "truth_raw_assistant_text": "The value is 287.22", "prediction_raw_text": "The value is 285.99 \u0e0b\u0e36\u0e48\u0e07"}
{"id": "line_41", "truth_parsed_k": 288.09, "prediction_parsed_k_with_c_conv": 284.78, "prediction_parsed_k_strict": 284.78, "absolute_error_k_vs_c_conv_pred": 3.3100000000000023, "score_vs_c_conv_pred": 10.774359354368157, "absolute_error_k_vs_strict_pred": 3.3100000000000023, "score_vs_pred_strict": 10.774359354368157, "truth_raw_assistant_text": "The value is 288.09", "prediction_raw_text": "The value is 284.78"}
{"id": "line_42", "truth_parsed_k": 288.64, "prediction_parsed_k_with_c_conv": 284.93, "prediction_parsed_k_strict": 284.93, "absolute_error_k_vs_c_conv_pred": 3.7099999999999795, "score_vs_c_conv_pred": 7.808144961155694, "absolute_error_k_vs_strict_pred": 3.7099999999999795, "score_vs_pred_strict": 7.808144961155694, "truth_raw_assistant_text": "The value is 288.64", "prediction_raw_text": "The value is 284.93\uf97a"}
{"id": "line_43", "truth_parsed_k": 288.84, "prediction_parsed_k_with_c_conv": 285.5, "prediction_parsed_k_strict": 285.5, "absolute_error_k_vs_c_conv_pred": 3.339999999999975, "score_vs_c_conv_pred": 10.540199176206787, "absolute_error_k_vs_strict_pred": 3.339999999999975, "score_vs_pred_strict": 10.540199176206787, "truth_raw_assistant_text": "The value is 288.84", "prediction_raw_text": "The value is 285.50\u0e04\u0e27\u0e1a\u0e04\u0e38\u0e21"}
{"id": "line_44", "truth_parsed_k": 288.67, "prediction_parsed_k_with_c_conv": 285.74, "prediction_parsed_k_strict": 285.74, "absolute_error_k_vs_c_conv_pred": 2.930000000000007, "score_vs_c_conv_pred": 13.931355662636857, "absolute_error_k_vs_strict_pred": 2.930000000000007, "score_vs_pred_strict": 13.931355662636857, "truth_raw_assistant_text": "The value is 288.67", "prediction_raw_text": "The value is 285.74."}
{"id": "line_45", "truth_parsed_k": 288.16, "prediction_parsed_k_with_c_conv": 284.91, "prediction_parsed_k_strict": 284.91, "absolute_error_k_vs_c_conv_pred": 3.25, "score_vs_c_conv_pred": 11.248878255127204, "absolute_error_k_vs_strict_pred": 3.25, "score_vs_pred_strict": 11.248878255127204, "truth_raw_assistant_text": "The value is 288.16", "prediction_raw_text": "The value is 284.91"}
{"id": "line_46", "truth_parsed_k": 287.29, "prediction_parsed_k_with_c_conv": 284.93, "prediction_parsed_k_strict": 284.93, "absolute_error_k_vs_c_conv_pred": 2.3600000000000136, "score_vs_c_conv_pred": 19.492226149747992, "absolute_error_k_vs_strict_pred": 2.3600000000000136, "score_vs_pred_strict": 19.492226149747992, "truth_raw_assistant_text": "The value is 287.29", "prediction_raw_text": "The value is 284.93\u05d5\ufffd"}
{"id": "line_47", "truth_parsed_k": 286.35, "prediction_parsed_k_with_c_conv": 284.89, "prediction_parsed_k_strict": 284.89, "absolute_error_k_vs_c_conv_pred": 1.4600000000000364, "score_vs_c_conv_pred": 31.60154419021939, "absolute_error_k_vs_strict_pred": 1.4600000000000364, "score_vs_pred_strict": 31.60154419021939, "truth_raw_assistant_text": "The value is 286.35", "prediction_raw_text": "The value is 284.89"}
{"id": "line_48", "truth_parsed_k": 285.76, "prediction_parsed_k_with_c_conv": 284.81, "prediction_parsed_k_strict": 284.81, "absolute_error_k_vs_c_conv_pred": 0.9499999999999886, "score_vs_c_conv_pred": 42.056807714813495, "absolute_error_k_vs_strict_pred": 0.9499999999999886, "score_vs_pred_strict": 42.056807714813495, "truth_raw_assistant_text": "The value is 285.76", "prediction_raw_text": "The value is 284.81\u0e19\u0e34\u0e14"}
{"id": "line_49", "truth_parsed_k": 285.38, "prediction_parsed_k_with_c_conv": 285.77, "prediction_parsed_k_strict": 285.77, "absolute_error_k_vs_c_conv_pred": 0.38999999999998636, "score_vs_c_conv_pred": 61.873550911913796, "absolute_error_k_vs_strict_pred": 0.38999999999998636, "score_vs_pred_strict": 61.873550911913796, "truth_raw_assistant_text": "The value is 285.38", "prediction_raw_text": "The value is 285.77\u0e15\u0e49\u0e2d\u0e19"}
{"id": "line_50", "truth_parsed_k": 285.35, "prediction_parsed_k_with_c_conv": 285.77, "prediction_parsed_k_strict": 285.77, "absolute_error_k_vs_c_conv_pred": 0.4199999999999591, "score_vs_c_conv_pred": 60.34890313391317, "absolute_error_k_vs_strict_pred": 0.4199999999999591, "score_vs_pred_strict": 60.34890313391317, "truth_raw_assistant_text": "The value is 285.35", "prediction_raw_text": "The value is 285.77\u0e2a\u0e48\u0e27\u0e19\u0e15\u0e31\u0e27"}
{"id": "line_51", "truth_parsed_k": 286.1, "prediction_parsed_k_with_c_conv": 284.91, "prediction_parsed_k_strict": 284.91, "absolute_error_k_vs_c_conv_pred": 1.1899999999999977, "score_vs_c_conv_pred": 36.630334216240726, "absolute_error_k_vs_strict_pred": 1.1899999999999977, "score_vs_pred_strict": 36.630334216240726, "truth_raw_assistant_text": "The value is 286.10", "prediction_raw_text": "The value is 284.91\u0e08\u0e49\u0e32\u0e07"}
{"id": "line_52", "truth_parsed_k": 287.22, "prediction_parsed_k_with_c_conv": 285.47, "prediction_parsed_k_strict": 285.47, "absolute_error_k_vs_c_conv_pred": 1.75, "score_vs_c_conv_pred": 27.07694498829184, "absolute_error_k_vs_strict_pred": 1.75, "score_vs_pred_strict": 27.07694498829184, "truth_raw_assistant_text": "The value is 287.22", "prediction_raw_text": "The value is 285.47\u0e40\u0e14\u0e37\u0e2d\u0e19"}
{"id": "line_53", "truth_parsed_k": 287.98, "prediction_parsed_k_with_c_conv": 285.37, "prediction_parsed_k_strict": 285.37, "absolute_error_k_vs_c_conv_pred": 2.6100000000000136, "score_vs_c_conv_pred": 16.91091697886058, "absolute_error_k_vs_strict_pred": 2.6100000000000136, "score_vs_pred_strict": 16.91091697886058, "truth_raw_assistant_text": "The value is 287.98", "prediction_raw_text": "The value is 285.37\u0e40\u0e17\u0e48\u0e32\u0e44\u0e2b\u0e23\u0e48"}
{"id": "line_54", "truth_parsed_k": 288.53, "prediction_parsed_k_with_c_conv": 285.75, "prediction_parsed_k_strict": 285.75, "absolute_error_k_vs_c_conv_pred": 2.7799999999999727, "score_vs_c_conv_pred": 15.287090825853355, "absolute_error_k_vs_strict_pred": 2.7799999999999727, "score_vs_pred_strict": 15.287090825853355, "truth_raw_assistant_text": "The value is 288.53", "prediction_raw_text": "The value is 285.75"}
{"id": "line_55", "truth_parsed_k": 288.85, "prediction_parsed_k_with_c_conv": 285.82, "prediction_parsed_k_strict": 285.82, "absolute_error_k_vs_c_conv_pred": 3.0300000000000296, "score_vs_c_conv_pred": 13.064026234499227, "absolute_error_k_vs_strict_pred": 3.0300000000000296, "score_vs_pred_strict": 13.064026234499227, "truth_raw_assistant_text": "The value is 288.85", "prediction_raw_text": "The value is 285.82\u0e41\u0e04\u0e48"}
{"id": "line_56", "truth_parsed_k": 288.7, "prediction_parsed_k_with_c_conv": 285.46, "prediction_parsed_k_strict": 285.46, "absolute_error_k_vs_c_conv_pred": 3.240000000000009, "score_vs_c_conv_pred": 11.328784102502688, "absolute_error_k_vs_strict_pred": 3.240000000000009, "score_vs_pred_strict": 11.328784102502688, "truth_raw_assistant_text": "The value is 288.70", "prediction_raw_text": "The value is 285.46\u0e17\u0e35\u0e21\u0e07\u0e32\u0e19"}
{"id": "line_57", "truth_parsed_k": 288.13, "prediction_parsed_k_with_c_conv": 285.71, "prediction_parsed_k_strict": 285.71, "absolute_error_k_vs_c_conv_pred": 2.420000000000016, "score_vs_c_conv_pred": 18.849772199067893, "absolute_error_k_vs_strict_pred": 2.420000000000016, "score_vs_pred_strict": 18.849772199067893, "truth_raw_assistant_text": "The value is 288.13", "prediction_raw_text": "The value is 285.71\u0e17\u0e35\u0e48\u0e19\u0e48\u0e32"}
{"id": "line_58", "truth_parsed_k": 287.18, "prediction_parsed_k_with_c_conv": 285.56, "prediction_parsed_k_strict": 285.56, "absolute_error_k_vs_c_conv_pred": 1.6200000000000045, "score_vs_c_conv_pred": 29.01185031602793, "absolute_error_k_vs_strict_pred": 1.6200000000000045, "score_vs_pred_strict": 29.01185031602793, "truth_raw_assistant_text": "The value is 287.18", "prediction_raw_text": "The value is 285.56"}
{"id": "line_59", "truth_parsed_k": 286.22, "prediction_parsed_k_with_c_conv": 286.7, "prediction_parsed_k_strict": 286.7, "absolute_error_k_vs_c_conv_pred": 0.47999999999996135, "score_vs_c_conv_pred": 57.53644489985772, "absolute_error_k_vs_strict_pred": 0.47999999999996135, "score_vs_pred_strict": 57.53644489985772, "truth_raw_assistant_text": "The value is 286.22", "prediction_raw_text": "The value is 286.70\u0e14\u0e49\u0e27\u0e22\u0e04\u0e27\u0e32\u0e21"}
{"id": "line_60", "truth_parsed_k": 285.37, "prediction_parsed_k_with_c_conv": 285.64, "prediction_parsed_k_strict": 285.64, "absolute_error_k_vs_c_conv_pred": 0.2699999999999818, "score_vs_c_conv_pred": 69.01710786994701, "absolute_error_k_vs_strict_pred": 0.2699999999999818, "score_vs_pred_strict": 69.01710786994701, "truth_raw_assistant_text": "The value is 285.37", "prediction_raw_text": "The value is 285.64\u0e40\u0e2d\u0e32\u0e44\u0e27\u0e49"}
{"id": "line_61", "truth_parsed_k": 285.15, "prediction_parsed_k_with_c_conv": 285.4, "prediction_parsed_k_strict": 285.4, "absolute_error_k_vs_c_conv_pred": 0.25, "score_vs_c_conv_pred": 70.41629275170907, "absolute_error_k_vs_strict_pred": 0.25, "score_vs_pred_strict": 70.41629275170907, "truth_raw_assistant_text": "The value is 285.15", "prediction_raw_text": "The value is 285.40\u0e08\u0e23\u0e34\u0e07"}
{"id": "line_62", "truth_parsed_k": 285.61, "prediction_parsed_k_with_c_conv": 285.77, "prediction_parsed_k_strict": 285.77, "absolute_error_k_vs_c_conv_pred": 0.15999999999996817, "score_vs_c_conv_pred": 77.8063969632971, "absolute_error_k_vs_strict_pred": 0.15999999999996817, "score_vs_pred_strict": 77.8063969632971, "truth_raw_assistant_text": "The value is 285.61", "prediction_raw_text": "The value is 285.77\u0e40\u0e17\u0e48\u0e32\u0e44\u0e2b\u0e23\u0e48"}
{"id": "line_63", "truth_parsed_k": 286.03, "prediction_parsed_k_with_c_conv": 284.5, "prediction_parsed_k_strict": 284.5, "absolute_error_k_vs_c_conv_pred": 1.5299999999999727, "score_vs_c_conv_pred": 30.437796706032373, "absolute_error_k_vs_strict_pred": 1.5299999999999727, "score_vs_pred_strict": 30.437796706032373, "truth_raw_assistant_text": "The value is 286.03", "prediction_raw_text": "The value is 284.50"}
{"id": "line_64", "truth_parsed_k": 286.97, "prediction_parsed_k_with_c_conv": 285.7, "prediction_parsed_k_strict": 285.7, "absolute_error_k_vs_c_conv_pred": 1.2700000000000387, "score_vs_c_conv_pred": 35.04001232177314, "absolute_error_k_vs_strict_pred": 1.2700000000000387, "score_vs_pred_strict": 35.04001232177314, "truth_raw_assistant_text": "The value is 286.97", "prediction_raw_text": "The value is 285.70\u0e44\u0e21\u0e48\u0e27"}
{"id": "line_65", "truth_parsed_k": 287.98, "prediction_parsed_k_with_c_conv": 285.52, "prediction_parsed_k_strict": 285.52, "absolute_error_k_vs_c_conv_pred": 2.4600000000000364, "score_vs_c_conv_pred": 18.429829952686394, "absolute_error_k_vs_strict_pred": 2.4600000000000364, "score_vs_pred_strict": 18.429829952686394, "truth_raw_assistant_text": "The value is 287.98", "prediction_raw_text": "The value is 285.52\uf9c9"}
{"id": "line_66", "truth_parsed_k": 288.54, "prediction_parsed_k_with_c_conv": 285.84, "prediction_parsed_k_strict": 285.84, "absolute_error_k_vs_c_conv_pred": 2.7000000000000455, "score_vs_c_conv_pred": 16.03906292182582, "absolute_error_k_vs_strict_pred": 2.7000000000000455, "score_vs_pred_strict": 16.03906292182582, "truth_raw_assistant_text": "The value is 288.54", "prediction_raw_text": "The value is 285.84."}
{"id": "line_67", "truth_parsed_k": 288.79, "prediction_parsed_k_with_c_conv": 286.99, "prediction_parsed_k_strict": 286.99, "absolute_error_k_vs_c_conv_pred": 1.8000000000000114, "score_vs_c_conv_pred": 26.36826590937107, "absolute_error_k_vs_strict_pred": 1.8000000000000114, "score_vs_pred_strict": 26.36826590937107, "truth_raw_assistant_text": "The value is 288.79", "prediction_raw_text": "The value is 286.99"}
{"id": "line_68", "truth_parsed_k": 288.6, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 3.7800000000000296, "score_vs_c_conv_pred": 7.321057376815487, "absolute_error_k_vs_strict_pred": 3.7800000000000296, "score_vs_pred_strict": 7.321057376815487, "truth_raw_assistant_text": "The value is 288.60", "prediction_raw_text": "The value is 284.82\u0e40\u0e1a\u0e35\u0e22"}
{"id": "line_69", "truth_parsed_k": 287.95, "prediction_parsed_k_with_c_conv": 284.93, "prediction_parsed_k_strict": 284.93, "absolute_error_k_vs_c_conv_pred": 3.019999999999982, "score_vs_c_conv_pred": 13.149512816791553, "absolute_error_k_vs_strict_pred": 3.019999999999982, "score_vs_pred_strict": 13.149512816791553, "truth_raw_assistant_text": "The value is 287.95", "prediction_raw_text": "The value is 284.93\u0e1a\u0e2d\u0e23\u0e4c"}
{"id": "line_70", "truth_parsed_k": 287.1, "prediction_parsed_k_with_c_conv": 284.96, "prediction_parsed_k_strict": 284.96, "absolute_error_k_vs_c_conv_pred": 2.140000000000043, "score_vs_c_conv_pred": 21.988418466700878, "absolute_error_k_vs_strict_pred": 2.140000000000043, "score_vs_pred_strict": 21.988418466700878, "truth_raw_assistant_text": "The value is 287.10", "prediction_raw_text": "The value is 284.96\u0e01\u0e23\u0e30\u0e15\u0e38\u0e49\u0e19"}
{"id": "line_71", "truth_parsed_k": 286.1, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 1.2800000000000296, "score_vs_c_conv_pred": 34.84766685535777, "absolute_error_k_vs_strict_pred": 1.2800000000000296, "score_vs_pred_strict": 34.84766685535777, "truth_raw_assistant_text": "The value is 286.10", "prediction_raw_text": "The value is 284.82"}
{"id": "line_72", "truth_parsed_k": 285.66, "prediction_parsed_k_with_c_conv": 285.9, "prediction_parsed_k_strict": 285.9, "absolute_error_k_vs_c_conv_pred": 0.23999999999995225, "score_vs_c_conv_pred": 71.14412762073957, "absolute_error_k_vs_strict_pred": 0.23999999999995225, "score_vs_pred_strict": 71.14412762073957, "truth_raw_assistant_text": "The value is 285.66", "prediction_raw_text": "The value is 285.90 \u0e1e\u0e23\u0e49\u0e2d\u0e21"}
{"id": "line_73", "truth_parsed_k": 285.25, "prediction_parsed_k_with_c_conv": 285.71, "prediction_parsed_k_strict": 285.71, "absolute_error_k_vs_c_conv_pred": 0.45999999999997954, "score_vs_c_conv_pred": 58.44168142671445, "absolute_error_k_vs_strict_pred": 0.45999999999997954, "score_vs_pred_strict": 58.44168142671445, "truth_raw_assistant_text": "The value is 285.25", "prediction_raw_text": "The value is 285.71\u0e1b\u0e23\u0e2a\u0e34"}
{"id": "line_74", "truth_parsed_k": 285.4, "prediction_parsed_k_with_c_conv": 285.88, "prediction_parsed_k_strict": 285.88, "absolute_error_k_vs_c_conv_pred": 0.4800000000000182, "score_vs_c_conv_pred": 57.53644489985519, "absolute_error_k_vs_strict_pred": 0.4800000000000182, "score_vs_pred_strict": 57.53644489985519, "truth_raw_assistant_text": "The value is 285.40", "prediction_raw_text": "The value is 285.88"}
{"id": "line_75", "truth_parsed_k": 286.08, "prediction_parsed_k_with_c_conv": 285.82, "prediction_parsed_k_strict": 285.82, "absolute_error_k_vs_c_conv_pred": 0.2599999999999909, "score_vs_c_conv_pred": 69.70761367278908, "absolute_error_k_vs_strict_pred": 0.2599999999999909, "score_vs_pred_strict": 69.70761367278908, "truth_raw_assistant_text": "The value is 286.08", "prediction_raw_text": "The value is 285.82\u0e0a\u0e34\u0e49\u0e19"}
{"id": "line_76", "truth_parsed_k": 286.99, "prediction_parsed_k_with_c_conv": 285.71, "prediction_parsed_k_strict": 285.71, "absolute_error_k_vs_c_conv_pred": 1.2800000000000296, "score_vs_c_conv_pred": 34.84766685535777, "absolute_error_k_vs_strict_pred": 1.2800000000000296, "score_vs_pred_strict": 34.84766685535777, "truth_raw_assistant_text": "The value is 286.99", "prediction_raw_text": "The value is 285.71\u0e1e\u0e35"}
{"id": "line_77", "truth_parsed_k": 287.99, "prediction_parsed_k_with_c_conv": 284.81, "prediction_parsed_k_strict": 284.81, "absolute_error_k_vs_c_conv_pred": 3.180000000000007, "score_vs_c_conv_pred": 11.813263452031219, "absolute_error_k_vs_strict_pred": 3.180000000000007, "score_vs_pred_strict": 11.813263452031219, "truth_raw_assistant_text": "The value is 287.99", "prediction_raw_text": "The value is 284.81\uf9db"}
{"id": "line_78", "truth_parsed_k": 288.6, "prediction_parsed_k_with_c_conv": 285.95, "prediction_parsed_k_strict": 285.95, "absolute_error_k_vs_c_conv_pred": 2.650000000000034, "score_vs_c_conv_pred": 16.519937321978738, "absolute_error_k_vs_strict_pred": 2.650000000000034, "score_vs_pred_strict": 16.519937321978738, "truth_raw_assistant_text": "The value is 288.60", "prediction_raw_text": "The value is 285.95\u0e23\u0e31\u0e1a\u0e1c\u0e34\u0e14\u0e0a\u0e2d\u0e1a"}
{"id": "line_79", "truth_parsed_k": 288.78, "prediction_parsed_k_with_c_conv": 284.57, "prediction_parsed_k_strict": 284.57, "absolute_error_k_vs_c_conv_pred": 4.2099999999999795, "score_vs_c_conv_pred": 4.508029244751999, "absolute_error_k_vs_strict_pred": 4.2099999999999795, "score_vs_pred_strict": 4.508029244751999, "truth_raw_assistant_text": "The value is 288.78", "prediction_raw_text": "The value is 284.57\u0e1b\u0e49\u0e2d\u0e07\u0e01\u0e31\u0e19"}
{"id": "line_80", "truth_parsed_k": 288.58, "prediction_parsed_k_with_c_conv": 285.48, "prediction_parsed_k_strict": 285.48, "absolute_error_k_vs_c_conv_pred": 3.099999999999966, "score_vs_c_conv_pred": 12.473100466522569, "absolute_error_k_vs_strict_pred": 3.099999999999966, "score_vs_pred_strict": 12.473100466522569, "truth_raw_assistant_text": "The value is 288.58", "prediction_raw_text": "The value is 285.48\uf9bf"}
{"id": "line_81", "truth_parsed_k": 288.08, "prediction_parsed_k_with_c_conv": 284.79, "prediction_parsed_k_strict": 284.79, "absolute_error_k_vs_c_conv_pred": 3.2899999999999636, "score_vs_c_conv_pred": 10.93160506137697, "absolute_error_k_vs_strict_pred": 3.2899999999999636, "score_vs_pred_strict": 10.93160506137697, "truth_raw_assistant_text": "The value is 288.08", "prediction_raw_text": "The value is 284.79."}
{"id": "line_82", "truth_parsed_k": 287.18, "prediction_parsed_k_with_c_conv": 285.45, "prediction_parsed_k_strict": 285.45, "absolute_error_k_vs_c_conv_pred": 1.7300000000000182, "score_vs_c_conv_pred": 27.365722563368855, "absolute_error_k_vs_strict_pred": 1.7300000000000182, "score_vs_pred_strict": 27.365722563368855, "truth_raw_assistant_text": "The value is 287.18", "prediction_raw_text": "The value is 285.45\u0e40\u0e04\u0e25\u0e37\u0e48\u0e2d\u0e19\u0e44\u0e2b\u0e27"}
{"id": "line_83", "truth_parsed_k": 286.12, "prediction_parsed_k_with_c_conv": 284.88, "prediction_parsed_k_strict": 284.88, "absolute_error_k_vs_c_conv_pred": 1.240000000000009, "score_vs_c_conv_pred": 35.625432134447486, "absolute_error_k_vs_strict_pred": 1.240000000000009, "score_vs_pred_strict": 35.625432134447486, "truth_raw_assistant_text": "The value is 286.12", "prediction_raw_text": "The value is 284.88"}
{"id": "line_84", "truth_parsed_k": 285.71, "prediction_parsed_k_with_c_conv": 284.93, "prediction_parsed_k_strict": 284.93, "absolute_error_k_vs_c_conv_pred": 0.7799999999999727, "score_vs_c_conv_pred": 46.69226365238002, "absolute_error_k_vs_strict_pred": 0.7799999999999727, "score_vs_pred_strict": 46.69226365238002, "truth_raw_assistant_text": "The value is 285.71", "prediction_raw_text": "The value is 284.93\uf933"}
{"id": "line_85", "truth_parsed_k": 285.36, "prediction_parsed_k_with_c_conv": 286.96, "prediction_parsed_k_strict": 286.96, "absolute_error_k_vs_c_conv_pred": 1.599999999999966, "score_vs_c_conv_pred": 29.322265866447573, "absolute_error_k_vs_strict_pred": 1.599999999999966, "score_vs_pred_strict": 29.322265866447573, "truth_raw_assistant_text": "The value is 285.36", "prediction_raw_text": "The value is 286.96\u0e40\u0e23\u0e35\u0e22\u0e19\u0e23\u0e39\u0e49"}
{"id": "line_86", "truth_parsed_k": 285.63, "prediction_parsed_k_with_c_conv": 286.5, "prediction_parsed_k_strict": 286.5, "absolute_error_k_vs_c_conv_pred": 0.8700000000000045, "score_vs_c_conv_pred": 44.13925561171903, "absolute_error_k_vs_strict_pred": 0.8700000000000045, "score_vs_pred_strict": 44.13925561171903, "truth_raw_assistant_text": "The value is 285.63", "prediction_raw_text": "The value is 286.50\u0e23\u0e16\u0e22\u0e19\u0e15\u0e4c"}
{"id": "line_87", "truth_parsed_k": 286.05, "prediction_parsed_k_with_c_conv": 284.91, "prediction_parsed_k_strict": 284.91, "absolute_error_k_vs_c_conv_pred": 1.1399999999999864, "score_vs_c_conv_pred": 37.674195564666256, "absolute_error_k_vs_strict_pred": 1.1399999999999864, "score_vs_pred_strict": 37.674195564666256, "truth_raw_assistant_text": "The value is 286.05", "prediction_raw_text": "The value is 284.91"}
{"id": "line_88", "truth_parsed_k": 287.06, "prediction_parsed_k_with_c_conv": 285.38, "prediction_parsed_k_strict": 285.38, "absolute_error_k_vs_c_conv_pred": 1.6800000000000068, "score_vs_c_conv_pred": 28.1015128963355, "absolute_error_k_vs_strict_pred": 1.6800000000000068, "score_vs_pred_strict": 28.1015128963355, "truth_raw_assistant_text": "The value is 287.06", "prediction_raw_text": "The value is 285.38."}
{"id": "line_89", "truth_parsed_k": 288.14, "prediction_parsed_k_with_c_conv": 285.56, "prediction_parsed_k_strict": 285.56, "absolute_error_k_vs_c_conv_pred": 2.579999999999984, "score_vs_c_conv_pred": 17.207922755977467, "absolute_error_k_vs_strict_pred": 2.579999999999984, "score_vs_pred_strict": 17.207922755977467, "truth_raw_assistant_text": "The value is 288.14", "prediction_raw_text": "The value is 285.56"}
{"id": "line_90", "truth_parsed_k": 288.57, "prediction_parsed_k_with_c_conv": 285.95, "prediction_parsed_k_strict": 285.95, "absolute_error_k_vs_c_conv_pred": 2.6200000000000045, "score_vs_c_conv_pred": 16.812638582158147, "absolute_error_k_vs_strict_pred": 2.6200000000000045, "score_vs_pred_strict": 16.812638582158147, "truth_raw_assistant_text": "The value is 288.57", "prediction_raw_text": "The value is 285.95."}
{"id": "line_91", "truth_parsed_k": 288.81, "prediction_parsed_k_with_c_conv": 285.56, "prediction_parsed_k_strict": 285.56, "absolute_error_k_vs_c_conv_pred": 3.25, "score_vs_c_conv_pred": 11.248878255127204, "absolute_error_k_vs_strict_pred": 3.25, "score_vs_pred_strict": 11.248878255127204, "truth_raw_assistant_text": "The value is 288.81", "prediction_raw_text": "The value is 285.56"}
{"id": "line_92", "truth_parsed_k": 288.71, "prediction_parsed_k_with_c_conv": 285.72, "prediction_parsed_k_strict": 285.72, "absolute_error_k_vs_c_conv_pred": 2.9899999999999523, "score_vs_c_conv_pred": 13.407613043226718, "absolute_error_k_vs_strict_pred": 2.9899999999999523, "score_vs_pred_strict": 13.407613043226718, "truth_raw_assistant_text": "The value is 288.71", "prediction_raw_text": "The value is 285.72"}
{"id": "line_93", "truth_parsed_k": 288.16, "prediction_parsed_k_with_c_conv": 284.81, "prediction_parsed_k_strict": 284.81, "absolute_error_k_vs_c_conv_pred": 3.3500000000000227, "score_vs_c_conv_pred": 10.462596094237774, "absolute_error_k_vs_strict_pred": 3.3500000000000227, "score_vs_pred_strict": 10.462596094237774, "truth_raw_assistant_text": "The value is 288.16", "prediction_raw_text": "The value is 284.81\u2329"}
{"id": "line_94", "truth_parsed_k": 287.31, "prediction_parsed_k_with_c_conv": 285.4, "prediction_parsed_k_strict": 285.4, "absolute_error_k_vs_c_conv_pred": 1.910000000000025, "score_vs_c_conv_pred": 24.87186702019766, "absolute_error_k_vs_strict_pred": 1.910000000000025, "score_vs_pred_strict": 24.87186702019766, "truth_raw_assistant_text": "The value is 287.31", "prediction_raw_text": "The value is 285.40\u0e40\u0e1b\u0e34\u0e14\u0e15\u0e31\u0e27"}
{"id": "line_95", "truth_parsed_k": 286.34, "prediction_parsed_k_with_c_conv": 285.94, "prediction_parsed_k_strict": 285.94, "absolute_error_k_vs_c_conv_pred": 0.39999999999997726, "score_vs_c_conv_pred": 61.355683974570695, "absolute_error_k_vs_strict_pred": 0.39999999999997726, "score_vs_pred_strict": 61.355683974570695, "truth_raw_assistant_text": "The value is 286.34", "prediction_raw_text": "The value is 285.94"}
{"id": "line_96", "truth_parsed_k": 285.55, "prediction_parsed_k_with_c_conv": 286.81, "prediction_parsed_k_strict": 286.81, "absolute_error_k_vs_c_conv_pred": 1.259999999999991, "score_vs_c_conv_pred": 35.233741580417814, "absolute_error_k_vs_strict_pred": 1.259999999999991, "score_vs_pred_strict": 35.233741580417814, "truth_raw_assistant_text": "The value is 285.55", "prediction_raw_text": "The value is 286.81\uf913"}
{"id": "line_97", "truth_parsed_k": 285.25, "prediction_parsed_k_with_c_conv": 284.98, "prediction_parsed_k_strict": 284.98, "absolute_error_k_vs_c_conv_pred": 0.2699999999999818, "score_vs_c_conv_pred": 69.01710786994701, "absolute_error_k_vs_strict_pred": 0.2699999999999818, "score_vs_pred_strict": 69.01710786994701, "truth_raw_assistant_text": "The value is 285.25", "prediction_raw_text": "The value is 284.98\u0e01\u0e33\u0e01\u0e31\u0e1a"}
{"id": "line_98", "truth_parsed_k": 285.54, "prediction_parsed_k_with_c_conv": 284.87, "prediction_parsed_k_strict": 284.87, "absolute_error_k_vs_c_conv_pred": 0.6700000000000159, "score_vs_c_conv_pred": 50.18197185563442, "absolute_error_k_vs_strict_pred": 0.6700000000000159, "score_vs_pred_strict": 50.18197185563442, "truth_raw_assistant_text": "The value is 285.54", "prediction_raw_text": "The value is 284.87\u0e2a\u0e31\u0e07\u0e04\u0e21"}
{"id": "line_99", "truth_parsed_k": 286.2, "prediction_parsed_k_with_c_conv": 285.69, "prediction_parsed_k_strict": 285.69, "absolute_error_k_vs_c_conv_pred": 0.5099999999999909, "score_vs_c_conv_pred": 56.2332107093644, "absolute_error_k_vs_strict_pred": 0.5099999999999909, "score_vs_pred_strict": 56.2332107093644, "truth_raw_assistant_text": "The value is 286.20", "prediction_raw_text": "The value is 285.69\u0e41\u0e01\u0e49\u0e1b\u0e31\u0e0d\u0e2b\u0e32"}
{"id": "line_100", "truth_parsed_k": 287.19, "prediction_parsed_k_with_c_conv": 286.7, "prediction_parsed_k_strict": 286.7, "absolute_error_k_vs_c_conv_pred": 0.4900000000000091, "score_vs_c_conv_pred": 57.09498827854313, "absolute_error_k_vs_strict_pred": 0.4900000000000091, "score_vs_pred_strict": 57.09498827854313, "truth_raw_assistant_text": "The value is 287.19", "prediction_raw_text": "The value is 286.70."}
{"id": "line_101", "truth_parsed_k": 288.09, "prediction_parsed_k_with_c_conv": 285.97, "prediction_parsed_k_strict": 285.97, "absolute_error_k_vs_c_conv_pred": 2.1199999999999477, "score_vs_c_conv_pred": 22.227251525047663, "absolute_error_k_vs_strict_pred": 2.1199999999999477, "score_vs_pred_strict": 22.227251525047663, "truth_raw_assistant_text": "The value is 288.09", "prediction_raw_text": "The value is 285.97\u0e15\u0e35"}
{"id": "line_102", "truth_parsed_k": 288.52, "prediction_parsed_k_with_c_conv": 285.76, "prediction_parsed_k_strict": 285.76, "absolute_error_k_vs_c_conv_pred": 2.759999999999991, "score_vs_c_conv_pred": 15.473124386525205, "absolute_error_k_vs_strict_pred": 2.759999999999991, "score_vs_pred_strict": 15.473124386525205, "truth_raw_assistant_text": "The value is 288.52", "prediction_raw_text": "The value is 285.76."}
{"id": "line_103", "truth_parsed_k": 288.91, "prediction_parsed_k_with_c_conv": 285.82, "prediction_parsed_k_strict": 285.82, "absolute_error_k_vs_c_conv_pred": 3.090000000000032, "score_vs_c_conv_pred": 12.556728641830572, "absolute_error_k_vs_strict_pred": 3.090000000000032, "score_vs_pred_strict": 12.556728641830572, "truth_raw_assistant_text": "The value is 288.91", "prediction_raw_text": "The value is 285.82\u0e40\u0e08\u0e23\u0e34"}
{"id": "line_104", "truth_parsed_k": 288.62, "prediction_parsed_k_with_c_conv": 284.93, "prediction_parsed_k_strict": 284.93, "absolute_error_k_vs_c_conv_pred": 3.6899999999999977, "score_vs_c_conv_pred": 7.9489465933544, "absolute_error_k_vs_strict_pred": 3.6899999999999977, "score_vs_pred_strict": 7.9489465933544, "truth_raw_assistant_text": "The value is 288.62", "prediction_raw_text": "The value is 284.93\u0e17\u0e35\u0e48\u0e1c\u0e48\u0e32\u0e19"}
{"id": "line_105", "truth_parsed_k": 288.14, "prediction_parsed_k_with_c_conv": 286.8, "prediction_parsed_k_strict": 286.8, "absolute_error_k_vs_c_conv_pred": 1.339999999999975, "score_vs_c_conv_pred": 33.72158288735778, "absolute_error_k_vs_strict_pred": 1.339999999999975, "score_vs_pred_strict": 33.72158288735778, "truth_raw_assistant_text": "The value is 288.14", "prediction_raw_text": "The value is 286.80."}
{"id": "line_106", "truth_parsed_k": 287.22, "prediction_parsed_k_with_c_conv": 285.6, "prediction_parsed_k_strict": 285.6, "absolute_error_k_vs_c_conv_pred": 1.6200000000000045, "score_vs_c_conv_pred": 29.01185031602793, "absolute_error_k_vs_strict_pred": 1.6200000000000045, "score_vs_pred_strict": 29.01185031602793, "truth_raw_assistant_text": "The value is 287.22", "prediction_raw_text": "The value is 285.60\u0e0a\u0e37\u0e48\u0e2d"}
{"id": "line_107", "truth_parsed_k": 286.42, "prediction_parsed_k_with_c_conv": 286.44, "prediction_parsed_k_strict": 286.44, "absolute_error_k_vs_c_conv_pred": 0.01999999999998181, "score_vs_c_conv_pred": 96.00330887748554, "absolute_error_k_vs_strict_pred": 0.01999999999998181, "score_vs_pred_strict": 96.00330887748554, "truth_raw_assistant_text": "The value is 286.42", "prediction_raw_text": "The value is 286.44."}
{"id": "line_108", "truth_parsed_k": 285.6, "prediction_parsed_k_with_c_conv": 286.78, "prediction_parsed_k_strict": 286.78, "absolute_error_k_vs_c_conv_pred": 1.17999999999995, "score_vs_c_conv_pred": 36.83589438090132, "absolute_error_k_vs_strict_pred": 1.17999999999995, "score_vs_pred_strict": 36.83589438090132, "truth_raw_assistant_text": "The value is 285.60", "prediction_raw_text": "The value is 286.78"}
{"id": "line_109", "truth_parsed_k": 285.35, "prediction_parsed_k_with_c_conv": 285.92, "prediction_parsed_k_strict": 285.92, "absolute_error_k_vs_c_conv_pred": 0.5699999999999932, "score_vs_c_conv_pred": 53.801943857655445, "absolute_error_k_vs_strict_pred": 0.5699999999999932, "score_vs_pred_strict": 53.801943857655445, "truth_raw_assistant_text": "The value is 285.35", "prediction_raw_text": "The value is 285.92\u0e18\u0e38"}
{"id": "line_110", "truth_parsed_k": 285.55, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 0.6100000000000136, "score_vs_c_conv_pred": 52.29507519742948, "absolute_error_k_vs_strict_pred": 0.6100000000000136, "score_vs_pred_strict": 52.29507519742948, "truth_raw_assistant_text": "The value is 285.55", "prediction_raw_text": "The value is 284.94\ufb2b"}
{"id": "line_111", "truth_parsed_k": 286.1, "prediction_parsed_k_with_c_conv": 284.91, "prediction_parsed_k_strict": 284.91, "absolute_error_k_vs_c_conv_pred": 1.1899999999999977, "score_vs_c_conv_pred": 36.630334216240726, "absolute_error_k_vs_strict_pred": 1.1899999999999977, "score_vs_pred_strict": 36.630334216240726, "truth_raw_assistant_text": "The value is 286.10", "prediction_raw_text": "The value is 284.91RGBO"}
{"id": "line_112", "truth_parsed_k": 287.08, "prediction_parsed_k_with_c_conv": 285.53, "prediction_parsed_k_strict": 285.53, "absolute_error_k_vs_c_conv_pred": 1.5500000000000114, "score_vs_c_conv_pred": 30.11433034447373, "absolute_error_k_vs_strict_pred": 1.5500000000000114, "score_vs_pred_strict": 30.11433034447373, "truth_raw_assistant_text": "The value is 287.08", "prediction_raw_text": "The value is 285.53\u0e40\u0e17\u0e35\u0e22"}
{"id": "line_113", "truth_parsed_k": 288.08, "prediction_parsed_k_with_c_conv": 285.62, "prediction_parsed_k_strict": 285.62, "absolute_error_k_vs_c_conv_pred": 2.4599999999999795, "score_vs_c_conv_pred": 18.429829952686983, "absolute_error_k_vs_strict_pred": 2.4599999999999795, "score_vs_pred_strict": 18.429829952686983, "truth_raw_assistant_text": "The value is 288.08", "prediction_raw_text": "The value is 285.62."}
{"id": "line_114", "truth_parsed_k": 288.73, "prediction_parsed_k_with_c_conv": 285.82, "prediction_parsed_k_strict": 285.82, "absolute_error_k_vs_c_conv_pred": 2.910000000000025, "score_vs_c_conv_pred": 14.108224940898095, "absolute_error_k_vs_strict_pred": 2.910000000000025, "score_vs_pred_strict": 14.108224940898095, "truth_raw_assistant_text": "The value is 288.73", "prediction_raw_text": "The value is 285.82"}
{"id": "line_115", "truth_parsed_k": 288.89, "prediction_parsed_k_with_c_conv": 284.98, "prediction_parsed_k_strict": 284.98, "absolute_error_k_vs_c_conv_pred": 3.909999999999968, "score_vs_c_conv_pred": 6.439197058986501, "absolute_error_k_vs_strict_pred": 3.909999999999968, "score_vs_pred_strict": 6.439197058986501, "truth_raw_assistant_text": "The value is 288.89", "prediction_raw_text": "The value is 284.98"}
{"id": "line_116", "truth_parsed_k": 288.72, "prediction_parsed_k_with_c_conv": 285.82, "prediction_parsed_k_strict": 285.82, "absolute_error_k_vs_c_conv_pred": 2.900000000000034, "score_vs_c_conv_pred": 14.197097136438463, "absolute_error_k_vs_strict_pred": 2.900000000000034, "score_vs_pred_strict": 14.197097136438463, "truth_raw_assistant_text": "The value is 288.72", "prediction_raw_text": "The value is 285.82\u0e08\u0e49\u0e32"}
{"id": "line_117", "truth_parsed_k": 288.1, "prediction_parsed_k_with_c_conv": 286.47, "prediction_parsed_k_strict": 286.47, "absolute_error_k_vs_c_conv_pred": 1.6299999999999955, "score_vs_c_conv_pred": 28.85797417842264, "absolute_error_k_vs_strict_pred": 1.6299999999999955, "score_vs_pred_strict": 28.85797417842264, "truth_raw_assistant_text": "The value is 288.10", "prediction_raw_text": "The value is 286.47\u0e34\u0e48\u0e19"}
{"id": "line_118", "truth_parsed_k": 287.4, "prediction_parsed_k_with_c_conv": 286.27, "prediction_parsed_k_strict": 286.27, "absolute_error_k_vs_c_conv_pred": 1.1299999999999955, "score_vs_c_conv_pred": 37.8879129627178, "absolute_error_k_vs_strict_pred": 1.1299999999999955, "score_vs_pred_strict": 37.8879129627178, "truth_raw_assistant_text": "The value is 287.40", "prediction_raw_text": "The value is 286.27."}
{"id": "line_119", "truth_parsed_k": 286.52, "prediction_parsed_k_with_c_conv": 285.69, "prediction_parsed_k_strict": 285.69, "absolute_error_k_vs_c_conv_pred": 0.8299999999999841, "score_vs_c_conv_pred": 45.244159352399585, "absolute_error_k_vs_strict_pred": 0.8299999999999841, "score_vs_pred_strict": 45.244159352399585, "truth_raw_assistant_text": "The value is 286.52", "prediction_raw_text": "The value is 285.69\u0e0b\u0e4c"}
{"id": "line_120", "truth_parsed_k": 285.64, "prediction_parsed_k_with_c_conv": 284.58, "prediction_parsed_k_strict": 284.58, "absolute_error_k_vs_c_conv_pred": 1.0600000000000023, "score_vs_c_conv_pred": 39.433400621654805, "absolute_error_k_vs_strict_pred": 1.0600000000000023, "score_vs_pred_strict": 39.433400621654805, "truth_raw_assistant_text": "The value is 285.64", "prediction_raw_text": "The value is 284.58"}
{"id": "line_121", "truth_parsed_k": 285.53, "prediction_parsed_k_with_c_conv": 284.86, "prediction_parsed_k_strict": 284.86, "absolute_error_k_vs_c_conv_pred": 0.6699999999999591, "score_vs_c_conv_pred": 50.18197185563634, "absolute_error_k_vs_strict_pred": 0.6699999999999591, "score_vs_pred_strict": 50.18197185563634, "truth_raw_assistant_text": "The value is 285.53", "prediction_raw_text": "The value is 284.86\u0e0a\u0e39"}
{"id": "line_122", "truth_parsed_k": 285.73, "prediction_parsed_k_with_c_conv": 285.51, "prediction_parsed_k_strict": 285.51, "absolute_error_k_vs_c_conv_pred": 0.22000000000002728, "score_vs_c_conv_pred": 72.66161362986213, "absolute_error_k_vs_strict_pred": 0.22000000000002728, "score_vs_pred_strict": 72.66161362986213, "truth_raw_assistant_text": "The value is 285.73", "prediction_raw_text": "The value is 285.51\u0639\u064e"}
{"id": "line_123", "truth_parsed_k": 286.26, "prediction_parsed_k_with_c_conv": 284.93, "prediction_parsed_k_strict": 284.93, "absolute_error_k_vs_c_conv_pred": 1.329999999999984, "score_vs_c_conv_pred": 33.906023682434295, "absolute_error_k_vs_strict_pred": 1.329999999999984, "score_vs_pred_strict": 33.906023682434295, "truth_raw_assistant_text": "The value is 286.26", "prediction_raw_text": "The value is 284.93\u05e4\u05d9\u05d9\u05e1\u05d1"}
{"id": "line_124", "truth_parsed_k": 287.32, "prediction_parsed_k_with_c_conv": 285.5, "prediction_parsed_k_strict": 285.5, "absolute_error_k_vs_c_conv_pred": 1.8199999999999932, "score_vs_c_conv_pred": 26.08993525777755, "absolute_error_k_vs_strict_pred": 1.8199999999999932, "score_vs_pred_strict": 26.08993525777755, "truth_raw_assistant_text": "The value is 287.32", "prediction_raw_text": "The value is 285.50\u0e27\u0e34\u0e18\u0e35"}
{"id": "line_125", "truth_parsed_k": 288.3, "prediction_parsed_k_with_c_conv": 285.89, "prediction_parsed_k_strict": 285.89, "absolute_error_k_vs_c_conv_pred": 2.410000000000025, "score_vs_c_conv_pred": 18.9557890714651, "absolute_error_k_vs_strict_pred": 2.410000000000025, "score_vs_pred_strict": 18.9557890714651, "truth_raw_assistant_text": "The value is 288.30", "prediction_raw_text": "The value is 285.89\u0e41\u0e21\u0e49\u0e27\u0e48\u0e32"}
{"id": "line_126", "truth_parsed_k": 288.73, "prediction_parsed_k_with_c_conv": 285.51, "prediction_parsed_k_strict": 285.51, "absolute_error_k_vs_c_conv_pred": 3.2200000000000273, "score_vs_c_conv_pred": 11.489310663167618, "absolute_error_k_vs_strict_pred": 3.2200000000000273, "score_vs_pred_strict": 11.489310663167618, "truth_raw_assistant_text": "The value is 288.73", "prediction_raw_text": "The value is 285.51\u0631\u064e"}
{"id": "line_127", "truth_parsed_k": 289.01, "prediction_parsed_k_with_c_conv": 285.08, "prediction_parsed_k_strict": 285.08, "absolute_error_k_vs_c_conv_pred": 3.930000000000007, "score_vs_c_conv_pred": 6.306053396855771, "absolute_error_k_vs_strict_pred": 3.930000000000007, "score_vs_pred_strict": 6.306053396855771, "truth_raw_assistant_text": "The value is 289.01", "prediction_raw_text": "The value is 285.08\u0e02\u0e31"}
{"id": "line_128", "truth_parsed_k": 288.93, "prediction_parsed_k_with_c_conv": 284.69, "prediction_parsed_k_strict": 284.69, "absolute_error_k_vs_c_conv_pred": 4.240000000000009, "score_vs_c_conv_pred": 4.322316434143003, "absolute_error_k_vs_strict_pred": 4.240000000000009, "score_vs_pred_strict": 4.322316434143003, "truth_raw_assistant_text": "The value is 288.93", "prediction_raw_text": "The value is 284.69\u0e23\u0e32\u0e04\u0e32\u0e16\u0e39\u0e01"}
{"id": "line_129", "truth_parsed_k": 288.23, "prediction_parsed_k_with_c_conv": 284.96183655051, "prediction_parsed_k_strict": 284.96183655051, "absolute_error_k_vs_c_conv_pred": 3.2681634494900322, "score_vs_c_conv_pred": 11.104345369292556, "absolute_error_k_vs_strict_pred": 3.2681634494900322, "score_vs_pred_strict": 11.104345369292556, "truth_raw_assistant_text": "The value is 288.23", "prediction_raw_text": "The value is 284.96183655051."}
{"id": "line_130", "truth_parsed_k": 287.39, "prediction_parsed_k_with_c_conv": 284.76, "prediction_parsed_k_strict": 284.76, "absolute_error_k_vs_c_conv_pred": 2.6299999999999955, "score_vs_c_conv_pred": 16.714717562232785, "absolute_error_k_vs_strict_pred": 2.6299999999999955, "score_vs_pred_strict": 16.714717562232785, "truth_raw_assistant_text": "The value is 287.39", "prediction_raw_text": "The value is 284.76."}
{"id": "line_131", "truth_parsed_k": 286.45, "prediction_parsed_k_with_c_conv": 284.81, "prediction_parsed_k_strict": 284.81, "absolute_error_k_vs_c_conv_pred": 1.6399999999999864, "score_vs_c_conv_pred": 28.704972341771384, "absolute_error_k_vs_strict_pred": 1.6399999999999864, "score_vs_pred_strict": 28.704972341771384, "truth_raw_assistant_text": "The value is 286.45", "prediction_raw_text": "The value is 284.81"}
{"id": "line_132", "truth_parsed_k": 285.49, "prediction_parsed_k_with_c_conv": 284.9, "prediction_parsed_k_strict": 284.9, "absolute_error_k_vs_c_conv_pred": 0.5900000000000318, "score_vs_c_conv_pred": 53.03797060980495, "absolute_error_k_vs_strict_pred": 0.5900000000000318, "score_vs_pred_strict": 53.03797060980495, "truth_raw_assistant_text": "The value is 285.49", "prediction_raw_text": "The value is 284.90"}
{"id": "line_133", "truth_parsed_k": 285.49, "prediction_parsed_k_with_c_conv": 285.24, "prediction_parsed_k_strict": 285.24, "absolute_error_k_vs_c_conv_pred": 0.25, "score_vs_c_conv_pred": 70.41629275170907, "absolute_error_k_vs_strict_pred": 0.25, "score_vs_pred_strict": 70.41629275170907, "truth_raw_assistant_text": "The value is 285.49", "prediction_raw_text": "The value is 285.24"}
{"id": "line_134", "truth_parsed_k": 285.59, "prediction_parsed_k_with_c_conv": 285.94, "prediction_parsed_k_strict": 285.94, "absolute_error_k_vs_c_conv_pred": 0.35000000000002274, "score_vs_c_conv_pred": 64.05075644816651, "absolute_error_k_vs_strict_pred": 0.35000000000002274, "score_vs_pred_strict": 64.05075644816651, "truth_raw_assistant_text": "The value is 285.59", "prediction_raw_text": "The value is 285.94\uf9be"}
{"id": "line_135", "truth_parsed_k": 286.1, "prediction_parsed_k_with_c_conv": 284.88, "prediction_parsed_k_strict": 284.88, "absolute_error_k_vs_c_conv_pred": 1.2200000000000273, "score_vs_c_conv_pred": 36.02290430727667, "absolute_error_k_vs_strict_pred": 1.2200000000000273, "score_vs_pred_strict": 36.02290430727667, "truth_raw_assistant_text": "The value is 286.10", "prediction_raw_text": "The value is 284.88\u0e01\u0e35"}
{"id": "line_136", "truth_parsed_k": 287.24, "prediction_parsed_k_with_c_conv": 285.37, "prediction_parsed_k_strict": 285.37, "absolute_error_k_vs_c_conv_pred": 1.8700000000000045, "score_vs_c_conv_pred": 25.406440422737266, "absolute_error_k_vs_strict_pred": 1.8700000000000045, "score_vs_pred_strict": 25.406440422737266, "truth_raw_assistant_text": "The value is 287.24", "prediction_raw_text": "The value is 285.37\u0e2d\u0e38\u0e1b"}
{"id": "line_137", "truth_parsed_k": 288.19, "prediction_parsed_k_with_c_conv": 284.68, "prediction_parsed_k_strict": 284.68, "absolute_error_k_vs_c_conv_pred": 3.509999999999991, "score_vs_c_conv_pred": 9.250430794624586, "absolute_error_k_vs_strict_pred": 3.509999999999991, "score_vs_pred_strict": 9.250430794624586, "truth_raw_assistant_text": "The value is 288.19", "prediction_raw_text": "The value is 284.68\u0e2b\u0e22\u0e38\u0e14"}
{"id": "line_138", "truth_parsed_k": 288.86, "prediction_parsed_k_with_c_conv": 285.62, "prediction_parsed_k_strict": 285.62, "absolute_error_k_vs_c_conv_pred": 3.240000000000009, "score_vs_c_conv_pred": 11.328784102502688, "absolute_error_k_vs_strict_pred": 3.240000000000009, "score_vs_pred_strict": 11.328784102502688, "truth_raw_assistant_text": "The value is 288.86", "prediction_raw_text": "The value is 285.62."}
{"id": "line_139", "truth_parsed_k": 289.1, "prediction_parsed_k_with_c_conv": 284.88, "prediction_parsed_k_strict": 284.88, "absolute_error_k_vs_c_conv_pred": 4.220000000000027, "score_vs_c_conv_pred": 4.445982556456385, "absolute_error_k_vs_strict_pred": 4.220000000000027, "score_vs_pred_strict": 4.445982556456385, "truth_raw_assistant_text": "The value is 289.10", "prediction_raw_text": "The value is 284.88"}
{"id": "line_140", "truth_parsed_k": 288.88, "prediction_parsed_k_with_c_conv": 285.9, "prediction_parsed_k_strict": 285.9, "absolute_error_k_vs_c_conv_pred": 2.980000000000018, "score_vs_c_conv_pred": 13.494199133282237, "absolute_error_k_vs_strict_pred": 2.980000000000018, "score_vs_pred_strict": 13.494199133282237, "truth_raw_assistant_text": "The value is 288.88", "prediction_raw_text": "The value is 285.90\u0e2b\u0e23\u0e37\u0e2d\u0e44\u0e21\u0e48"}
{"id": "line_141", "truth_parsed_k": 288.21, "prediction_parsed_k_with_c_conv": 284.87, "prediction_parsed_k_strict": 284.87, "absolute_error_k_vs_c_conv_pred": 3.339999999999975, "score_vs_c_conv_pred": 10.540199176206787, "absolute_error_k_vs_strict_pred": 3.339999999999975, "score_vs_pred_strict": 10.540199176206787, "truth_raw_assistant_text": "The value is 288.21", "prediction_raw_text": "The value is 284.87."}
{"id": "line_142", "truth_parsed_k": 287.44, "prediction_parsed_k_with_c_conv": 285.51, "prediction_parsed_k_strict": 285.51, "absolute_error_k_vs_c_conv_pred": 1.9300000000000068, "score_vs_c_conv_pred": 24.608507944947235, "absolute_error_k_vs_strict_pred": 1.9300000000000068, "score_vs_pred_strict": 24.608507944947235, "truth_raw_assistant_text": "The value is 287.44", "prediction_raw_text": "The value is 285.51\u0e2d\u0e34\u0e2a"}
{"id": "line_143", "truth_parsed_k": 286.44, "prediction_parsed_k_with_c_conv": 286.69, "prediction_parsed_k_strict": 286.69, "absolute_error_k_vs_c_conv_pred": 0.25, "score_vs_c_conv_pred": 70.41629275170907, "absolute_error_k_vs_strict_pred": 0.25, "score_vs_pred_strict": 70.41629275170907, "truth_raw_assistant_text": "The value is 286.44", "prediction_raw_text": "The value is 286.69\u0e40\u0e15\u0e34\u0e1a"}
{"id": "line_144", "truth_parsed_k": 285.77, "prediction_parsed_k_with_c_conv": 285.34, "prediction_parsed_k_strict": 285.34, "absolute_error_k_vs_c_conv_pred": 0.4300000000000068, "score_vs_c_conv_pred": 59.85928508539595, "absolute_error_k_vs_strict_pred": 0.4300000000000068, "score_vs_pred_strict": 59.85928508539595, "truth_raw_assistant_text": "The value is 285.77", "prediction_raw_text": "The value is 285.34\uf96d"}
{"id": "line_145", "truth_parsed_k": 285.58, "prediction_parsed_k_with_c_conv": 286.42, "prediction_parsed_k_strict": 286.42, "absolute_error_k_vs_c_conv_pred": 0.8400000000000318, "score_vs_c_conv_pred": 44.96365420341644, "absolute_error_k_vs_strict_pred": 0.8400000000000318, "score_vs_pred_strict": 44.96365420341644, "truth_raw_assistant_text": "The value is 285.58", "prediction_raw_text": "The value is 286.42\u0e19\u0e31\u0e48\u0e19\u0e40\u0e2d\u0e07"}
{"id": "line_146", "truth_parsed_k": 285.66, "prediction_parsed_k_with_c_conv": 284.91, "prediction_parsed_k_strict": 284.91, "absolute_error_k_vs_c_conv_pred": 0.75, "score_vs_c_conv_pred": 47.60004345944323, "absolute_error_k_vs_strict_pred": 0.75, "score_vs_pred_strict": 47.60004345944323, "truth_raw_assistant_text": "The value is 285.66", "prediction_raw_text": "The value is 284.91\u0e1e\u0e24\u0e15\u0e34\u0e01\u0e23\u0e23\u0e21"}
{"id": "line_147", "truth_parsed_k": 286.26, "prediction_parsed_k_with_c_conv": 285.62, "prediction_parsed_k_strict": 285.62, "absolute_error_k_vs_c_conv_pred": 0.6399999999999864, "score_vs_c_conv_pred": 51.217801513357244, "absolute_error_k_vs_strict_pred": 0.6399999999999864, "score_vs_pred_strict": 51.217801513357244, "truth_raw_assistant_text": "The value is 286.26", "prediction_raw_text": "The value is 285.62."}
{"id": "line_148", "truth_parsed_k": 287.3, "prediction_parsed_k_with_c_conv": 286.9, "prediction_parsed_k_strict": 286.9, "absolute_error_k_vs_c_conv_pred": 0.4000000000000341, "score_vs_c_conv_pred": 61.355683974567775, "absolute_error_k_vs_strict_pred": 0.4000000000000341, "score_vs_pred_strict": 61.355683974567775, "truth_raw_assistant_text": "The value is 287.30", "prediction_raw_text": "The value is 286.90\uf940"}
{"id": "line_149", "truth_parsed_k": 288.31, "prediction_parsed_k_with_c_conv": 285.85, "prediction_parsed_k_strict": 285.85, "absolute_error_k_vs_c_conv_pred": 2.4599999999999795, "score_vs_c_conv_pred": 18.429829952686983, "absolute_error_k_vs_strict_pred": 2.4599999999999795, "score_vs_pred_strict": 18.429829952686983, "truth_raw_assistant_text": "The value is 288.31", "prediction_raw_text": "The value is 285.85\u0e01\u0e49\u0e32"}
{"id": "line_150", "truth_parsed_k": 288.88, "prediction_parsed_k_with_c_conv": 284.5, "prediction_parsed_k_strict": 284.5, "absolute_error_k_vs_c_conv_pred": 4.3799999999999955, "score_vs_c_conv_pred": 3.4721995870300892, "absolute_error_k_vs_strict_pred": 4.3799999999999955, "score_vs_pred_strict": 3.4721995870300892, "truth_raw_assistant_text": "The value is 288.88", "prediction_raw_text": "The value is 284.50\u0e1b\u0e23\u0e30\u0e2b\u0e22\u0e31\u0e14"}
{"id": "line_151", "truth_parsed_k": 289.07, "prediction_parsed_k_with_c_conv": 285.87, "prediction_parsed_k_strict": 285.87, "absolute_error_k_vs_c_conv_pred": 3.1999999999999886, "score_vs_c_conv_pred": 11.65079990761112, "absolute_error_k_vs_strict_pred": 3.1999999999999886, "score_vs_pred_strict": 11.65079990761112, "truth_raw_assistant_text": "The value is 289.07", "prediction_raw_text": "The value is 285.87\u0e07\u0e48\u0e32\u0e22\u0e46"}
{"id": "line_152", "truth_parsed_k": 288.97, "prediction_parsed_k_with_c_conv": 285.69, "prediction_parsed_k_strict": 285.69, "absolute_error_k_vs_c_conv_pred": 3.2800000000000296, "score_vs_c_conv_pred": 11.010573595407891, "absolute_error_k_vs_strict_pred": 3.2800000000000296, "score_vs_pred_strict": 11.010573595407891, "truth_raw_assistant_text": "The value is 288.97", "prediction_raw_text": "The value is 285.69."}
{"id": "line_153", "truth_parsed_k": 288.27, "prediction_parsed_k_with_c_conv": 286.85, "prediction_parsed_k_strict": 286.85, "absolute_error_k_vs_c_conv_pred": 1.419999999999959, "score_vs_c_conv_pred": 32.28984366362286, "absolute_error_k_vs_strict_pred": 1.419999999999959, "score_vs_pred_strict": 32.28984366362286, "truth_raw_assistant_text": "The value is 288.27", "prediction_raw_text": "The value is 286.85"}
{"id": "line_154", "truth_parsed_k": 287.49, "prediction_parsed_k_with_c_conv": 285.87, "prediction_parsed_k_strict": 285.87, "absolute_error_k_vs_c_conv_pred": 1.6200000000000045, "score_vs_c_conv_pred": 29.01185031602793, "absolute_error_k_vs_strict_pred": 1.6200000000000045, "score_vs_pred_strict": 29.01185031602793, "truth_raw_assistant_text": "The value is 287.49", "prediction_raw_text": "The value is 285.87\u0e1b\u0e34\u0e14"}
{"id": "line_155", "truth_parsed_k": 286.49, "prediction_parsed_k_with_c_conv": 285.39, "prediction_parsed_k_strict": 285.39, "absolute_error_k_vs_c_conv_pred": 1.1000000000000227, "score_vs_c_conv_pred": 38.53943468230319, "absolute_error_k_vs_strict_pred": 1.1000000000000227, "score_vs_pred_strict": 38.53943468230319, "truth_raw_assistant_text": "The value is 286.49", "prediction_raw_text": "The value is 285.39\u0e1b\u0e39"}
{"id": "line_156", "truth_parsed_k": 285.84, "prediction_parsed_k_with_c_conv": 285.62, "prediction_parsed_k_strict": 285.62, "absolute_error_k_vs_c_conv_pred": 0.21999999999997044, "score_vs_c_conv_pred": 72.66161362986657, "absolute_error_k_vs_strict_pred": 0.21999999999997044, "score_vs_pred_strict": 72.66161362986657, "truth_raw_assistant_text": "The value is 285.84", "prediction_raw_text": "The value is 285.62"}
{"id": "line_157", "truth_parsed_k": 285.65, "prediction_parsed_k_with_c_conv": 286.96, "prediction_parsed_k_strict": 286.96, "absolute_error_k_vs_c_conv_pred": 1.3100000000000023, "score_vs_c_conv_pred": 34.278738986277645, "absolute_error_k_vs_strict_pred": 1.3100000000000023, "score_vs_pred_strict": 34.278738986277645, "truth_raw_assistant_text": "The value is 285.65", "prediction_raw_text": "The value is 286.96\uf9fa"}
{"id": "line_158", "truth_parsed_k": 285.64, "prediction_parsed_k_with_c_conv": 286.85, "prediction_parsed_k_strict": 286.85, "absolute_error_k_vs_c_conv_pred": 1.2100000000000364, "score_vs_c_conv_pred": 36.22386233549141, "absolute_error_k_vs_strict_pred": 1.2100000000000364, "score_vs_pred_strict": 36.22386233549141, "truth_raw_assistant_text": "The value is 285.64", "prediction_raw_text": "The value is 286.85."}
{"id": "line_159", "truth_parsed_k": 286.17, "prediction_parsed_k_with_c_conv": 284.77, "prediction_parsed_k_strict": 284.77, "absolute_error_k_vs_c_conv_pred": 1.400000000000034, "score_vs_c_conv_pred": 32.640705315322236, "absolute_error_k_vs_strict_pred": 1.400000000000034, "score_vs_pred_strict": 32.640705315322236, "truth_raw_assistant_text": "The value is 286.17", "prediction_raw_text": "The value is 284.77\u0e1a\u0e49\u0e32\u0e07"}
{"id": "line_160", "truth_parsed_k": 287.25, "prediction_parsed_k_with_c_conv": 285.58, "prediction_parsed_k_strict": 285.58, "absolute_error_k_vs_c_conv_pred": 1.670000000000016, "score_vs_c_conv_pred": 28.251114674483503, "absolute_error_k_vs_strict_pred": 1.670000000000016, "score_vs_pred_strict": 28.251114674483503, "truth_raw_assistant_text": "The value is 287.25", "prediction_raw_text": "The value is 285.58\u0e22\u0e31\u0e19"}
{"id": "line_161", "truth_parsed_k": 288.28, "prediction_parsed_k_with_c_conv": 284.72, "prediction_parsed_k_strict": 284.72, "absolute_error_k_vs_c_conv_pred": 3.5599999999999454, "score_vs_c_conv_pred": 8.882552794402299, "absolute_error_k_vs_strict_pred": 3.5599999999999454, "score_vs_pred_strict": 8.882552794402299, "truth_raw_assistant_text": "The value is 288.28", "prediction_raw_text": "The value is 284.72\u0e40\u0e28\u0e23\u0e29\u0e10\u0e01\u0e34\u0e08"}
{"id": "line_162", "truth_parsed_k": 288.71, "prediction_parsed_k_with_c_conv": 285.76, "prediction_parsed_k_strict": 285.76, "absolute_error_k_vs_c_conv_pred": 2.9499999999999886, "score_vs_c_conv_pred": 13.7556405151264, "absolute_error_k_vs_strict_pred": 2.9499999999999886, "score_vs_pred_strict": 13.7556405151264, "truth_raw_assistant_text": "The value is 288.71", "prediction_raw_text": "The value is 285.76."}
{"id": "line_163", "truth_parsed_k": 288.95, "prediction_parsed_k_with_c_conv": 285.96, "prediction_parsed_k_strict": 285.96, "absolute_error_k_vs_c_conv_pred": 2.990000000000009, "score_vs_c_conv_pred": 13.40761304322623, "absolute_error_k_vs_strict_pred": 2.990000000000009, "score_vs_pred_strict": 13.40761304322623, "truth_raw_assistant_text": "The value is 288.95", "prediction_raw_text": "The value is 285.96"}
{"id": "line_164", "truth_parsed_k": 288.72, "prediction_parsed_k_with_c_conv": 285.87, "prediction_parsed_k_strict": 285.87, "absolute_error_k_vs_c_conv_pred": 2.8500000000000227, "score_vs_c_conv_pred": 14.645911705964421, "absolute_error_k_vs_strict_pred": 2.8500000000000227, "score_vs_pred_strict": 14.645911705964421, "truth_raw_assistant_text": "The value is 288.72", "prediction_raw_text": "The value is 285.87"}
{"id": "line_165", "truth_parsed_k": 288.2, "prediction_parsed_k_with_c_conv": 285.96, "prediction_parsed_k_strict": 285.96, "absolute_error_k_vs_c_conv_pred": 2.240000000000009, "score_vs_c_conv_pred": 20.825030164741108, "absolute_error_k_vs_strict_pred": 2.240000000000009, "score_vs_pred_strict": 20.825030164741108, "truth_raw_assistant_text": "The value is 288.20", "prediction_raw_text": "The value is 285.96."}
{"id": "line_166", "truth_parsed_k": 287.48, "prediction_parsed_k_with_c_conv": 285.67, "prediction_parsed_k_strict": 285.67, "absolute_error_k_vs_c_conv_pred": 1.8100000000000023, "score_vs_c_conv_pred": 26.228740981648546, "absolute_error_k_vs_strict_pred": 1.8100000000000023, "score_vs_pred_strict": 26.228740981648546, "truth_raw_assistant_text": "The value is 287.48", "prediction_raw_text": "The value is 285.67 \u0e14\u0e31\u0e07"}
{"id": "line_167", "truth_parsed_k": 286.57, "prediction_parsed_k_with_c_conv": 284.7, "prediction_parsed_k_strict": 284.7, "absolute_error_k_vs_c_conv_pred": 1.8700000000000045, "score_vs_c_conv_pred": 25.406440422737266, "absolute_error_k_vs_strict_pred": 1.8700000000000045, "score_vs_pred_strict": 25.406440422737266, "truth_raw_assistant_text": "The value is 286.57", "prediction_raw_text": "The value is 284.70\u0e40\u0e1a\u0e37"}
{"id": "line_168", "truth_parsed_k": 285.82, "prediction_parsed_k_with_c_conv": 286.61, "prediction_parsed_k_strict": 286.61, "absolute_error_k_vs_c_conv_pred": 0.7900000000000205, "score_vs_c_conv_pred": 46.396345830448524, "absolute_error_k_vs_strict_pred": 0.7900000000000205, "score_vs_pred_strict": 46.396345830448524, "truth_raw_assistant_text": "The value is 285.82", "prediction_raw_text": "The value is 286.61."}
{"id": "line_169", "truth_parsed_k": 285.55, "prediction_parsed_k_with_c_conv": 286.52, "prediction_parsed_k_strict": 286.52, "absolute_error_k_vs_c_conv_pred": 0.9699999999999704, "score_vs_c_conv_pred": 41.56042037244584, "absolute_error_k_vs_strict_pred": 0.9699999999999704, "score_vs_pred_strict": 41.56042037244584, "truth_raw_assistant_text": "The value is 285.55", "prediction_raw_text": "The value is 286.52\uf9db"}
{"id": "line_170", "truth_parsed_k": 285.55, "prediction_parsed_k_with_c_conv": 285.84, "prediction_parsed_k_strict": 285.84, "absolute_error_k_vs_c_conv_pred": 0.2899999999999636, "score_vs_c_conv_pred": 67.68704736641223, "absolute_error_k_vs_strict_pred": 0.2899999999999636, "score_vs_pred_strict": 67.68704736641223, "truth_raw_assistant_text": "The value is 285.55", "prediction_raw_text": "The value is 285.84"}
{"id": "line_171", "truth_parsed_k": 286.12, "prediction_parsed_k_with_c_conv": 285.56, "prediction_parsed_k_strict": 285.56, "absolute_error_k_vs_c_conv_pred": 0.5600000000000023, "score_vs_c_conv_pred": 54.19221519323817, "absolute_error_k_vs_strict_pred": 0.5600000000000023, "score_vs_pred_strict": 54.19221519323817, "truth_raw_assistant_text": "The value is 286.12", "prediction_raw_text": "The value is 285.56."}
{"id": "line_172", "truth_parsed_k": 287.17, "prediction_parsed_k_with_c_conv": 284.59, "prediction_parsed_k_strict": 284.59, "absolute_error_k_vs_c_conv_pred": 2.580000000000041, "score_vs_c_conv_pred": 17.207922755976888, "absolute_error_k_vs_strict_pred": 2.580000000000041, "score_vs_pred_strict": 17.207922755976888, "truth_raw_assistant_text": "The value is 287.17", "prediction_raw_text": "The value is 284.59."}
{"id": "line_173", "truth_parsed_k": 288.09, "prediction_parsed_k_with_c_conv": 285.87, "prediction_parsed_k_strict": 285.87, "absolute_error_k_vs_c_conv_pred": 2.2199999999999704, "score_vs_c_conv_pred": 21.053721567334694, "absolute_error_k_vs_strict_pred": 2.2199999999999704, "score_vs_pred_strict": 21.053721567334694, "truth_raw_assistant_text": "The value is 288.09", "prediction_raw_text": "The value is 285.87."}
{"id": "line_174", "truth_parsed_k": 288.68, "prediction_parsed_k_with_c_conv": 285.96, "prediction_parsed_k_strict": 285.96, "absolute_error_k_vs_c_conv_pred": 2.7200000000000273, "score_vs_c_conv_pred": 15.84909211061859, "absolute_error_k_vs_strict_pred": 2.7200000000000273, "score_vs_pred_strict": 15.84909211061859, "truth_raw_assistant_text": "The value is 288.68", "prediction_raw_text": "The value is 285.96\u0e20\u0e32\u0e29\u0e35"}
{"id": "line_175", "truth_parsed_k": 288.96, "prediction_parsed_k_with_c_conv": 284.93, "prediction_parsed_k_strict": 284.93, "absolute_error_k_vs_c_conv_pred": 4.029999999999973, "score_vs_c_conv_pred": 5.650034332126874, "absolute_error_k_vs_strict_pred": 4.029999999999973, "score_vs_pred_strict": 5.650034332126874, "truth_raw_assistant_text": "The value is 288.96", "prediction_raw_text": "The value is 284.93\u0e17\u0e31\u0e19\u0e17\u0e35"}
{"id": "line_176", "truth_parsed_k": 288.76, "prediction_parsed_k_with_c_conv": 285.3, "prediction_parsed_k_strict": 285.3, "absolute_error_k_vs_c_conv_pred": 3.4599999999999795, "score_vs_c_conv_pred": 9.623404219809629, "absolute_error_k_vs_strict_pred": 3.4599999999999795, "score_vs_pred_strict": 9.623404219809629, "truth_raw_assistant_text": "The value is 288.76", "prediction_raw_text": "The value is 285.30."}
{"id": "line_177", "truth_parsed_k": 288.2, "prediction_parsed_k_with_c_conv": 287.28, "prediction_parsed_k_strict": 287.28, "absolute_error_k_vs_c_conv_pred": 0.9200000000000159, "score_vs_c_conv_pred": 42.818978898095395, "absolute_error_k_vs_strict_pred": 0.9200000000000159, "score_vs_pred_strict": 42.818978898095395, "truth_raw_assistant_text": "The value is 288.20", "prediction_raw_text": "The value is 287.28\u0e22\u0e38\u0e04"}
{"id": "line_178", "truth_parsed_k": 287.37, "prediction_parsed_k_with_c_conv": 285.37, "prediction_parsed_k_strict": 285.37, "absolute_error_k_vs_c_conv_pred": 2.0, "score_vs_c_conv_pred": 23.70652048310418, "absolute_error_k_vs_strict_pred": 2.0, "score_vs_pred_strict": 23.70652048310418, "truth_raw_assistant_text": "The value is 287.37", "prediction_raw_text": "The value is 285.37\u0e32\u0e18\u0e34"}
{"id": "line_179", "truth_parsed_k": 286.35, "prediction_parsed_k_with_c_conv": 284.92, "prediction_parsed_k_strict": 284.92, "absolute_error_k_vs_c_conv_pred": 1.4300000000000068, "score_vs_c_conv_pred": 32.11611241065043, "absolute_error_k_vs_strict_pred": 1.4300000000000068, "score_vs_pred_strict": 32.11611241065043, "truth_raw_assistant_text": "The value is 286.35", "prediction_raw_text": "The value is 284.92\uf9aa"}
{"id": "line_180", "truth_parsed_k": 285.64, "prediction_parsed_k_with_c_conv": 285.48, "prediction_parsed_k_strict": 285.48, "absolute_error_k_vs_c_conv_pred": 0.15999999999996817, "score_vs_c_conv_pred": 77.8063969632971, "absolute_error_k_vs_strict_pred": 0.15999999999996817, "score_vs_pred_strict": 77.8063969632971, "truth_raw_assistant_text": "The value is 285.64", "prediction_raw_text": "The value is 285.48\u0e2b\u0e49\u0e32\u0e21"}
{"id": "line_181", "truth_parsed_k": 285.35, "prediction_parsed_k_with_c_conv": 284.87, "prediction_parsed_k_strict": 284.87, "absolute_error_k_vs_c_conv_pred": 0.4800000000000182, "score_vs_c_conv_pred": 57.53644489985519, "absolute_error_k_vs_strict_pred": 0.4800000000000182, "score_vs_pred_strict": 57.53644489985519, "truth_raw_assistant_text": "The value is 285.35", "prediction_raw_text": "The value is 284.87\u0e22\u0e37\u0e19\u0e22\u0e31\u0e19"}
{"id": "line_182", "truth_parsed_k": 285.65, "prediction_parsed_k_with_c_conv": 284.78, "prediction_parsed_k_strict": 284.78, "absolute_error_k_vs_c_conv_pred": 0.8700000000000045, "score_vs_c_conv_pred": 44.13925561171903, "absolute_error_k_vs_strict_pred": 0.8700000000000045, "score_vs_pred_strict": 44.13925561171903, "truth_raw_assistant_text": "The value is 285.65", "prediction_raw_text": "The value is 284.78\u0e2a\u0e21\u0e1a\u0e39"}
{"id": "line_183", "truth_parsed_k": 286.19, "prediction_parsed_k_with_c_conv": 285.89, "prediction_parsed_k_strict": 285.89, "absolute_error_k_vs_c_conv_pred": 0.30000000000001137, "score_vs_c_conv_pred": 67.04586824652068, "absolute_error_k_vs_strict_pred": 0.30000000000001137, "score_vs_pred_strict": 67.04586824652068, "truth_raw_assistant_text": "The value is 286.19", "prediction_raw_text": "The value is 285.89\u0e14\u0e48\u0e27\u0e19"}
{"id": "line_184", "truth_parsed_k": 287.29, "prediction_parsed_k_with_c_conv": 284.92, "prediction_parsed_k_strict": 284.92, "absolute_error_k_vs_c_conv_pred": 2.3700000000000045, "score_vs_c_conv_pred": 19.3840804188251, "absolute_error_k_vs_strict_pred": 2.3700000000000045, "score_vs_pred_strict": 19.3840804188251, "truth_raw_assistant_text": "The value is 287.29", "prediction_raw_text": "The value is 284.92."}
{"id": "line_185", "truth_parsed_k": 288.29, "prediction_parsed_k_with_c_conv": 285.98, "prediction_parsed_k_strict": 285.98, "absolute_error_k_vs_c_conv_pred": 2.3100000000000023, "score_vs_c_conv_pred": 20.039567163977523, "absolute_error_k_vs_strict_pred": 2.3100000000000023, "score_vs_pred_strict": 20.039567163977523, "truth_raw_assistant_text": "The value is 288.29", "prediction_raw_text": "The value is 285.98<|repo_name|>"}
{"id": "line_186", "truth_parsed_k": 288.81, "prediction_parsed_k_with_c_conv": 285.49, "prediction_parsed_k_strict": 285.49, "absolute_error_k_vs_c_conv_pred": 3.319999999999993, "score_vs_c_conv_pred": 10.696079500386857, "absolute_error_k_vs_strict_pred": 3.319999999999993, "score_vs_pred_strict": 10.696079500386857, "truth_raw_assistant_text": "The value is 288.81", "prediction_raw_text": "The value is 285.49"}
{"id": "line_187", "truth_parsed_k": 289.18, "prediction_parsed_k_with_c_conv": 284.83, "prediction_parsed_k_strict": 284.83, "absolute_error_k_vs_c_conv_pred": 4.350000000000023, "score_vs_c_conv_pred": 3.652121753975035, "absolute_error_k_vs_strict_pred": 4.350000000000023, "score_vs_pred_strict": 3.652121753975035, "truth_raw_assistant_text": "The value is 289.18", "prediction_raw_text": "The value is 284.83 \u0623\u064a\u0636\u0627\u064b"}
{"id": "line_188", "truth_parsed_k": 288.92, "prediction_parsed_k_with_c_conv": 285.97, "prediction_parsed_k_strict": 285.97, "absolute_error_k_vs_c_conv_pred": 2.9499999999999886, "score_vs_c_conv_pred": 13.7556405151264, "absolute_error_k_vs_strict_pred": 2.9499999999999886, "score_vs_pred_strict": 13.7556405151264, "truth_raw_assistant_text": "The value is 288.92", "prediction_raw_text": "The value is 285.97\u0e01\u0e32\u0e23\u0e40\u0e25\u0e48\u0e19"}
{"id": "line_189", "truth_parsed_k": 288.34, "prediction_parsed_k_with_c_conv": 287.42, "prediction_parsed_k_strict": 287.42, "absolute_error_k_vs_c_conv_pred": 0.9199999999999591, "score_vs_c_conv_pred": 42.81897889809685, "absolute_error_k_vs_strict_pred": 0.9199999999999591, "score_vs_pred_strict": 42.81897889809685, "truth_raw_assistant_text": "The value is 288.34", "prediction_raw_text": "The value is 287.42."}
{"id": "line_190", "truth_parsed_k": 287.63, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 2.659999999999968, "score_vs_c_conv_pred": 16.423073005999058, "absolute_error_k_vs_strict_pred": 2.659999999999968, "score_vs_pred_strict": 16.423073005999058, "truth_raw_assistant_text": "The value is 287.63", "prediction_raw_text": "The value is 284.97\u0e23\u0e32\u0e22\u0e44\u0e14\u0e49"}
{"id": "line_191", "truth_parsed_k": 286.76, "prediction_parsed_k_with_c_conv": 285.86, "prediction_parsed_k_strict": 285.86, "absolute_error_k_vs_c_conv_pred": 0.8999999999999773, "score_vs_c_conv_pred": 43.33934776341783, "absolute_error_k_vs_strict_pred": 0.8999999999999773, "score_vs_pred_strict": 43.33934776341783, "truth_raw_assistant_text": "The value is 286.76", "prediction_raw_text": "The value is 285.86"}
{"id": "line_192", "truth_parsed_k": 285.94, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 1.1200000000000045, "score_vs_c_conv_pred": 38.103340118118844, "absolute_error_k_vs_strict_pred": 1.1200000000000045, "score_vs_pred_strict": 38.103340118118844, "truth_raw_assistant_text": "The value is 285.94", "prediction_raw_text": "The value is 284.82\u0e08\u0e30\u0e40\u0e1b\u0e47\u0e19"}
{"id": "line_193", "truth_parsed_k": 285.54, "prediction_parsed_k_with_c_conv": 285.52, "prediction_parsed_k_strict": 285.52, "absolute_error_k_vs_c_conv_pred": 0.020000000000038654, "score_vs_c_conv_pred": 96.00330887747498, "absolute_error_k_vs_strict_pred": 0.020000000000038654, "score_vs_pred_strict": 96.00330887747498, "truth_raw_assistant_text": "The value is 285.54", "prediction_raw_text": "The value is 285.52\u45f4"}
{"id": "line_194", "truth_parsed_k": 286.01, "prediction_parsed_k_with_c_conv": 284.9, "prediction_parsed_k_strict": 284.9, "absolute_error_k_vs_c_conv_pred": 1.1100000000000136, "score_vs_c_conv_pred": 38.320504608045646, "absolute_error_k_vs_strict_pred": 1.1100000000000136, "score_vs_pred_strict": 38.320504608045646, "truth_raw_assistant_text": "The value is 286.01", "prediction_raw_text": "The value is 284.90\u0e19\u0e31\u0e01\u0e40\u0e23\u0e35\u0e22\u0e19"}
{"id": "line_195", "truth_parsed_k": 286.34, "prediction_parsed_k_with_c_conv": 285.92, "prediction_parsed_k_strict": 285.92, "absolute_error_k_vs_c_conv_pred": 0.4199999999999591, "score_vs_c_conv_pred": 60.34890313391317, "absolute_error_k_vs_strict_pred": 0.4199999999999591, "score_vs_pred_strict": 60.34890313391317, "truth_raw_assistant_text": "The value is 286.34", "prediction_raw_text": "The value is 285.92\u0e2b\u0e25\u0e31\u0e01\u0e2a\u0e39\u0e15\u0e23"}
{"id": "line_196", "truth_parsed_k": 287.36, "prediction_parsed_k_with_c_conv": 285.76, "prediction_parsed_k_strict": 285.76, "absolute_error_k_vs_c_conv_pred": 1.6000000000000227, "score_vs_c_conv_pred": 29.322265866446685, "absolute_error_k_vs_strict_pred": 1.6000000000000227, "score_vs_pred_strict": 29.322265866446685, "truth_raw_assistant_text": "The value is 287.36", "prediction_raw_text": "The value is 285.76\u0e01\u0e47\u0e08\u0e30"}
{"id": "line_197", "truth_parsed_k": 288.28, "prediction_parsed_k_with_c_conv": 285.74, "prediction_parsed_k_strict": 285.74, "absolute_error_k_vs_c_conv_pred": 2.5399999999999636, "score_vs_c_conv_pred": 17.609095923295737, "absolute_error_k_vs_strict_pred": 2.5399999999999636, "score_vs_pred_strict": 17.609095923295737, "truth_raw_assistant_text": "The value is 288.28", "prediction_raw_text": "The value is 285.74."}
{"id": "line_198", "truth_parsed_k": 288.76, "prediction_parsed_k_with_c_conv": 285.58, "prediction_parsed_k_strict": 285.58, "absolute_error_k_vs_c_conv_pred": 3.180000000000007, "score_vs_c_conv_pred": 11.813263452031219, "absolute_error_k_vs_strict_pred": 3.180000000000007, "score_vs_pred_strict": 11.813263452031219, "truth_raw_assistant_text": "The value is 288.76", "prediction_raw_text": "The value is 285.58."}
{"id": "line_199", "truth_parsed_k": 289.1, "prediction_parsed_k_with_c_conv": 284.96, "prediction_parsed_k_strict": 284.96, "absolute_error_k_vs_c_conv_pred": 4.140000000000043, "score_vs_c_conv_pred": 4.946405799627785, "absolute_error_k_vs_strict_pred": 4.140000000000043, "score_vs_pred_strict": 4.946405799627785, "truth_raw_assistant_text": "The value is 289.10", "prediction_raw_text": "The value is 284.96."}
{"id": "line_200", "truth_parsed_k": 289.04, "prediction_parsed_k_with_c_conv": 286.5, "prediction_parsed_k_strict": 286.5, "absolute_error_k_vs_c_conv_pred": 2.5400000000000205, "score_vs_c_conv_pred": 17.609095923295158, "absolute_error_k_vs_strict_pred": 2.5400000000000205, "score_vs_pred_strict": 17.609095923295158, "truth_raw_assistant_text": "The value is 289.04", "prediction_raw_text": "The value is 286.50\u0e14\u0e19\u0e15\u0e23\u0e35"}
{"id": "line_201", "truth_parsed_k": 288.36, "prediction_parsed_k_with_c_conv": 284.84, "prediction_parsed_k_strict": 284.84, "absolute_error_k_vs_c_conv_pred": 3.5200000000000387, "score_vs_c_conv_pred": 9.176452037088579, "absolute_error_k_vs_strict_pred": 3.5200000000000387, "score_vs_pred_strict": 9.176452037088579, "truth_raw_assistant_text": "The value is 288.36", "prediction_raw_text": "The value is 284.84."}
{"id": "line_202", "truth_parsed_k": 287.61, "prediction_parsed_k_with_c_conv": 284.66, "prediction_parsed_k_strict": 284.66, "absolute_error_k_vs_c_conv_pred": 2.9499999999999886, "score_vs_c_conv_pred": 13.7556405151264, "absolute_error_k_vs_strict_pred": 2.9499999999999886, "score_vs_pred_strict": 13.7556405151264, "truth_raw_assistant_text": "The value is 287.61", "prediction_raw_text": "The value is 284.66"}
{"id": "line_203", "truth_parsed_k": 286.79, "prediction_parsed_k_with_c_conv": 285.87, "prediction_parsed_k_strict": 285.87, "absolute_error_k_vs_c_conv_pred": 0.9200000000000159, "score_vs_c_conv_pred": 42.818978898095395, "absolute_error_k_vs_strict_pred": 0.9200000000000159, "score_vs_pred_strict": 42.818978898095395, "truth_raw_assistant_text": "The value is 286.79", "prediction_raw_text": "The value is 285.87."}
{"id": "line_204", "truth_parsed_k": 286.1, "prediction_parsed_k_with_c_conv": 285.5, "prediction_parsed_k_strict": 285.5, "absolute_error_k_vs_c_conv_pred": 0.6000000000000227, "score_vs_c_conv_pred": 52.663961114064094, "absolute_error_k_vs_strict_pred": 0.6000000000000227, "score_vs_pred_strict": 52.663961114064094, "truth_raw_assistant_text": "The value is 286.10", "prediction_raw_text": "The value is 285.50\u0e40\u0e0b\u0e2d\u0e23\u0e4c"}
{"id": "line_205", "truth_parsed_k": 285.71, "prediction_parsed_k_with_c_conv": 285.95, "prediction_parsed_k_strict": 285.95, "absolute_error_k_vs_c_conv_pred": 0.2400000000000091, "score_vs_c_conv_pred": 71.14412762073536, "absolute_error_k_vs_strict_pred": 0.2400000000000091, "score_vs_pred_strict": 71.14412762073536, "truth_raw_assistant_text": "The value is 285.71", "prediction_raw_text": "The value is 285.95\uf985"}
{"id": "line_206", "truth_parsed_k": 286.0, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 1.0600000000000023, "score_vs_c_conv_pred": 39.433400621654805, "absolute_error_k_vs_strict_pred": 1.0600000000000023, "score_vs_pred_strict": 39.433400621654805, "truth_raw_assistant_text": "The value is 286.00", "prediction_raw_text": "The value is 284.94"}
{"id": "line_207", "truth_parsed_k": 286.43, "prediction_parsed_k_with_c_conv": 286.61, "prediction_parsed_k_strict": 286.61, "absolute_error_k_vs_c_conv_pred": 0.18000000000000682, "score_vs_c_conv_pred": 75.98005307873946, "absolute_error_k_vs_strict_pred": 0.18000000000000682, "score_vs_pred_strict": 75.98005307873946, "truth_raw_assistant_text": "The value is 286.43", "prediction_raw_text": "The value is 286.61\u0e02\u0e27\u0e31"}
{"id": "line_208", "truth_parsed_k": 287.46, "prediction_parsed_k_with_c_conv": 284.7, "prediction_parsed_k_strict": 284.7, "absolute_error_k_vs_c_conv_pred": 2.759999999999991, "score_vs_c_conv_pred": 15.473124386525205, "absolute_error_k_vs_strict_pred": 2.759999999999991, "score_vs_pred_strict": 15.473124386525205, "truth_raw_assistant_text": "The value is 287.46", "prediction_raw_text": "The value is 284.70\u0e01\u0e48\u0e2d\u0e19"}
{"id": "line_209", "truth_parsed_k": 288.44, "prediction_parsed_k_with_c_conv": 285.8, "prediction_parsed_k_strict": 285.8, "absolute_error_k_vs_c_conv_pred": 2.6399999999999864, "score_vs_c_conv_pred": 16.617151329389145, "absolute_error_k_vs_strict_pred": 2.6399999999999864, "score_vs_pred_strict": 16.617151329389145, "truth_raw_assistant_text": "The value is 288.44", "prediction_raw_text": "The value is 285.80"}
{"id": "line_210", "truth_parsed_k": 289.02, "prediction_parsed_k_with_c_conv": 284.95, "prediction_parsed_k_strict": 284.95, "absolute_error_k_vs_c_conv_pred": 4.069999999999993, "score_vs_c_conv_pred": 5.392037141290995, "absolute_error_k_vs_strict_pred": 4.069999999999993, "score_vs_pred_strict": 5.392037141290995, "truth_raw_assistant_text": "The value is 289.02", "prediction_raw_text": "The value is 284.95\u0e2a\u0e16\u0e34\u0e15\u0e34"}
{"id": "line_211", "truth_parsed_k": 289.16, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 4.220000000000027, "score_vs_c_conv_pred": 4.445982556456385, "absolute_error_k_vs_strict_pred": 4.220000000000027, "score_vs_pred_strict": 4.445982556456385, "truth_raw_assistant_text": "The value is 289.16", "prediction_raw_text": "The value is 284.94"}
{"id": "line_212", "truth_parsed_k": 288.89, "prediction_parsed_k_with_c_conv": 285.36, "prediction_parsed_k_strict": 285.36, "absolute_error_k_vs_c_conv_pred": 3.5299999999999727, "score_vs_c_conv_pred": 9.102675961334816, "absolute_error_k_vs_strict_pred": 3.5299999999999727, "score_vs_pred_strict": 9.102675961334816, "truth_raw_assistant_text": "The value is 288.89", "prediction_raw_text": "The value is 285.36\u0e1c\u0e25\u0e34\u0e15\u0e20\u0e31\u0e13\u0e11\u0e4c"}
{"id": "line_213", "truth_parsed_k": 288.43, "prediction_parsed_k_with_c_conv": 285.34, "prediction_parsed_k_strict": 285.34, "absolute_error_k_vs_c_conv_pred": 3.090000000000032, "score_vs_c_conv_pred": 12.556728641830572, "absolute_error_k_vs_strict_pred": 3.090000000000032, "score_vs_pred_strict": 12.556728641830572, "truth_raw_assistant_text": "The value is 288.43", "prediction_raw_text": "The value is 285.34."}
{"id": "line_214", "truth_parsed_k": 287.59, "prediction_parsed_k_with_c_conv": 284.73, "prediction_parsed_k_strict": 284.73, "absolute_error_k_vs_c_conv_pred": 2.859999999999957, "score_vs_c_conv_pred": 14.555548363428606, "absolute_error_k_vs_strict_pred": 2.859999999999957, "score_vs_pred_strict": 14.555548363428606, "truth_raw_assistant_text": "The value is 287.59", "prediction_raw_text": "The value is 284.73\uf966"}
{"id": "line_215", "truth_parsed_k": 286.81, "prediction_parsed_k_with_c_conv": 285.64, "prediction_parsed_k_strict": 285.64, "absolute_error_k_vs_c_conv_pred": 1.170000000000016, "score_vs_c_conv_pred": 37.043035793130116, "absolute_error_k_vs_strict_pred": 1.170000000000016, "score_vs_pred_strict": 37.043035793130116, "truth_raw_assistant_text": "The value is 286.81", "prediction_raw_text": "The value is 285.64."}
{"id": "line_216", "truth_parsed_k": 285.89, "prediction_parsed_k_with_c_conv": 285.77, "prediction_parsed_k_strict": 285.77, "absolute_error_k_vs_c_conv_pred": 0.12000000000000455, "score_vs_c_conv_pred": 81.8787824457204, "absolute_error_k_vs_strict_pred": 0.12000000000000455, "score_vs_pred_strict": 81.8787824457204, "truth_raw_assistant_text": "The value is 285.89", "prediction_raw_text": "The value is 285.77."}
{"id": "line_217", "truth_parsed_k": 285.67, "prediction_parsed_k_with_c_conv": 286.55, "prediction_parsed_k_strict": 286.55, "absolute_error_k_vs_c_conv_pred": 0.8799999999999955, "score_vs_c_conv_pred": 43.86997085960033, "absolute_error_k_vs_strict_pred": 0.8799999999999955, "score_vs_pred_strict": 43.86997085960033, "truth_raw_assistant_text": "The value is 285.67", "prediction_raw_text": "The value is 286.55\u0e40\u0e2b\u0e25\u0e47\u0e01"}
{"id": "line_218", "truth_parsed_k": 285.96, "prediction_parsed_k_with_c_conv": 284.7, "prediction_parsed_k_strict": 284.7, "absolute_error_k_vs_c_conv_pred": 1.259999999999991, "score_vs_c_conv_pred": 35.233741580417814, "absolute_error_k_vs_strict_pred": 1.259999999999991, "score_vs_pred_strict": 35.233741580417814, "truth_raw_assistant_text": "The value is 285.96", "prediction_raw_text": "The value is 284.70\u0e40\u0e2b\u0e25\u0e48\u0e32\u0e19\u0e35\u0e49"}
{"id": "line_219", "truth_parsed_k": 286.71, "prediction_parsed_k_with_c_conv": 284.7, "prediction_parsed_k_strict": 284.7, "absolute_error_k_vs_c_conv_pred": 2.009999999999991, "score_vs_c_conv_pred": 23.580096538183405, "absolute_error_k_vs_strict_pred": 2.009999999999991, "score_vs_pred_strict": 23.580096538183405, "truth_raw_assistant_text": "The value is 286.71", "prediction_raw_text": "The value is 284.70\u0e2a\u0e23\u0e38\u0e1b"}
{"id": "line_220", "truth_parsed_k": 287.61, "prediction_parsed_k_with_c_conv": 284.83, "prediction_parsed_k_strict": 284.83, "absolute_error_k_vs_c_conv_pred": 2.7800000000000296, "score_vs_c_conv_pred": 15.287090825852822, "absolute_error_k_vs_strict_pred": 2.7800000000000296, "score_vs_pred_strict": 15.287090825852822, "truth_raw_assistant_text": "The value is 287.61", "prediction_raw_text": "The value is 284.83\u0625\u0650"}
{"id": "line_221", "truth_parsed_k": 288.41, "prediction_parsed_k_with_c_conv": 286.94, "prediction_parsed_k_strict": 286.94, "absolute_error_k_vs_c_conv_pred": 1.4700000000000273, "score_vs_c_conv_pred": 31.432183563992467, "absolute_error_k_vs_strict_pred": 1.4700000000000273, "score_vs_pred_strict": 31.432183563992467, "truth_raw_assistant_text": "The value is 288.41", "prediction_raw_text": "The value is 286.94."}
{"id": "line_222", "truth_parsed_k": 288.98, "prediction_parsed_k_with_c_conv": 284.87, "prediction_parsed_k_strict": 284.87, "absolute_error_k_vs_c_conv_pred": 4.110000000000014, "score_vs_c_conv_pred": 5.136488359299129, "absolute_error_k_vs_strict_pred": 4.110000000000014, "score_vs_pred_strict": 5.136488359299129, "truth_raw_assistant_text": "The value is 288.98", "prediction_raw_text": "The value is 284.87."}
{"id": "line_223", "truth_parsed_k": 289.34, "prediction_parsed_k_with_c_conv": 285.34, "prediction_parsed_k_strict": 285.34, "absolute_error_k_vs_c_conv_pred": 4.0, "score_vs_c_conv_pred": 5.845167438219834, "absolute_error_k_vs_strict_pred": 4.0, "score_vs_pred_strict": 5.845167438219834, "truth_raw_assistant_text": "The value is 289.34", "prediction_raw_text": "The value is 285.34\u0e2d\u0e38\u0e1b\u0e01\u0e23"}
{"id": "line_224", "truth_parsed_k": 289.05, "prediction_parsed_k_with_c_conv": 285.98, "prediction_parsed_k_strict": 285.98, "absolute_error_k_vs_c_conv_pred": 3.069999999999993, "score_vs_c_conv_pred": 12.724768193723124, "absolute_error_k_vs_strict_pred": 3.069999999999993, "score_vs_pred_strict": 12.724768193723124, "truth_raw_assistant_text": "The value is 289.05", "prediction_raw_text": "The value is 285.98\u0e07\u0e04\u0e4c"}
{"id": "line_225", "truth_parsed_k": 288.38, "prediction_parsed_k_with_c_conv": 284.83, "prediction_parsed_k_strict": 284.83, "absolute_error_k_vs_c_conv_pred": 3.5500000000000114, "score_vs_c_conv_pred": 8.955727434012662, "absolute_error_k_vs_strict_pred": 3.5500000000000114, "score_vs_pred_strict": 8.955727434012662, "truth_raw_assistant_text": "The value is 288.38", "prediction_raw_text": "The value is 284.83\u0e40\u0e1b\u0e49\u0e32"}
{"id": "line_226", "truth_parsed_k": 287.69, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 2.8700000000000045, "score_vs_c_conv_pred": 14.465487239993047, "absolute_error_k_vs_strict_pred": 2.8700000000000045, "score_vs_pred_strict": 14.465487239993047, "truth_raw_assistant_text": "The value is 287.69", "prediction_raw_text": "The value is 284.82\u0e22\u0e48\u0e2d\u0e21"}
{"id": "line_227", "truth_parsed_k": 286.84, "prediction_parsed_k_with_c_conv": 284.92, "prediction_parsed_k_strict": 284.92, "absolute_error_k_vs_c_conv_pred": 1.919999999999959, "score_vs_c_conv_pred": 24.73986552643037, "absolute_error_k_vs_strict_pred": 1.919999999999959, "score_vs_pred_strict": 24.73986552643037, "truth_raw_assistant_text": "The value is 286.84", "prediction_raw_text": "The value is 284.92\uf9b4"}
{"id": "line_228", "truth_parsed_k": 286.12, "prediction_parsed_k_with_c_conv": 284.87, "prediction_parsed_k_strict": 284.87, "absolute_error_k_vs_c_conv_pred": 1.25, "score_vs_c_conv_pred": 35.42887468651077, "absolute_error_k_vs_strict_pred": 1.25, "score_vs_pred_strict": 35.42887468651077, "truth_raw_assistant_text": "The value is 286.12", "prediction_raw_text": "The value is 284.87\u0e44\u0e1f\u0e1f\u0e49\u0e32"}
{"id": "line_229", "truth_parsed_k": 285.86, "prediction_parsed_k_with_c_conv": 284.77, "prediction_parsed_k_strict": 284.77, "absolute_error_k_vs_c_conv_pred": 1.0900000000000318, "score_vs_c_conv_pred": 38.76015928537909, "absolute_error_k_vs_strict_pred": 1.0900000000000318, "score_vs_pred_strict": 38.76015928537909, "truth_raw_assistant_text": "The value is 285.86", "prediction_raw_text": "The value is 284.77\u0e01\u0e32\u0e23\u0e28\u0e36\u0e01\u0e29\u0e32"}
{"id": "line_230", "truth_parsed_k": 285.87, "prediction_parsed_k_with_c_conv": 284.92, "prediction_parsed_k_strict": 284.92, "absolute_error_k_vs_c_conv_pred": 0.9499999999999886, "score_vs_c_conv_pred": 42.056807714813495, "absolute_error_k_vs_strict_pred": 0.9499999999999886, "score_vs_pred_strict": 42.056807714813495, "truth_raw_assistant_text": "The value is 285.87", "prediction_raw_text": "The value is 284.92\u0e1c\u0e25\u0e34\u0e15\u0e20\u0e31\u0e13\u0e11\u0e4c"}
{"id": "line_231", "truth_parsed_k": 286.51, "prediction_parsed_k_with_c_conv": 286.52, "prediction_parsed_k_strict": 286.52, "absolute_error_k_vs_c_conv_pred": 0.009999999999990905, "score_vs_c_conv_pred": 97.92757378196347, "absolute_error_k_vs_strict_pred": 0.009999999999990905, "score_vs_pred_strict": 97.92757378196347, "truth_raw_assistant_text": "The value is 286.51", "prediction_raw_text": "The value is 286.52\u0e04\u0e25\u0e34\u0e01"}
{"id": "line_232", "truth_parsed_k": 287.49, "prediction_parsed_k_with_c_conv": 286.71, "prediction_parsed_k_strict": 286.71, "absolute_error_k_vs_c_conv_pred": 0.7800000000000296, "score_vs_c_conv_pred": 46.69226365237833, "absolute_error_k_vs_strict_pred": 0.7800000000000296, "score_vs_pred_strict": 46.69226365237833, "truth_raw_assistant_text": "The value is 287.49", "prediction_raw_text": "The value is 286.71"}
{"id": "line_233", "truth_parsed_k": 288.45, "prediction_parsed_k_with_c_conv": 285.95, "prediction_parsed_k_strict": 285.95, "absolute_error_k_vs_c_conv_pred": 2.5, "score_vs_c_conv_pred": 18.016336211152296, "absolute_error_k_vs_strict_pred": 2.5, "score_vs_pred_strict": 18.016336211152296, "truth_raw_assistant_text": "The value is 288.45", "prediction_raw_text": "The value is 285.95\ufb2a"}
{"id": "line_234", "truth_parsed_k": 288.94, "prediction_parsed_k_with_c_conv": 285.78, "prediction_parsed_k_strict": 285.78, "absolute_error_k_vs_c_conv_pred": 3.160000000000025, "score_vs_c_conv_pred": 11.97671312415396, "absolute_error_k_vs_strict_pred": 3.160000000000025, "score_vs_pred_strict": 11.97671312415396, "truth_raw_assistant_text": "The value is 288.94", "prediction_raw_text": "The value is 285.78\u0e08\u0e31"}
{"id": "line_235", "truth_parsed_k": 289.12, "prediction_parsed_k_with_c_conv": 285.44, "prediction_parsed_k_strict": 285.44, "absolute_error_k_vs_c_conv_pred": 3.680000000000007, "score_vs_c_conv_pred": 8.019624457567708, "absolute_error_k_vs_strict_pred": 3.680000000000007, "score_vs_pred_strict": 8.019624457567708, "truth_raw_assistant_text": "The value is 289.12", "prediction_raw_text": "The value is 285.44\u0e01\u0e47\u0e40\u0e1b\u0e47\u0e19"}
{"id": "line_236", "truth_parsed_k": 289.02, "prediction_parsed_k_with_c_conv": 284.74, "prediction_parsed_k_strict": 284.74, "absolute_error_k_vs_c_conv_pred": 4.279999999999973, "score_vs_c_conv_pred": 4.076675042708677, "absolute_error_k_vs_strict_pred": 4.279999999999973, "score_vs_pred_strict": 4.076675042708677, "truth_raw_assistant_text": "The value is 289.02", "prediction_raw_text": "The value is 284.74\u0e41\u0e2b\u0e48\u0e07\u0e19\u0e35\u0e49"}
{"id": "line_237", "truth_parsed_k": 288.42, "prediction_parsed_k_with_c_conv": 285.7, "prediction_parsed_k_strict": 285.7, "absolute_error_k_vs_c_conv_pred": 2.7200000000000273, "score_vs_c_conv_pred": 15.84909211061859, "absolute_error_k_vs_strict_pred": 2.7200000000000273, "score_vs_pred_strict": 15.84909211061859, "truth_raw_assistant_text": "The value is 288.42", "prediction_raw_text": "The value is 285.70\u0e2a\u0e31\u0e07\u0e40\u0e01\u0e15"}
{"id": "line_238", "truth_parsed_k": 287.64, "prediction_parsed_k_with_c_conv": 284.92, "prediction_parsed_k_strict": 284.92, "absolute_error_k_vs_c_conv_pred": 2.7199999999999704, "score_vs_c_conv_pred": 15.849092110619134, "absolute_error_k_vs_strict_pred": 2.7199999999999704, "score_vs_pred_strict": 15.849092110619134, "truth_raw_assistant_text": "The value is 287.64", "prediction_raw_text": "The value is 284.92\u0e40\u0e2b\u0e47\u0e19\u0e27\u0e48\u0e32"}
{"id": "line_239", "truth_parsed_k": 286.72, "prediction_parsed_k_with_c_conv": 284.5, "prediction_parsed_k_strict": 284.5, "absolute_error_k_vs_c_conv_pred": 2.2200000000000273, "score_vs_c_conv_pred": 21.05372156733405, "absolute_error_k_vs_strict_pred": 2.2200000000000273, "score_vs_pred_strict": 21.05372156733405, "truth_raw_assistant_text": "The value is 286.72", "prediction_raw_text": "The value is 284.50\u0e40\u0e17\u0e35\u0e22\u0e1a"}
{"id": "line_240", "truth_parsed_k": 286.04, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 1.1000000000000227, "score_vs_c_conv_pred": 38.53943468230319, "absolute_error_k_vs_strict_pred": 1.1000000000000227, "score_vs_pred_strict": 38.53943468230319, "truth_raw_assistant_text": "The value is 286.04", "prediction_raw_text": "The value is 284.94\u0e1a\u0e39"}
{"id": "line_241", "truth_parsed_k": 285.58, "prediction_parsed_k_with_c_conv": 284.76, "prediction_parsed_k_strict": 284.76, "absolute_error_k_vs_c_conv_pred": 0.8199999999999932, "score_vs_c_conv_pred": 45.52761724140508, "absolute_error_k_vs_strict_pred": 0.8199999999999932, "score_vs_pred_strict": 45.52761724140508, "truth_raw_assistant_text": "The value is 285.58", "prediction_raw_text": "The value is 284.76."}
{"id": "line_242", "truth_parsed_k": 285.97, "prediction_parsed_k_with_c_conv": 284.95, "prediction_parsed_k_strict": 284.95, "absolute_error_k_vs_c_conv_pred": 1.0200000000000387, "score_vs_c_conv_pred": 40.358066602658205, "absolute_error_k_vs_strict_pred": 1.0200000000000387, "score_vs_pred_strict": 40.358066602658205, "truth_raw_assistant_text": "The value is 285.97", "prediction_raw_text": "The value is 284.95\uf939"}
{"id": "line_243", "truth_parsed_k": 286.32, "prediction_parsed_k_with_c_conv": 285.84, "prediction_parsed_k_strict": 285.84, "absolute_error_k_vs_c_conv_pred": 0.4800000000000182, "score_vs_c_conv_pred": 57.53644489985519, "absolute_error_k_vs_strict_pred": 0.4800000000000182, "score_vs_pred_strict": 57.53644489985519, "truth_raw_assistant_text": "The value is 286.32", "prediction_raw_text": "The value is 285.84\u0e2d\u0e31\u0e15"}
{"id": "line_244", "truth_parsed_k": 287.32, "prediction_parsed_k_with_c_conv": 285.9, "prediction_parsed_k_strict": 285.9, "absolute_error_k_vs_c_conv_pred": 1.420000000000016, "score_vs_c_conv_pred": 32.28984366362187, "absolute_error_k_vs_strict_pred": 1.420000000000016, "score_vs_pred_strict": 32.28984366362187, "truth_raw_assistant_text": "The value is 287.32", "prediction_raw_text": "The value is 285.90\uf967"}
{"id": "line_245", "truth_parsed_k": 288.25, "prediction_parsed_k_with_c_conv": 285.58, "prediction_parsed_k_strict": 285.58, "absolute_error_k_vs_c_conv_pred": 2.670000000000016, "score_vs_c_conv_pred": 16.326555874691216, "absolute_error_k_vs_strict_pred": 2.670000000000016, "score_vs_pred_strict": 16.326555874691216, "truth_raw_assistant_text": "The value is 288.25", "prediction_raw_text": "The value is 285.58"}
{"id": "line_246", "truth_parsed_k": 288.84, "prediction_parsed_k_with_c_conv": 285.6, "prediction_parsed_k_strict": 285.6, "absolute_error_k_vs_c_conv_pred": 3.2399999999999523, "score_vs_c_conv_pred": 11.328784102503143, "absolute_error_k_vs_strict_pred": 3.2399999999999523, "score_vs_pred_strict": 11.328784102503143, "truth_raw_assistant_text": "The value is 288.84", "prediction_raw_text": "The value is 285.60 \uc624\ufffd"}
{"id": "line_247", "truth_parsed_k": 289.11, "prediction_parsed_k_with_c_conv": 284.88, "prediction_parsed_k_strict": 284.88, "absolute_error_k_vs_c_conv_pred": 4.230000000000018, "score_vs_c_conv_pred": 4.38407850435194, "absolute_error_k_vs_strict_pred": 4.230000000000018, "score_vs_pred_strict": 4.38407850435194, "truth_raw_assistant_text": "The value is 289.11", "prediction_raw_text": "The value is 284.88\u0e1f\u0e38\u0e15"}
{"id": "line_248", "truth_parsed_k": 288.92, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 4.100000000000023, "score_vs_c_conv_pred": 5.200148556338924, "absolute_error_k_vs_strict_pred": 4.100000000000023, "score_vs_pred_strict": 5.200148556338924, "truth_raw_assistant_text": "The value is 288.92", "prediction_raw_text": "The value is 284.82\u0e2d\u0e31\u0e1e"}
{"id": "line_249", "truth_parsed_k": 288.4, "prediction_parsed_k_with_c_conv": 286.70981292394, "prediction_parsed_k_strict": 286.70981292394, "absolute_error_k_vs_c_conv_pred": 1.690187076059999, "score_vs_c_conv_pred": 27.949962240291505, "absolute_error_k_vs_strict_pred": 1.690187076059999, "score_vs_pred_strict": 27.949962240291505, "truth_raw_assistant_text": "The value is 288.40", "prediction_raw_text": "The value is 286.70981292394."}
{"id": "line_250", "truth_parsed_k": 287.6, "prediction_parsed_k_with_c_conv": 284.64, "prediction_parsed_k_strict": 284.64, "absolute_error_k_vs_c_conv_pred": 2.9600000000000364, "score_vs_c_conv_pred": 13.668211052588275, "absolute_error_k_vs_strict_pred": 2.9600000000000364, "score_vs_pred_strict": 13.668211052588275, "truth_raw_assistant_text": "The value is 287.60", "prediction_raw_text": "The value is 284.64\u0628\u0650"}
{"id": "line_251", "truth_parsed_k": 286.72, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 1.75, "score_vs_c_conv_pred": 27.07694498829184, "absolute_error_k_vs_strict_pred": 1.75, "score_vs_pred_strict": 27.07694498829184, "truth_raw_assistant_text": "The value is 286.72", "prediction_raw_text": "The value is 284.97"}
{"id": "line_252", "truth_parsed_k": 285.87, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 1.0500000000000114, "score_vs_c_conv_pred": 39.661607502758514, "absolute_error_k_vs_strict_pred": 1.0500000000000114, "score_vs_pred_strict": 39.661607502758514, "truth_raw_assistant_text": "The value is 285.87", "prediction_raw_text": "The value is 284.82\u0e2d\u0e31\u0e15\u0e42\u0e19"}
{"id": "line_253", "truth_parsed_k": 285.75, "prediction_parsed_k_with_c_conv": 285.78, "prediction_parsed_k_strict": 285.78, "absolute_error_k_vs_c_conv_pred": 0.029999999999972715, "score_vs_c_conv_pred": 94.20742681836049, "absolute_error_k_vs_strict_pred": 0.029999999999972715, "score_vs_pred_strict": 94.20742681836049, "truth_raw_assistant_text": "The value is 285.75", "prediction_raw_text": "The value is 285.78\uf97a"}
{"id": "line_254", "truth_parsed_k": 285.66, "prediction_parsed_k_with_c_conv": 285.71, "prediction_parsed_k_strict": 285.71, "absolute_error_k_vs_c_conv_pred": 0.049999999999954525, "score_vs_c_conv_pred": 90.93939122286744, "absolute_error_k_vs_strict_pred": 0.049999999999954525, "score_vs_pred_strict": 90.93939122286744, "truth_raw_assistant_text": "The value is 285.66", "prediction_raw_text": "The value is 285.71\u0e40\u0e2b\u0e25\u0e48\u0e32"}
{"id": "line_255", "truth_parsed_k": 286.18, "prediction_parsed_k_with_c_conv": 286.29, "prediction_parsed_k_strict": 286.29, "absolute_error_k_vs_c_conv_pred": 0.11000000000001364, "score_vs_c_conv_pred": 83.00095526617443, "absolute_error_k_vs_strict_pred": 0.11000000000001364, "score_vs_pred_strict": 83.00095526617443, "truth_raw_assistant_text": "The value is 286.18", "prediction_raw_text": "The value is 286.29\u0e1b\u0e0f\u0e34"}
{"id": "line_256", "truth_parsed_k": 287.3, "prediction_parsed_k_with_c_conv": 285.62, "prediction_parsed_k_strict": 285.62, "absolute_error_k_vs_c_conv_pred": 1.6800000000000068, "score_vs_c_conv_pred": 28.1015128963355, "absolute_error_k_vs_strict_pred": 1.6800000000000068, "score_vs_pred_strict": 28.1015128963355, "truth_raw_assistant_text": "The value is 287.30", "prediction_raw_text": "The value is 285.62"}
{"id": "line_257", "truth_parsed_k": 288.19, "prediction_parsed_k_with_c_conv": 285.53, "prediction_parsed_k_strict": 285.53, "absolute_error_k_vs_c_conv_pred": 2.660000000000025, "score_vs_c_conv_pred": 16.4230730059985, "absolute_error_k_vs_strict_pred": 2.660000000000025, "score_vs_pred_strict": 16.4230730059985, "truth_raw_assistant_text": "The value is 288.19", "prediction_raw_text": "The value is 285.53\u0e40\u0e01\u0e32\u0e2b\u0e25\u0e35"}
{"id": "line_258", "truth_parsed_k": 288.74, "prediction_parsed_k_with_c_conv": 284.93, "prediction_parsed_k_strict": 284.93, "absolute_error_k_vs_c_conv_pred": 3.8100000000000023, "score_vs_c_conv_pred": 7.1149728188685035, "absolute_error_k_vs_strict_pred": 3.8100000000000023, "score_vs_pred_strict": 7.1149728188685035, "truth_raw_assistant_text": "The value is 288.74", "prediction_raw_text": "The value is 284.93\u0e19\u0e49\u0e2d\u0e22"}
{"id": "line_259", "truth_parsed_k": 289.09, "prediction_parsed_k_with_c_conv": 286.59, "prediction_parsed_k_strict": 286.59, "absolute_error_k_vs_c_conv_pred": 2.5, "score_vs_c_conv_pred": 18.016336211152296, "absolute_error_k_vs_strict_pred": 2.5, "score_vs_pred_strict": 18.016336211152296, "truth_raw_assistant_text": "The value is 289.09", "prediction_raw_text": "The value is 286.59\u0e1b\u0e23\u0e31\u0e1a\u0e1b\u0e23\u0e38\u0e07"}
{"id": "line_260", "truth_parsed_k": 288.8, "prediction_parsed_k_with_c_conv": 284.72, "prediction_parsed_k_strict": 284.72, "absolute_error_k_vs_c_conv_pred": 4.079999999999984, "score_vs_c_conv_pred": 5.3279222281299665, "absolute_error_k_vs_strict_pred": 4.079999999999984, "score_vs_pred_strict": 5.3279222281299665, "truth_raw_assistant_text": "The value is 288.80", "prediction_raw_text": "The value is 284.72\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19"}
{"id": "line_261", "truth_parsed_k": 288.09, "prediction_parsed_k_with_c_conv": 285.94, "prediction_parsed_k_strict": 285.94, "absolute_error_k_vs_c_conv_pred": 2.1499999999999773, "score_vs_c_conv_pred": 21.869791619321642, "absolute_error_k_vs_strict_pred": 2.1499999999999773, "score_vs_pred_strict": 21.869791619321642, "truth_raw_assistant_text": "The value is 288.09", "prediction_raw_text": "The value is 285.94."}
{"id": "line_262", "truth_parsed_k": 287.37, "prediction_parsed_k_with_c_conv": 285.96, "prediction_parsed_k_strict": 285.96, "absolute_error_k_vs_c_conv_pred": 1.410000000000025, "score_vs_c_conv_pred": 32.46470304951512, "absolute_error_k_vs_strict_pred": 1.410000000000025, "score_vs_pred_strict": 32.46470304951512, "truth_raw_assistant_text": "The value is 287.37", "prediction_raw_text": "The value is 285.96."}
{"id": "line_263", "truth_parsed_k": 286.4, "prediction_parsed_k_with_c_conv": 286.29, "prediction_parsed_k_strict": 286.29, "absolute_error_k_vs_c_conv_pred": 0.1099999999999568, "score_vs_c_conv_pred": 83.00095526618094, "absolute_error_k_vs_strict_pred": 0.1099999999999568, "score_vs_pred_strict": 83.00095526618094, "truth_raw_assistant_text": "The value is 286.40", "prediction_raw_text": "The value is 286.29 for 1871-11 based on the provided values."}
{"id": "line_264", "truth_parsed_k": 285.62, "prediction_parsed_k_with_c_conv": 286.8, "prediction_parsed_k_strict": 286.8, "absolute_error_k_vs_c_conv_pred": 1.1800000000000068, "score_vs_c_conv_pred": 36.83589438090016, "absolute_error_k_vs_strict_pred": 1.1800000000000068, "score_vs_pred_strict": 36.83589438090016, "truth_raw_assistant_text": "The value is 285.62", "prediction_raw_text": "The value is 286.80"}
{"id": "line_265", "truth_parsed_k": 285.37, "prediction_parsed_k_with_c_conv": 284.75, "prediction_parsed_k_strict": 284.75, "absolute_error_k_vs_c_conv_pred": 0.6200000000000045, "score_vs_c_conv_pred": 51.93117437838052, "absolute_error_k_vs_strict_pred": 0.6200000000000045, "score_vs_pred_strict": 51.93117437838052, "truth_raw_assistant_text": "The value is 285.37", "prediction_raw_text": "The value is 284.75\uf9e3"}
{"id": "line_266", "truth_parsed_k": 285.57, "prediction_parsed_k_with_c_conv": 285.52, "prediction_parsed_k_strict": 285.52, "absolute_error_k_vs_c_conv_pred": 0.05000000000001137, "score_vs_c_conv_pred": 90.9393912228587, "absolute_error_k_vs_strict_pred": 0.05000000000001137, "score_vs_pred_strict": 90.9393912228587, "truth_raw_assistant_text": "The value is 285.57", "prediction_raw_text": "The value is 285.52\u00ea\u0309"}
{"id": "line_267", "truth_parsed_k": 286.53, "prediction_parsed_k_with_c_conv": 285.84, "prediction_parsed_k_strict": 285.84, "absolute_error_k_vs_c_conv_pred": 0.6899999999999977, "score_vs_c_conv_pred": 49.512912574160495, "absolute_error_k_vs_strict_pred": 0.6899999999999977, "score_vs_pred_strict": 49.512912574160495, "truth_raw_assistant_text": "The value is 286.53", "prediction_raw_text": "The value is 285.84\ufb1d"}
{"id": "line_268", "truth_parsed_k": 287.42, "prediction_parsed_k_with_c_conv": 286.56, "prediction_parsed_k_strict": 286.56, "absolute_error_k_vs_c_conv_pred": 0.8600000000000136, "score_vs_c_conv_pred": 44.41126045786328, "absolute_error_k_vs_strict_pred": 0.8600000000000136, "score_vs_pred_strict": 44.41126045786328, "truth_raw_assistant_text": "The value is 287.42", "prediction_raw_text": "The value is 286.56."}
{"id": "line_269", "truth_parsed_k": 288.22, "prediction_parsed_k_with_c_conv": 284.78, "prediction_parsed_k_strict": 284.78, "absolute_error_k_vs_c_conv_pred": 3.4400000000000546, "score_vs_c_conv_pred": 9.774052169676084, "absolute_error_k_vs_strict_pred": 3.4400000000000546, "score_vs_pred_strict": 9.774052169676084, "truth_raw_assistant_text": "The value is 288.22", "prediction_raw_text": "The value is 284.78\u0e02\u0e49\u0e32"}
{"id": "line_270", "truth_parsed_k": 288.7, "prediction_parsed_k_with_c_conv": 284.84, "prediction_parsed_k_strict": 284.84, "absolute_error_k_vs_c_conv_pred": 3.8600000000000136, "score_vs_c_conv_pred": 6.774965134999311, "absolute_error_k_vs_strict_pred": 3.8600000000000136, "score_vs_pred_strict": 6.774965134999311, "truth_raw_assistant_text": "The value is 288.70", "prediction_raw_text": "The value is 284.84\u0e44\u0e21\u0e48\u0e15\u0e49\u0e2d\u0e07"}
{"id": "line_271", "truth_parsed_k": 288.96, "prediction_parsed_k_with_c_conv": 285.98, "prediction_parsed_k_strict": 285.98, "absolute_error_k_vs_c_conv_pred": 2.9799999999999613, "score_vs_c_conv_pred": 13.494199133282736, "absolute_error_k_vs_strict_pred": 2.9799999999999613, "score_vs_pred_strict": 13.494199133282736, "truth_raw_assistant_text": "The value is 288.96", "prediction_raw_text": "The value is 285.98\u0631\u064e"}
{"id": "line_272", "truth_parsed_k": 288.83, "prediction_parsed_k_with_c_conv": 285.86, "prediction_parsed_k_strict": 285.86, "absolute_error_k_vs_c_conv_pred": 2.9699999999999704, "score_vs_c_conv_pred": 13.581064533790299, "absolute_error_k_vs_strict_pred": 2.9699999999999704, "score_vs_pred_strict": 13.581064533790299, "truth_raw_assistant_text": "The value is 288.83", "prediction_raw_text": "The value is 285.86."}
{"id": "line_273", "truth_parsed_k": 288.33, "prediction_parsed_k_with_c_conv": 285.8, "prediction_parsed_k_strict": 285.8, "absolute_error_k_vs_c_conv_pred": 2.5299999999999727, "score_vs_c_conv_pred": 17.710330068113056, "absolute_error_k_vs_strict_pred": 2.5299999999999727, "score_vs_pred_strict": 17.710330068113056, "truth_raw_assistant_text": "The value is 288.33", "prediction_raw_text": "The value is 285.80"}
{"id": "line_274", "truth_parsed_k": 287.49, "prediction_parsed_k_with_c_conv": 286.8, "prediction_parsed_k_strict": 286.8, "absolute_error_k_vs_c_conv_pred": 0.6899999999999977, "score_vs_c_conv_pred": 49.512912574160495, "absolute_error_k_vs_strict_pred": 0.6899999999999977, "score_vs_pred_strict": 49.512912574160495, "truth_raw_assistant_text": "The value is 287.49", "prediction_raw_text": "The value is 286.80\uf985"}
{"id": "line_275", "truth_parsed_k": 286.48, "prediction_parsed_k_with_c_conv": 285.43, "prediction_parsed_k_strict": 285.43, "absolute_error_k_vs_c_conv_pred": 1.0500000000000114, "score_vs_c_conv_pred": 39.661607502758514, "absolute_error_k_vs_strict_pred": 1.0500000000000114, "score_vs_pred_strict": 39.661607502758514, "truth_raw_assistant_text": "The value is 286.48", "prediction_raw_text": "The value is 285.43."}
{"id": "line_276", "truth_parsed_k": 285.8, "prediction_parsed_k_with_c_conv": 284.99, "prediction_parsed_k_strict": 284.99, "absolute_error_k_vs_c_conv_pred": 0.8100000000000023, "score_vs_c_conv_pred": 45.81409069644933, "absolute_error_k_vs_strict_pred": 0.8100000000000023, "score_vs_pred_strict": 45.81409069644933, "truth_raw_assistant_text": "The value is 285.80", "prediction_raw_text": "The value is 284.99\u0e21\u0e31\u0e01"}
{"id": "line_277", "truth_parsed_k": 285.41, "prediction_parsed_k_with_c_conv": 287.37, "prediction_parsed_k_strict": 287.37, "absolute_error_k_vs_c_conv_pred": 1.9599999999999795, "score_vs_c_conv_pred": 24.218236609364517, "absolute_error_k_vs_strict_pred": 1.9599999999999795, "score_vs_pred_strict": 24.218236609364517, "truth_raw_assistant_text": "The value is 285.41", "prediction_raw_text": "The value is 287.37\u0e1a\u0e33\u0e23\u0e38\u0e07"}
{"id": "line_278", "truth_parsed_k": 285.94, "prediction_parsed_k_with_c_conv": 285.6, "prediction_parsed_k_strict": 285.6, "absolute_error_k_vs_c_conv_pred": 0.339999999999975, "score_vs_c_conv_pred": 64.62371957006627, "absolute_error_k_vs_strict_pred": 0.339999999999975, "score_vs_pred_strict": 64.62371957006627, "truth_raw_assistant_text": "The value is 285.94", "prediction_raw_text": "The value is 285.60\u0e23\u0e31\u0e1a\u0e1c\u0e34\u0e14"}
{"id": "line_279", "truth_parsed_k": 286.47, "prediction_parsed_k_with_c_conv": 285.8, "prediction_parsed_k_strict": 285.8, "absolute_error_k_vs_c_conv_pred": 0.6700000000000159, "score_vs_c_conv_pred": 50.18197185563442, "absolute_error_k_vs_strict_pred": 0.6700000000000159, "score_vs_pred_strict": 50.18197185563442, "truth_raw_assistant_text": "The value is 286.47", "prediction_raw_text": "The value is 285.80\u0e2b\u0e19\u0e31\u0e07\u0e2a"}
{"id": "line_280", "truth_parsed_k": 287.4, "prediction_parsed_k_with_c_conv": 285.84, "prediction_parsed_k_strict": 285.84, "absolute_error_k_vs_c_conv_pred": 1.5600000000000023, "score_vs_c_conv_pred": 29.95404266403099, "absolute_error_k_vs_strict_pred": 1.5600000000000023, "score_vs_pred_strict": 29.95404266403099, "truth_raw_assistant_text": "The value is 287.40", "prediction_raw_text": "The value is 285.84."}
{"id": "line_281", "truth_parsed_k": 288.32, "prediction_parsed_k_with_c_conv": 285.82, "prediction_parsed_k_strict": 285.82, "absolute_error_k_vs_c_conv_pred": 2.5, "score_vs_c_conv_pred": 18.016336211152296, "absolute_error_k_vs_strict_pred": 2.5, "score_vs_pred_strict": 18.016336211152296, "truth_raw_assistant_text": "The value is 288.32", "prediction_raw_text": "The value is 285.82\uf9bf"}
{"id": "line_282", "truth_parsed_k": 289.04, "prediction_parsed_k_with_c_conv": 285.92, "prediction_parsed_k_strict": 285.92, "absolute_error_k_vs_c_conv_pred": 3.1200000000000045, "score_vs_c_conv_pred": 12.306619251205364, "absolute_error_k_vs_strict_pred": 3.1200000000000045, "score_vs_pred_strict": 12.306619251205364, "truth_raw_assistant_text": "The value is 289.04", "prediction_raw_text": "The value is 285.92\uf998"}
{"id": "line_283", "truth_parsed_k": 289.27, "prediction_parsed_k_with_c_conv": 285.9, "prediction_parsed_k_strict": 285.9, "absolute_error_k_vs_c_conv_pred": 3.3700000000000045, "score_vs_c_conv_pred": 10.308057645382362, "absolute_error_k_vs_strict_pred": 3.3700000000000045, "score_vs_pred_strict": 10.308057645382362, "truth_raw_assistant_text": "The value is 289.27", "prediction_raw_text": "The value is 285.90\u0e2a\u0e33\u0e19\u0e31\u0e01"}
{"id": "line_284", "truth_parsed_k": 289.11, "prediction_parsed_k_with_c_conv": 285.88, "prediction_parsed_k_strict": 285.88, "absolute_error_k_vs_c_conv_pred": 3.230000000000018, "score_vs_c_conv_pred": 11.408927765250688, "absolute_error_k_vs_strict_pred": 3.230000000000018, "score_vs_pred_strict": 11.408927765250688, "truth_raw_assistant_text": "The value is 289.11", "prediction_raw_text": "The value is 285.88\u0e22\u0e38\u0e04"}
{"id": "line_285", "truth_parsed_k": 288.58, "prediction_parsed_k_with_c_conv": 285.84, "prediction_parsed_k_strict": 285.84, "absolute_error_k_vs_c_conv_pred": 2.740000000000009, "score_vs_c_conv_pred": 15.660452104108114, "absolute_error_k_vs_strict_pred": 2.740000000000009, "score_vs_pred_strict": 15.660452104108114, "truth_raw_assistant_text": "The value is 288.58", "prediction_raw_text": "The value is 285.84."}
{"id": "line_286", "truth_parsed_k": 287.79, "prediction_parsed_k_with_c_conv": 285.67, "prediction_parsed_k_strict": 285.67, "absolute_error_k_vs_c_conv_pred": 2.1200000000000045, "score_vs_c_conv_pred": 22.227251525046988, "absolute_error_k_vs_strict_pred": 2.1200000000000045, "score_vs_pred_strict": 22.227251525046988, "truth_raw_assistant_text": "The value is 287.79", "prediction_raw_text": "The value is 285.67."}
{"id": "line_287", "truth_parsed_k": 286.85, "prediction_parsed_k_with_c_conv": 285.5, "prediction_parsed_k_strict": 285.5, "absolute_error_k_vs_c_conv_pred": 1.3500000000000227, "score_vs_c_conv_pred": 33.538396801276996, "absolute_error_k_vs_strict_pred": 1.3500000000000227, "score_vs_pred_strict": 33.538396801276996, "truth_raw_assistant_text": "The value is 286.85", "prediction_raw_text": "The value is 285.50\u7cbe\u5f69\u64ad\u62a5"}
{"id": "line_288", "truth_parsed_k": 286.09, "prediction_parsed_k_with_c_conv": 284.96, "prediction_parsed_k_strict": 284.96, "absolute_error_k_vs_c_conv_pred": 1.1299999999999955, "score_vs_c_conv_pred": 37.8879129627178, "absolute_error_k_vs_strict_pred": 1.1299999999999955, "score_vs_pred_strict": 37.8879129627178, "truth_raw_assistant_text": "The value is 286.09", "prediction_raw_text": "The value is 284.96."}
{"id": "line_289", "truth_parsed_k": 285.63, "prediction_parsed_k_with_c_conv": 286.88, "prediction_parsed_k_strict": 286.88, "absolute_error_k_vs_c_conv_pred": 1.25, "score_vs_c_conv_pred": 35.42887468651077, "absolute_error_k_vs_strict_pred": 1.25, "score_vs_pred_strict": 35.42887468651077, "truth_raw_assistant_text": "The value is 285.63", "prediction_raw_text": "The value is 286.88."}
{"id": "line_290", "truth_parsed_k": 285.88, "prediction_parsed_k_with_c_conv": 286.72, "prediction_parsed_k_strict": 286.72, "absolute_error_k_vs_c_conv_pred": 0.8400000000000318, "score_vs_c_conv_pred": 44.96365420341644, "absolute_error_k_vs_strict_pred": 0.8400000000000318, "score_vs_pred_strict": 44.96365420341644, "truth_raw_assistant_text": "The value is 285.88", "prediction_raw_text": "The value is 286.72\u0e28\u0e31\u0e01\u0e14\u0e34"}
{"id": "line_291", "truth_parsed_k": 286.45, "prediction_parsed_k_with_c_conv": 285.99, "prediction_parsed_k_strict": 285.99, "absolute_error_k_vs_c_conv_pred": 0.45999999999997954, "score_vs_c_conv_pred": 58.44168142671445, "absolute_error_k_vs_strict_pred": 0.45999999999997954, "score_vs_pred_strict": 58.44168142671445, "truth_raw_assistant_text": "The value is 286.45", "prediction_raw_text": "The value is 285.99"}
{"id": "line_292", "truth_parsed_k": 287.63, "prediction_parsed_k_with_c_conv": 286.82, "prediction_parsed_k_strict": 286.82, "absolute_error_k_vs_c_conv_pred": 0.8100000000000023, "score_vs_c_conv_pred": 45.81409069644933, "absolute_error_k_vs_strict_pred": 0.8100000000000023, "score_vs_pred_strict": 45.81409069644933, "truth_raw_assistant_text": "The value is 287.63", "prediction_raw_text": "The value is 286.82\u0e01\u0e32\u0e23\u0e1c\u0e25\u0e34\u0e15"}
{"id": "line_293", "truth_parsed_k": 288.63, "prediction_parsed_k_with_c_conv": 284.87, "prediction_parsed_k_strict": 284.87, "absolute_error_k_vs_c_conv_pred": 3.759999999999991, "score_vs_c_conv_pred": 7.4593285448395825, "absolute_error_k_vs_strict_pred": 3.759999999999991, "score_vs_pred_strict": 7.4593285448395825, "truth_raw_assistant_text": "The value is 288.63", "prediction_raw_text": "The value is 284.87."}
{"id": "line_294", "truth_parsed_k": 289.12, "prediction_parsed_k_with_c_conv": 286.7, "prediction_parsed_k_strict": 286.7, "absolute_error_k_vs_c_conv_pred": 2.420000000000016, "score_vs_c_conv_pred": 18.849772199067893, "absolute_error_k_vs_strict_pred": 2.420000000000016, "score_vs_pred_strict": 18.849772199067893, "truth_raw_assistant_text": "The value is 289.12", "prediction_raw_text": "The value is 286.70\u0e40\u0e27\u0e47\u0e1a\u0e44\u0e0b\u0e15\u0e4c"}
{"id": "line_295", "truth_parsed_k": 289.32, "prediction_parsed_k_with_c_conv": 285.89, "prediction_parsed_k_strict": 285.89, "absolute_error_k_vs_c_conv_pred": 3.430000000000007, "score_vs_c_conv_pred": 9.849693373363877, "absolute_error_k_vs_strict_pred": 3.430000000000007, "score_vs_pred_strict": 9.849693373363877, "truth_raw_assistant_text": "The value is 289.32", "prediction_raw_text": "The value is 285.89."}
{"id": "line_296", "truth_parsed_k": 289.19, "prediction_parsed_k_with_c_conv": 285.77, "prediction_parsed_k_strict": 285.77, "absolute_error_k_vs_c_conv_pred": 3.420000000000016, "score_vs_c_conv_pred": 9.925547651145816, "absolute_error_k_vs_strict_pred": 3.420000000000016, "score_vs_pred_strict": 9.925547651145816, "truth_raw_assistant_text": "The value is 289.19", "prediction_raw_text": "The value is 285.77\u0e04\u0e25\u0e34"}
{"id": "line_297", "truth_parsed_k": 288.57, "prediction_parsed_k_with_c_conv": 285.9, "prediction_parsed_k_strict": 285.9, "absolute_error_k_vs_c_conv_pred": 2.670000000000016, "score_vs_c_conv_pred": 16.326555874691216, "absolute_error_k_vs_strict_pred": 2.670000000000016, "score_vs_pred_strict": 16.326555874691216, "truth_raw_assistant_text": "The value is 288.57", "prediction_raw_text": "The value is 285.90"}
{"id": "line_298", "truth_parsed_k": 287.85, "prediction_parsed_k_with_c_conv": 286.62, "prediction_parsed_k_strict": 286.62, "absolute_error_k_vs_c_conv_pred": 1.2300000000000182, "score_vs_c_conv_pred": 35.82343487071261, "absolute_error_k_vs_strict_pred": 1.2300000000000182, "score_vs_pred_strict": 35.82343487071261, "truth_raw_assistant_text": "The value is 287.85", "prediction_raw_text": "The value is 286.62 \u064a\u064e"}
{"id": "line_299", "truth_parsed_k": 286.97, "prediction_parsed_k_with_c_conv": 285.9, "prediction_parsed_k_strict": 285.9, "absolute_error_k_vs_c_conv_pred": 1.07000000000005, "score_vs_c_conv_pred": 39.20711146809928, "absolute_error_k_vs_strict_pred": 1.07000000000005, "score_vs_pred_strict": 39.20711146809928, "truth_raw_assistant_text": "The value is 286.97", "prediction_raw_text": "The value is 285.90\u0e1e\u0e34\u0e21\u0e1e"}
{"id": "line_300", "truth_parsed_k": 286.09, "prediction_parsed_k_with_c_conv": 285.75, "prediction_parsed_k_strict": 285.75, "absolute_error_k_vs_c_conv_pred": 0.339999999999975, "score_vs_c_conv_pred": 64.62371957006627, "absolute_error_k_vs_strict_pred": 0.339999999999975, "score_vs_pred_strict": 64.62371957006627, "truth_raw_assistant_text": "The value is 286.09", "prediction_raw_text": "The value is 285.75 \u0e02\u0e13\u0e30\u0e17\u0e35\u0e48"}
{"id": "line_301", "truth_parsed_k": 285.55, "prediction_parsed_k_with_c_conv": 286.92, "prediction_parsed_k_strict": 286.92, "absolute_error_k_vs_c_conv_pred": 1.3700000000000045, "score_vs_c_conv_pred": 33.17572127461603, "absolute_error_k_vs_strict_pred": 1.3700000000000045, "score_vs_pred_strict": 33.17572127461603, "truth_raw_assistant_text": "The value is 285.55", "prediction_raw_text": "The value is 286.92\u0e1e\u0e23\u0e49\u0e2d\u0e21\u0e01\u0e31\u0e1a"}
{"id": "line_302", "truth_parsed_k": 285.83, "prediction_parsed_k_with_c_conv": 285.99, "prediction_parsed_k_strict": 285.99, "absolute_error_k_vs_c_conv_pred": 0.160000000000025, "score_vs_c_conv_pred": 77.80639696329173, "absolute_error_k_vs_strict_pred": 0.160000000000025, "score_vs_pred_strict": 77.80639696329173, "truth_raw_assistant_text": "The value is 285.83", "prediction_raw_text": "The value is 285.99\u0e04\u0e32\u0e2a\u0e34\u0e42\u0e19\u0e2d\u0e2d\u0e19\u0e44\u0e25\u0e19\u0e4c"}
{"id": "line_303", "truth_parsed_k": 286.38, "prediction_parsed_k_with_c_conv": 285.84, "prediction_parsed_k_strict": 285.84, "absolute_error_k_vs_c_conv_pred": 0.5400000000000205, "score_vs_c_conv_pred": 54.99014767102744, "absolute_error_k_vs_strict_pred": 0.5400000000000205, "score_vs_pred_strict": 54.99014767102744, "truth_raw_assistant_text": "The value is 286.38", "prediction_raw_text": "The value is 285.84."}
{"id": "line_304", "truth_parsed_k": 287.53, "prediction_parsed_k_with_c_conv": 284.78, "prediction_parsed_k_strict": 284.78, "absolute_error_k_vs_c_conv_pred": 2.75, "score_vs_c_conv_pred": 15.56662535132074, "absolute_error_k_vs_strict_pred": 2.75, "score_vs_pred_strict": 15.56662535132074, "truth_raw_assistant_text": "The value is 287.53", "prediction_raw_text": "The value is 284.78\u0e44\u0e02\u0e21\u0e31\u0e19"}
{"id": "line_305", "truth_parsed_k": 288.51, "prediction_parsed_k_with_c_conv": 285.45, "prediction_parsed_k_strict": 285.45, "absolute_error_k_vs_c_conv_pred": 3.0600000000000023, "score_vs_c_conv_pred": 12.809182842181798, "absolute_error_k_vs_strict_pred": 3.0600000000000023, "score_vs_pred_strict": 12.809182842181798, "truth_raw_assistant_text": "The value is 288.51", "prediction_raw_text": "The value is 285.45"}
{"id": "line_306", "truth_parsed_k": 289.03, "prediction_parsed_k_with_c_conv": 284.85, "prediction_parsed_k_strict": 284.85, "absolute_error_k_vs_c_conv_pred": 4.17999999999995, "score_vs_c_conv_pred": 4.695031737987065, "absolute_error_k_vs_strict_pred": 4.17999999999995, "score_vs_pred_strict": 4.695031737987065, "truth_raw_assistant_text": "The value is 289.03", "prediction_raw_text": "The value is 284.85."}
{"id": "line_307", "truth_parsed_k": 289.29, "prediction_parsed_k_with_c_conv": 284.8, "prediction_parsed_k_strict": 284.8, "absolute_error_k_vs_c_conv_pred": 4.490000000000009, "score_vs_c_conv_pred": 2.822583086765784, "absolute_error_k_vs_strict_pred": 4.490000000000009, "score_vs_pred_strict": 2.822583086765784, "truth_raw_assistant_text": "The value is 289.29", "prediction_raw_text": "The value is 284.80"}
{"id": "line_308", "truth_parsed_k": 289.07, "prediction_parsed_k_with_c_conv": 284.98, "prediction_parsed_k_strict": 284.98, "absolute_error_k_vs_c_conv_pred": 4.089999999999975, "score_vs_c_conv_pred": 5.263959607067569, "absolute_error_k_vs_strict_pred": 4.089999999999975, "score_vs_pred_strict": 5.263959607067569, "truth_raw_assistant_text": "The value is 289.07", "prediction_raw_text": "The value is 284.98\ufa18"}
{"id": "line_309", "truth_parsed_k": 288.52, "prediction_parsed_k_with_c_conv": 286.59, "prediction_parsed_k_strict": 286.59, "absolute_error_k_vs_c_conv_pred": 1.9300000000000068, "score_vs_c_conv_pred": 24.608507944947235, "absolute_error_k_vs_strict_pred": 1.9300000000000068, "score_vs_pred_strict": 24.608507944947235, "truth_raw_assistant_text": "The value is 288.52", "prediction_raw_text": "The value is 286.59\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19\u0e41\u0e1b\u0e25\u0e07"}
{"id": "line_310", "truth_parsed_k": 287.77, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 2.829999999999984, "score_vs_c_conv_pred": 14.827553209572864, "absolute_error_k_vs_strict_pred": 2.829999999999984, "score_vs_pred_strict": 14.827553209572864, "truth_raw_assistant_text": "The value is 287.77", "prediction_raw_text": "The value is 284.94\u0639\u064e"}
{"id": "line_311", "truth_parsed_k": 286.83, "prediction_parsed_k_with_c_conv": 285.62, "prediction_parsed_k_strict": 285.62, "absolute_error_k_vs_c_conv_pred": 1.2099999999999795, "score_vs_c_conv_pred": 36.22386233549256, "absolute_error_k_vs_strict_pred": 1.2099999999999795, "score_vs_pred_strict": 36.22386233549256, "truth_raw_assistant_text": "The value is 286.83", "prediction_raw_text": "The value is 285.62\u0e04\u0e48\u0e2d\u0e19\u0e02\u0e49\u0e32\u0e07"}
{"id": "line_312", "truth_parsed_k": 286.13, "prediction_parsed_k_with_c_conv": 285.55, "prediction_parsed_k_strict": 285.55, "absolute_error_k_vs_c_conv_pred": 0.5799999999999841, "score_vs_c_conv_pred": 53.417248017885676, "absolute_error_k_vs_strict_pred": 0.5799999999999841, "score_vs_pred_strict": 53.417248017885676, "truth_raw_assistant_text": "The value is 286.13", "prediction_raw_text": "The value is 285.55."}
{"id": "line_313", "truth_parsed_k": 286.12, "prediction_parsed_k_with_c_conv": 286.84, "prediction_parsed_k_strict": 286.84, "absolute_error_k_vs_c_conv_pred": 0.7199999999999704, "score_vs_c_conv_pred": 48.539496319757234, "absolute_error_k_vs_strict_pred": 0.7199999999999704, "score_vs_pred_strict": 48.539496319757234, "truth_raw_assistant_text": "The value is 286.12", "prediction_raw_text": "The value is 286.84\u0e2a\u0e21\u0e31\u0e04\u0e23\u0e2a\u0e21\u0e32\u0e0a\u0e34\u0e01"}
{"id": "line_314", "truth_parsed_k": 286.3, "prediction_parsed_k_with_c_conv": 285.52, "prediction_parsed_k_strict": 285.52, "absolute_error_k_vs_c_conv_pred": 0.7800000000000296, "score_vs_c_conv_pred": 46.69226365237833, "absolute_error_k_vs_strict_pred": 0.7800000000000296, "score_vs_pred_strict": 46.69226365237833, "truth_raw_assistant_text": "The value is 286.30", "prediction_raw_text": "The value is 285.52\u0e04\u0e27\u0e32\u0e21\u0e40\u0e1b\u0e47\u0e19"}
{"id": "line_315", "truth_parsed_k": 286.75, "prediction_parsed_k_with_c_conv": 285.97, "prediction_parsed_k_strict": 285.97, "absolute_error_k_vs_c_conv_pred": 0.7799999999999727, "score_vs_c_conv_pred": 46.69226365238002, "absolute_error_k_vs_strict_pred": 0.7799999999999727, "score_vs_pred_strict": 46.69226365238002, "truth_raw_assistant_text": "The value is 286.75", "prediction_raw_text": "The value is 285.97\u0e08\u0e31"}
{"id": "line_316", "truth_parsed_k": 287.71, "prediction_parsed_k_with_c_conv": 287.35, "prediction_parsed_k_strict": 287.35, "absolute_error_k_vs_c_conv_pred": 0.3599999999999568, "score_vs_c_conv_pred": 63.48973093072732, "absolute_error_k_vs_strict_pred": 0.3599999999999568, "score_vs_pred_strict": 63.48973093072732, "truth_raw_assistant_text": "The value is 287.71", "prediction_raw_text": "The value is 287.35\u0e44\u0e1b\u0e41\u0e25\u0e49\u0e27"}
{"id": "line_317", "truth_parsed_k": 288.66, "prediction_parsed_k_with_c_conv": 286.78, "prediction_parsed_k_strict": 286.78, "absolute_error_k_vs_c_conv_pred": 1.8800000000000523, "score_vs_c_conv_pred": 25.27179888820076, "absolute_error_k_vs_strict_pred": 1.8800000000000523, "score_vs_pred_strict": 25.27179888820076, "truth_raw_assistant_text": "The value is 288.66", "prediction_raw_text": "The value is 286.78."}
{"id": "line_318", "truth_parsed_k": 289.28, "prediction_parsed_k_with_c_conv": 284.81, "prediction_parsed_k_strict": 284.81, "absolute_error_k_vs_c_conv_pred": 4.46999999999997, "score_vs_c_conv_pred": 2.939535499945378, "absolute_error_k_vs_strict_pred": 4.46999999999997, "score_vs_pred_strict": 2.939535499945378, "truth_raw_assistant_text": "The value is 289.28", "prediction_raw_text": "The value is 284.81."}
{"id": "line_319", "truth_parsed_k": 289.48, "prediction_parsed_k_with_c_conv": 285.82, "prediction_parsed_k_strict": 285.82, "absolute_error_k_vs_c_conv_pred": 3.660000000000025, "score_vs_c_conv_pred": 8.161539151279552, "absolute_error_k_vs_strict_pred": 3.660000000000025, "score_vs_pred_strict": 8.161539151279552, "truth_raw_assistant_text": "The value is 289.48", "prediction_raw_text": "The value is 285.82\u0e43\u0e19\u0e02\u0e13\u0e30\u0e17\u0e35\u0e48"}
{"id": "line_320", "truth_parsed_k": 289.29, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 4.319999999999993, "score_vs_c_conv_pred": 3.8332541688072874, "absolute_error_k_vs_strict_pred": 4.319999999999993, "score_vs_pred_strict": 3.8332541688072874, "truth_raw_assistant_text": "The value is 289.29", "prediction_raw_text": "The value is 284.97\u0e21\u0e2b\u0e32\u0e27\u0e34\u0e17\u0e22\u0e32"}
{"id": "line_321", "truth_parsed_k": 288.65, "prediction_parsed_k_with_c_conv": 285.81, "prediction_parsed_k_strict": 285.81, "absolute_error_k_vs_c_conv_pred": 2.839999999999975, "score_vs_c_conv_pred": 14.73657930275375, "absolute_error_k_vs_strict_pred": 2.839999999999975, "score_vs_pred_strict": 14.73657930275375, "truth_raw_assistant_text": "The value is 288.65", "prediction_raw_text": "The value is 285.81."}
{"id": "line_322", "truth_parsed_k": 287.91, "prediction_parsed_k_with_c_conv": 285.96, "prediction_parsed_k_strict": 285.96, "absolute_error_k_vs_c_conv_pred": 1.9500000000000455, "score_vs_c_conv_pred": 24.347699602992044, "absolute_error_k_vs_strict_pred": 1.9500000000000455, "score_vs_pred_strict": 24.347699602992044, "truth_raw_assistant_text": "The value is 287.91", "prediction_raw_text": "The value is 285.96."}
{"id": "line_323", "truth_parsed_k": 287.1, "prediction_parsed_k_with_c_conv": 285.36, "prediction_parsed_k_strict": 285.36, "absolute_error_k_vs_c_conv_pred": 1.740000000000009, "score_vs_c_conv_pred": 27.22094667266837, "absolute_error_k_vs_strict_pred": 1.740000000000009, "score_vs_pred_strict": 27.22094667266837, "truth_raw_assistant_text": "The value is 287.10", "prediction_raw_text": "The value is 285.36."}
{"id": "line_324", "truth_parsed_k": 286.18, "prediction_parsed_k_with_c_conv": 286.66, "prediction_parsed_k_strict": 286.66, "absolute_error_k_vs_c_conv_pred": 0.4800000000000182, "score_vs_c_conv_pred": 57.53644489985519, "absolute_error_k_vs_strict_pred": 0.4800000000000182, "score_vs_pred_strict": 57.53644489985519, "truth_raw_assistant_text": "The value is 286.18", "prediction_raw_text": "The value is 286.66\uf942"}
{"id": "line_325", "truth_parsed_k": 286.05, "prediction_parsed_k_with_c_conv": 285.5, "prediction_parsed_k_strict": 285.5, "absolute_error_k_vs_c_conv_pred": 0.5500000000000114, "score_vs_c_conv_pred": 54.588226018543985, "absolute_error_k_vs_strict_pred": 0.5500000000000114, "score_vs_pred_strict": 54.588226018543985, "truth_raw_assistant_text": "The value is 286.05", "prediction_raw_text": "The value is 285.50\u0e40\u0e2a\u0e35\u0e22"}
{"id": "line_326", "truth_parsed_k": 286.13, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 1.1899999999999977, "score_vs_c_conv_pred": 36.630334216240726, "absolute_error_k_vs_strict_pred": 1.1899999999999977, "score_vs_pred_strict": 36.630334216240726, "truth_raw_assistant_text": "The value is 286.13", "prediction_raw_text": "The value is 284.94\u0e19\u0e30\u0e04\u0e23\u0e31\u0e1a"}
{"id": "line_327", "truth_parsed_k": 286.76, "prediction_parsed_k_with_c_conv": 285.88, "prediction_parsed_k_strict": 285.88, "absolute_error_k_vs_c_conv_pred": 0.8799999999999955, "score_vs_c_conv_pred": 43.86997085960033, "absolute_error_k_vs_strict_pred": 0.8799999999999955, "score_vs_pred_strict": 43.86997085960033, "truth_raw_assistant_text": "The value is 286.76", "prediction_raw_text": "The value is 285.88\uf929"}
{"id": "line_328", "truth_parsed_k": 287.58, "prediction_parsed_k_with_c_conv": 286.3, "prediction_parsed_k_strict": 286.3, "absolute_error_k_vs_c_conv_pred": 1.2799999999999727, "score_vs_c_conv_pred": 34.84766685535886, "absolute_error_k_vs_strict_pred": 1.2799999999999727, "score_vs_pred_strict": 34.84766685535886, "truth_raw_assistant_text": "The value is 287.58", "prediction_raw_text": "The value is 286.30"}
{"id": "line_329", "truth_parsed_k": 288.74, "prediction_parsed_k_with_c_conv": 284.83, "prediction_parsed_k_strict": 284.83, "absolute_error_k_vs_c_conv_pred": 3.910000000000025, "score_vs_c_conv_pred": 6.439197058986112, "absolute_error_k_vs_strict_pred": 3.910000000000025, "score_vs_pred_strict": 6.439197058986112, "truth_raw_assistant_text": "The value is 288.74", "prediction_raw_text": "The value is 284.83\u0e27\u0e35"}
{"id": "line_330", "truth_parsed_k": 289.42, "prediction_parsed_k_with_c_conv": 284.75, "prediction_parsed_k_strict": 284.75, "absolute_error_k_vs_c_conv_pred": 4.670000000000016, "score_vs_c_conv_pred": 1.7922586526814, "absolute_error_k_vs_strict_pred": 4.670000000000016, "score_vs_pred_strict": 1.7922586526814, "truth_raw_assistant_text": "The value is 289.42", "prediction_raw_text": "The value is 284.75\u0e27\u0e31\u0e12\u0e19\u0e18\u0e23\u0e23\u0e21"}
{"id": "line_331", "truth_parsed_k": 289.77, "prediction_parsed_k_with_c_conv": 285.99, "prediction_parsed_k_strict": 285.99, "absolute_error_k_vs_c_conv_pred": 3.7799999999999727, "score_vs_c_conv_pred": 7.321057376815887, "absolute_error_k_vs_strict_pred": 3.7799999999999727, "score_vs_pred_strict": 7.321057376815887, "truth_raw_assistant_text": "The value is 289.77", "prediction_raw_text": "The value is 285.99\u0e40\u0e2a\u0e49\u0e19\u0e17\u0e32\u0e07"}
{"id": "line_332", "truth_parsed_k": 289.43, "prediction_parsed_k_with_c_conv": 285.9, "prediction_parsed_k_strict": 285.9, "absolute_error_k_vs_c_conv_pred": 3.5300000000000296, "score_vs_c_conv_pred": 9.102675961334395, "absolute_error_k_vs_strict_pred": 3.5300000000000296, "score_vs_pred_strict": 9.102675961334395, "truth_raw_assistant_text": "The value is 289.43", "prediction_raw_text": "The value is 285.90\u0e27\u0e48\u0e32\u0e08\u0e30"}
{"id": "line_333", "truth_parsed_k": 288.78, "prediction_parsed_k_with_c_conv": 284.88, "prediction_parsed_k_strict": 284.88, "absolute_error_k_vs_c_conv_pred": 3.8999999999999773, "score_vs_c_conv_pred": 6.50601657418135, "absolute_error_k_vs_strict_pred": 3.8999999999999773, "score_vs_pred_strict": 6.50601657418135, "truth_raw_assistant_text": "The value is 288.78", "prediction_raw_text": "The value is 284.88\u0e08\u0e36\u0e07"}
{"id": "line_334", "truth_parsed_k": 287.93, "prediction_parsed_k_with_c_conv": 285.71, "prediction_parsed_k_strict": 285.71, "absolute_error_k_vs_c_conv_pred": 2.2200000000000273, "score_vs_c_conv_pred": 21.05372156733405, "absolute_error_k_vs_strict_pred": 2.2200000000000273, "score_vs_pred_strict": 21.05372156733405, "truth_raw_assistant_text": "The value is 287.93", "prediction_raw_text": "The value is 285.71"}
{"id": "line_335", "truth_parsed_k": 287.2, "prediction_parsed_k_with_c_conv": 285.66, "prediction_parsed_k_strict": 285.66, "absolute_error_k_vs_c_conv_pred": 1.5399999999999636, "score_vs_c_conv_pred": 30.27557783710595, "absolute_error_k_vs_strict_pred": 1.5399999999999636, "score_vs_pred_strict": 30.27557783710595, "truth_raw_assistant_text": "The value is 287.20", "prediction_raw_text": "The value is 285.66."}
{"id": "line_336", "truth_parsed_k": 286.56, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 1.740000000000009, "score_vs_c_conv_pred": 27.22094667266837, "absolute_error_k_vs_strict_pred": 1.740000000000009, "score_vs_pred_strict": 27.22094667266837, "truth_raw_assistant_text": "The value is 286.56", "prediction_raw_text": "The value is 284.82\u0e1d\u0e23\u0e31\u0e48\u0e07"}
{"id": "line_337", "truth_parsed_k": 285.93, "prediction_parsed_k_with_c_conv": 285.56, "prediction_parsed_k_strict": 285.56, "absolute_error_k_vs_c_conv_pred": 0.37000000000000455, "score_vs_c_conv_pred": 62.940155716763115, "absolute_error_k_vs_strict_pred": 0.37000000000000455, "score_vs_pred_strict": 62.940155716763115, "truth_raw_assistant_text": "The value is 285.93", "prediction_raw_text": "The value is 285.56\u064a\u064e\u0627"}
{"id": "line_338", "truth_parsed_k": 285.97, "prediction_parsed_k_with_c_conv": 284.73, "prediction_parsed_k_strict": 284.73, "absolute_error_k_vs_c_conv_pred": 1.240000000000009, "score_vs_c_conv_pred": 35.625432134447486, "absolute_error_k_vs_strict_pred": 1.240000000000009, "score_vs_pred_strict": 35.625432134447486, "truth_raw_assistant_text": "The value is 285.97", "prediction_raw_text": "The value is 284.73"}
{"id": "line_339", "truth_parsed_k": 286.65, "prediction_parsed_k_with_c_conv": 285.5, "prediction_parsed_k_strict": 285.5, "absolute_error_k_vs_c_conv_pred": 1.1499999999999773, "score_vs_c_conv_pred": 37.46216099823096, "absolute_error_k_vs_strict_pred": 1.1499999999999773, "score_vs_pred_strict": 37.46216099823096, "truth_raw_assistant_text": "The value is 286.65", "prediction_raw_text": "The value is 285.50"}
{"id": "line_340", "truth_parsed_k": 287.68, "prediction_parsed_k_with_c_conv": 285.66, "prediction_parsed_k_strict": 285.66, "absolute_error_k_vs_c_conv_pred": 2.019999999999982, "score_vs_c_conv_pred": 23.454263361515448, "absolute_error_k_vs_strict_pred": 2.019999999999982, "score_vs_pred_strict": 23.454263361515448, "truth_raw_assistant_text": "The value is 287.68", "prediction_raw_text": "The value is 285.66"}
{"id": "line_341", "truth_parsed_k": 288.49, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 3.5500000000000114, "score_vs_c_conv_pred": 8.955727434012662, "absolute_error_k_vs_strict_pred": 3.5500000000000114, "score_vs_pred_strict": 8.955727434012662, "truth_raw_assistant_text": "The value is 288.49", "prediction_raw_text": "The value is 284.94."}
{"id": "line_342", "truth_parsed_k": 288.96, "prediction_parsed_k_with_c_conv": 286.87, "prediction_parsed_k_strict": 286.87, "absolute_error_k_vs_c_conv_pred": 2.089999999999975, "score_vs_c_conv_pred": 22.589520452523903, "absolute_error_k_vs_strict_pred": 2.089999999999975, "score_vs_pred_strict": 22.589520452523903, "truth_raw_assistant_text": "The value is 288.96", "prediction_raw_text": "The value is 286.87\u0e19\u0e49\u0e33\u0e15\u0e32"}
{"id": "line_343", "truth_parsed_k": 289.32, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 4.5, "score_vs_c_conv_pred": 2.764296806852773, "absolute_error_k_vs_strict_pred": 4.5, "score_vs_pred_strict": 2.764296806852773, "truth_raw_assistant_text": "The value is 289.32", "prediction_raw_text": "The value is 284.82\u0e01\u0e48\u0e2d\u0e2a\u0e23\u0e49\u0e32\u0e07"}
{"id": "line_344", "truth_parsed_k": 289.06, "prediction_parsed_k_with_c_conv": 284.72, "prediction_parsed_k_strict": 284.72, "absolute_error_k_vs_c_conv_pred": 4.339999999999975, "score_vs_c_conv_pred": 3.7123639509267226, "absolute_error_k_vs_strict_pred": 4.339999999999975, "score_vs_pred_strict": 3.7123639509267226, "truth_raw_assistant_text": "The value is 289.06", "prediction_raw_text": "The value is 284.72\u0e0b\u0e4c"}
{"id": "line_345", "truth_parsed_k": 288.53, "prediction_parsed_k_with_c_conv": 285.76, "prediction_parsed_k_strict": 285.76, "absolute_error_k_vs_c_conv_pred": 2.769999999999982, "score_vs_c_conv_pred": 15.379946955126556, "absolute_error_k_vs_strict_pred": 2.769999999999982, "score_vs_pred_strict": 15.379946955126556, "truth_raw_assistant_text": "The value is 288.53", "prediction_raw_text": "The value is 285.76"}
{"id": "line_346", "truth_parsed_k": 287.73, "prediction_parsed_k_with_c_conv": 286.81, "prediction_parsed_k_strict": 286.81, "absolute_error_k_vs_c_conv_pred": 0.9200000000000159, "score_vs_c_conv_pred": 42.818978898095395, "absolute_error_k_vs_strict_pred": 0.9200000000000159, "score_vs_pred_strict": 42.818978898095395, "truth_raw_assistant_text": "The value is 287.73", "prediction_raw_text": "The value is 286.81\u0e08\u0e30\u0e40\u0e1b\u0e47\u0e19"}
{"id": "line_347", "truth_parsed_k": 286.82, "prediction_parsed_k_with_c_conv": 285.84, "prediction_parsed_k_strict": 285.84, "absolute_error_k_vs_c_conv_pred": 0.9800000000000182, "score_vs_c_conv_pred": 41.315616406399116, "absolute_error_k_vs_strict_pred": 0.9800000000000182, "score_vs_pred_strict": 41.315616406399116, "truth_raw_assistant_text": "The value is 286.82", "prediction_raw_text": "The value is 285.84."}
{"id": "line_348", "truth_parsed_k": 286.24, "prediction_parsed_k_with_c_conv": 286.25, "prediction_parsed_k_strict": 286.25, "absolute_error_k_vs_c_conv_pred": 0.009999999999990905, "score_vs_c_conv_pred": 97.92757378196347, "absolute_error_k_vs_strict_pred": 0.009999999999990905, "score_vs_pred_strict": 97.92757378196347, "truth_raw_assistant_text": "The value is 286.24", "prediction_raw_text": "The value is 286.25\u0e1e\u0e23\u0e35\u0e40\u0e21\u0e35\u0e22\u0e23\u0e4c\u0e25\u0e35\u0e01"}
{"id": "line_349", "truth_parsed_k": 285.74, "prediction_parsed_k_with_c_conv": 285.98, "prediction_parsed_k_strict": 285.98, "absolute_error_k_vs_c_conv_pred": 0.2400000000000091, "score_vs_c_conv_pred": 71.14412762073536, "absolute_error_k_vs_strict_pred": 0.2400000000000091, "score_vs_pred_strict": 71.14412762073536, "truth_raw_assistant_text": "The value is 285.74", "prediction_raw_text": "The value is 285.98\u0e17\u0e31\u0e48\u0e27"}
{"id": "line_350", "truth_parsed_k": 285.99, "prediction_parsed_k_with_c_conv": 285.23, "prediction_parsed_k_strict": 285.23, "absolute_error_k_vs_c_conv_pred": 0.7599999999999909, "score_vs_c_conv_pred": 47.29403731640398, "absolute_error_k_vs_strict_pred": 0.7599999999999909, "score_vs_pred_strict": 47.29403731640398, "truth_raw_assistant_text": "The value is 285.99", "prediction_raw_text": "The value is 285.23."}
{"id": "line_351", "truth_parsed_k": 286.43, "prediction_parsed_k_with_c_conv": 284.99, "prediction_parsed_k_strict": 284.99, "absolute_error_k_vs_c_conv_pred": 1.4399999999999977, "score_vs_c_conv_pred": 31.943494827208674, "absolute_error_k_vs_strict_pred": 1.4399999999999977, "score_vs_pred_strict": 31.943494827208674, "truth_raw_assistant_text": "The value is 286.43", "prediction_raw_text": "The value is 284.99\u0e1a\u0e38\u0e04\u0e04\u0e25"}
{"id": "line_352", "truth_parsed_k": 287.5, "prediction_parsed_k_with_c_conv": 285.54, "prediction_parsed_k_strict": 285.54, "absolute_error_k_vs_c_conv_pred": 1.9599999999999795, "score_vs_c_conv_pred": 24.218236609364517, "absolute_error_k_vs_strict_pred": 1.9599999999999795, "score_vs_pred_strict": 24.218236609364517, "truth_raw_assistant_text": "The value is 287.50", "prediction_raw_text": "The value is 285.54."}
{"id": "line_353", "truth_parsed_k": 288.46, "prediction_parsed_k_with_c_conv": 284.91, "prediction_parsed_k_strict": 284.91, "absolute_error_k_vs_c_conv_pred": 3.5499999999999545, "score_vs_c_conv_pred": 8.955727434013083, "absolute_error_k_vs_strict_pred": 3.5499999999999545, "score_vs_pred_strict": 8.955727434013083, "truth_raw_assistant_text": "The value is 288.46", "prediction_raw_text": "The value is 284.91\uf940"}
{"id": "line_354", "truth_parsed_k": 288.88, "prediction_parsed_k_with_c_conv": 285.67, "prediction_parsed_k_strict": 285.67, "absolute_error_k_vs_c_conv_pred": 3.2099999999999795, "score_vs_c_conv_pred": 11.569934228803058, "absolute_error_k_vs_strict_pred": 3.2099999999999795, "score_vs_pred_strict": 11.569934228803058, "truth_raw_assistant_text": "The value is 288.88", "prediction_raw_text": "The value is 285.67\u0e0a\u0e39"}
{"id": "line_355", "truth_parsed_k": 289.32, "prediction_parsed_k_with_c_conv": 284.85, "prediction_parsed_k_strict": 284.85, "absolute_error_k_vs_c_conv_pred": 4.46999999999997, "score_vs_c_conv_pred": 2.939535499945378, "absolute_error_k_vs_strict_pred": 4.46999999999997, "score_vs_pred_strict": 2.939535499945378, "truth_raw_assistant_text": "The value is 289.32", "prediction_raw_text": "The value is 284.85\u0e17\u0e31\u0e27"}
{"id": "line_356", "truth_parsed_k": 289.01, "prediction_parsed_k_with_c_conv": 286.71, "prediction_parsed_k_strict": 286.71, "absolute_error_k_vs_c_conv_pred": 2.3000000000000114, "score_vs_c_conv_pred": 20.150383167307574, "absolute_error_k_vs_strict_pred": 2.3000000000000114, "score_vs_pred_strict": 20.150383167307574, "truth_raw_assistant_text": "The value is 289.01", "prediction_raw_text": "The value is 286.71\u1f7d"}
{"id": "line_357", "truth_parsed_k": 288.45, "prediction_parsed_k_with_c_conv": 285.62, "prediction_parsed_k_strict": 285.62, "absolute_error_k_vs_c_conv_pred": 2.829999999999984, "score_vs_c_conv_pred": 14.827553209572864, "absolute_error_k_vs_strict_pred": 2.829999999999984, "score_vs_pred_strict": 14.827553209572864, "truth_raw_assistant_text": "The value is 288.45", "prediction_raw_text": "The value is 285.62."}
{"id": "line_358", "truth_parsed_k": 287.66, "prediction_parsed_k_with_c_conv": 285.89, "prediction_parsed_k_strict": 285.89, "absolute_error_k_vs_c_conv_pred": 1.7700000000000387, "score_vs_c_conv_pred": 26.79123142406169, "absolute_error_k_vs_strict_pred": 1.7700000000000387, "score_vs_pred_strict": 26.79123142406169, "truth_raw_assistant_text": "The value is 287.66", "prediction_raw_text": "The value is 285.89."}
{"id": "line_359", "truth_parsed_k": 286.79, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 1.8199999999999932, "score_vs_c_conv_pred": 26.08993525777755, "absolute_error_k_vs_strict_pred": 1.8199999999999932, "score_vs_pred_strict": 26.08993525777755, "truth_raw_assistant_text": "The value is 286.79", "prediction_raw_text": "The value is 284.97\u0e2b\u0e49\u0e2d\u0e07"}
{"id": "line_360", "truth_parsed_k": 286.18, "prediction_parsed_k_with_c_conv": 284.66, "prediction_parsed_k_strict": 284.66, "absolute_error_k_vs_c_conv_pred": 1.5199999999999818, "score_vs_c_conv_pred": 30.600998725619522, "absolute_error_k_vs_strict_pred": 1.5199999999999818, "score_vs_pred_strict": 30.600998725619522, "truth_raw_assistant_text": "The value is 286.18", "prediction_raw_text": "The value is 284.66\ufa5b"}
{"id": "line_361", "truth_parsed_k": 285.62, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 0.6499999999999773, "score_vs_c_conv_pred": 50.86807905493933, "absolute_error_k_vs_strict_pred": 0.6499999999999773, "score_vs_pred_strict": 50.86807905493933, "truth_raw_assistant_text": "The value is 285.62", "prediction_raw_text": "The value is 284.97\u0e01\u0e48\u0e2d\u0e19"}
{"id": "line_362", "truth_parsed_k": 286.04, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 1.2200000000000273, "score_vs_c_conv_pred": 36.02290430727667, "absolute_error_k_vs_strict_pred": 1.2200000000000273, "score_vs_pred_strict": 36.02290430727667, "truth_raw_assistant_text": "The value is 286.04", "prediction_raw_text": "The value is 284.82\u0e01\u0e27\u0e49\u0e32\u0e07"}
{"id": "line_363", "truth_parsed_k": 286.44, "prediction_parsed_k_with_c_conv": 285.92, "prediction_parsed_k_strict": 285.92, "absolute_error_k_vs_c_conv_pred": 0.5199999999999818, "score_vs_c_conv_pred": 55.81244822994027, "absolute_error_k_vs_strict_pred": 0.5199999999999818, "score_vs_pred_strict": 55.81244822994027, "truth_raw_assistant_text": "The value is 286.44", "prediction_raw_text": "The value is 285.92."}
{"id": "line_364", "truth_parsed_k": 287.46, "prediction_parsed_k_with_c_conv": 284.84, "prediction_parsed_k_strict": 284.84, "absolute_error_k_vs_c_conv_pred": 2.6200000000000045, "score_vs_c_conv_pred": 16.812638582158147, "absolute_error_k_vs_strict_pred": 2.6200000000000045, "score_vs_pred_strict": 16.812638582158147, "truth_raw_assistant_text": "The value is 287.46", "prediction_raw_text": "The value is 284.84\u0e01\u0e48\u0e2d\u0e2a\u0e23\u0e49\u0e32\u0e07"}
{"id": "line_365", "truth_parsed_k": 288.51, "prediction_parsed_k_with_c_conv": 284.91, "prediction_parsed_k_strict": 284.91, "absolute_error_k_vs_c_conv_pred": 3.599999999999966, "score_vs_c_conv_pred": 8.591826614963693, "absolute_error_k_vs_strict_pred": 3.599999999999966, "score_vs_pred_strict": 8.591826614963693, "truth_raw_assistant_text": "The value is 288.51", "prediction_raw_text": "The value is 284.91"}
{"id": "line_366", "truth_parsed_k": 289.15, "prediction_parsed_k_with_c_conv": 285.74, "prediction_parsed_k_strict": 285.74, "absolute_error_k_vs_c_conv_pred": 3.409999999999968, "score_vs_c_conv_pred": 10.001616206833642, "absolute_error_k_vs_strict_pred": 3.409999999999968, "score_vs_pred_strict": 10.001616206833642, "truth_raw_assistant_text": "The value is 289.15", "prediction_raw_text": "The value is 285.74."}
{"id": "line_367", "truth_parsed_k": 289.24, "prediction_parsed_k_with_c_conv": 284.6, "prediction_parsed_k_strict": 284.6, "absolute_error_k_vs_c_conv_pred": 4.639999999999986, "score_vs_c_conv_pred": 1.9612649656465808, "absolute_error_k_vs_strict_pred": 4.639999999999986, "score_vs_pred_strict": 1.9612649656465808, "truth_raw_assistant_text": "The value is 289.24", "prediction_raw_text": "The value is 284.60"}
{"id": "line_368", "truth_parsed_k": 289.18, "prediction_parsed_k_with_c_conv": 284.89, "prediction_parsed_k_strict": 284.89, "absolute_error_k_vs_c_conv_pred": 4.2900000000000205, "score_vs_c_conv_pred": 4.01561322333206, "absolute_error_k_vs_strict_pred": 4.2900000000000205, "score_vs_pred_strict": 4.01561322333206, "truth_raw_assistant_text": "The value is 289.18", "prediction_raw_text": "The value is 284.89\u0e28\u0e34"}
{"id": "line_369", "truth_parsed_k": 288.65, "prediction_parsed_k_with_c_conv": 286.94, "prediction_parsed_k_strict": 286.94, "absolute_error_k_vs_c_conv_pred": 1.7099999999999795, "score_vs_c_conv_pred": 27.657630579644565, "absolute_error_k_vs_strict_pred": 1.7099999999999795, "score_vs_pred_strict": 27.657630579644565, "truth_raw_assistant_text": "The value is 288.65", "prediction_raw_text": "The value is 286.94."}
{"id": "line_370", "truth_parsed_k": 287.84, "prediction_parsed_k_with_c_conv": 285.85, "prediction_parsed_k_strict": 285.85, "absolute_error_k_vs_c_conv_pred": 1.9899999999999523, "score_vs_c_conv_pred": 23.83354076959474, "absolute_error_k_vs_strict_pred": 1.9899999999999523, "score_vs_pred_strict": 23.83354076959474, "truth_raw_assistant_text": "The value is 287.84", "prediction_raw_text": "The value is 285.85\u0e21\u0e38\u0e21"}
{"id": "line_371", "truth_parsed_k": 286.77, "prediction_parsed_k_with_c_conv": 285.58, "prediction_parsed_k_strict": 285.58, "absolute_error_k_vs_c_conv_pred": 1.1899999999999977, "score_vs_c_conv_pred": 36.630334216240726, "absolute_error_k_vs_strict_pred": 1.1899999999999977, "score_vs_pred_strict": 36.630334216240726, "truth_raw_assistant_text": "The value is 286.77", "prediction_raw_text": "The value is 285.58"}
{"id": "line_372", "truth_parsed_k": 286.15, "prediction_parsed_k_with_c_conv": 284.99, "prediction_parsed_k_strict": 284.99, "absolute_error_k_vs_c_conv_pred": 1.1599999999999682, "score_vs_c_conv_pred": 37.25178296876349, "absolute_error_k_vs_strict_pred": 1.1599999999999682, "score_vs_pred_strict": 37.25178296876349, "truth_raw_assistant_text": "The value is 286.15", "prediction_raw_text": "The value is 284.99\u0e01\u0e32\u0e23\u0e04\u0e49\u0e32"}
{"id": "line_373", "truth_parsed_k": 285.64, "prediction_parsed_k_with_c_conv": 285.44, "prediction_parsed_k_strict": 285.44, "absolute_error_k_vs_c_conv_pred": 0.19999999999998863, "score_vs_c_conv_pred": 74.26974815987909, "absolute_error_k_vs_strict_pred": 0.19999999999998863, "score_vs_pred_strict": 74.26974815987909, "truth_raw_assistant_text": "The value is 285.64", "prediction_raw_text": "The value is 285.44\u4e09\u79cd\u804c\u4e1a"}
{"id": "line_374", "truth_parsed_k": 285.98, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 1.0400000000000205, "score_vs_c_conv_pred": 39.89176489367286, "absolute_error_k_vs_strict_pred": 1.0400000000000205, "score_vs_pred_strict": 39.89176489367286, "truth_raw_assistant_text": "The value is 285.98", "prediction_raw_text": "The value is 284.94."}
{"id": "line_375", "truth_parsed_k": 286.58, "prediction_parsed_k_with_c_conv": 285.4, "prediction_parsed_k_strict": 285.4, "absolute_error_k_vs_c_conv_pred": 1.1800000000000068, "score_vs_c_conv_pred": 36.83589438090016, "absolute_error_k_vs_strict_pred": 1.1800000000000068, "score_vs_pred_strict": 36.83589438090016, "truth_raw_assistant_text": "The value is 286.58", "prediction_raw_text": "The value is 285.40\u1fd3"}
{"id": "line_376", "truth_parsed_k": 287.55, "prediction_parsed_k_with_c_conv": 285.97, "prediction_parsed_k_strict": 285.97, "absolute_error_k_vs_c_conv_pred": 1.579999999999984, "score_vs_c_conv_pred": 29.63630150486678, "absolute_error_k_vs_strict_pred": 1.579999999999984, "score_vs_pred_strict": 29.63630150486678, "truth_raw_assistant_text": "The value is 287.55", "prediction_raw_text": "The value is 285.97\u0e01\u0e25\u0e32\u0e22\u0e40\u0e1b\u0e47\u0e19"}
{"id": "line_377", "truth_parsed_k": 288.39, "prediction_parsed_k_with_c_conv": 285.3, "prediction_parsed_k_strict": 285.3, "absolute_error_k_vs_c_conv_pred": 3.089999999999975, "score_vs_c_conv_pred": 12.55672864183105, "absolute_error_k_vs_strict_pred": 3.089999999999975, "score_vs_pred_strict": 12.55672864183105, "truth_raw_assistant_text": "The value is 288.39", "prediction_raw_text": "The value is 285.30"}
{"id": "line_378", "truth_parsed_k": 288.97, "prediction_parsed_k_with_c_conv": 285.82, "prediction_parsed_k_strict": 285.82, "absolute_error_k_vs_c_conv_pred": 3.150000000000034, "score_vs_c_conv_pred": 12.058811513375788, "absolute_error_k_vs_strict_pred": 3.150000000000034, "score_vs_pred_strict": 12.058811513375788, "truth_raw_assistant_text": "The value is 288.97", "prediction_raw_text": "The value is 285.82 \u0e18\u0e31\u0e19\u0e27\u0e32\u0e04\u0e21"}
{"id": "line_379", "truth_parsed_k": 289.29, "prediction_parsed_k_with_c_conv": 284.86, "prediction_parsed_k_strict": 284.86, "absolute_error_k_vs_c_conv_pred": 4.430000000000007, "score_vs_c_conv_pred": 3.1749752581800905, "absolute_error_k_vs_strict_pred": 4.430000000000007, "score_vs_pred_strict": 3.1749752581800905, "truth_raw_assistant_text": "The value is 289.29", "prediction_raw_text": "The value is 284.86\uf972"}
{"id": "line_380", "truth_parsed_k": 289.13, "prediction_parsed_k_with_c_conv": 285.82, "prediction_parsed_k_strict": 285.82, "absolute_error_k_vs_c_conv_pred": 3.3100000000000023, "score_vs_c_conv_pred": 10.774359354368157, "absolute_error_k_vs_strict_pred": 3.3100000000000023, "score_vs_pred_strict": 10.774359354368157, "truth_raw_assistant_text": "The value is 289.13", "prediction_raw_text": "The value is 285.82\uf967"}
{"id": "line_381", "truth_parsed_k": 288.63, "prediction_parsed_k_with_c_conv": 285.89, "prediction_parsed_k_strict": 285.89, "absolute_error_k_vs_c_conv_pred": 2.740000000000009, "score_vs_c_conv_pred": 15.660452104108114, "absolute_error_k_vs_strict_pred": 2.740000000000009, "score_vs_pred_strict": 15.660452104108114, "truth_raw_assistant_text": "The value is 288.63", "prediction_raw_text": "The value is 285.89\u0e40\u0e2a\u0e35\u0e48\u0e22"}
{"id": "line_382", "truth_parsed_k": 287.78, "prediction_parsed_k_with_c_conv": 284.99, "prediction_parsed_k_strict": 284.99, "absolute_error_k_vs_c_conv_pred": 2.7899999999999636, "score_vs_c_conv_pred": 15.194553790436993, "absolute_error_k_vs_strict_pred": 2.7899999999999636, "score_vs_pred_strict": 15.194553790436993, "truth_raw_assistant_text": "The value is 287.78", "prediction_raw_text": "The value is 284.99\u0628\u0650"}
{"id": "line_383", "truth_parsed_k": 286.85, "prediction_parsed_k_with_c_conv": 284.75, "prediction_parsed_k_strict": 284.75, "absolute_error_k_vs_c_conv_pred": 2.1000000000000227, "score_vs_c_conv_pred": 22.468221820365574, "absolute_error_k_vs_strict_pred": 2.1000000000000227, "score_vs_pred_strict": 22.468221820365574, "truth_raw_assistant_text": "The value is 286.85", "prediction_raw_text": "The value is 284.75."}
{"id": "line_384", "truth_parsed_k": 286.14, "prediction_parsed_k_with_c_conv": 284.99, "prediction_parsed_k_strict": 284.99, "absolute_error_k_vs_c_conv_pred": 1.1499999999999773, "score_vs_c_conv_pred": 37.46216099823096, "absolute_error_k_vs_strict_pred": 1.1499999999999773, "score_vs_pred_strict": 37.46216099823096, "truth_raw_assistant_text": "The value is 286.14", "prediction_raw_text": "The value is 284.99"}
{"id": "line_385", "truth_parsed_k": 285.55, "prediction_parsed_k_with_c_conv": 285.62, "prediction_parsed_k_strict": 285.62, "absolute_error_k_vs_c_conv_pred": 0.06999999999999318, "score_vs_c_conv_pred": 88.02538867500537, "absolute_error_k_vs_strict_pred": 0.06999999999999318, "score_vs_pred_strict": 88.02538867500537, "truth_raw_assistant_text": "The value is 285.55", "prediction_raw_text": "The value is 285.62\u0e42\u0e15\u0e4a\u0e30"}
{"id": "line_386", "truth_parsed_k": 285.67, "prediction_parsed_k_with_c_conv": 284.84, "prediction_parsed_k_strict": 284.84, "absolute_error_k_vs_c_conv_pred": 0.8300000000000409, "score_vs_c_conv_pred": 45.24415935239797, "absolute_error_k_vs_strict_pred": 0.8300000000000409, "score_vs_pred_strict": 45.24415935239797, "truth_raw_assistant_text": "The value is 285.67", "prediction_raw_text": "The value is 284.84\u0e04\u0e27\u0e49\u0e32"}
{"id": "line_387", "truth_parsed_k": 286.36, "prediction_parsed_k_with_c_conv": 284.81, "prediction_parsed_k_strict": 284.81, "absolute_error_k_vs_c_conv_pred": 1.5500000000000114, "score_vs_c_conv_pred": 30.11433034447373, "absolute_error_k_vs_strict_pred": 1.5500000000000114, "score_vs_pred_strict": 30.11433034447373, "truth_raw_assistant_text": "The value is 286.36", "prediction_raw_text": "The value is 284.81\u0e2d\u0e38\u0e15\u0e2a\u0e32\u0e2b"}
{"id": "line_388", "truth_parsed_k": 287.68, "prediction_parsed_k_with_c_conv": 285.57, "prediction_parsed_k_strict": 285.57, "absolute_error_k_vs_c_conv_pred": 2.1100000000000136, "score_vs_c_conv_pred": 22.347467130089584, "absolute_error_k_vs_strict_pred": 2.1100000000000136, "score_vs_pred_strict": 22.347467130089584, "truth_raw_assistant_text": "The value is 287.68", "prediction_raw_text": "The value is 285.57."}
{"id": "line_389", "truth_parsed_k": 288.5, "prediction_parsed_k_with_c_conv": 285.96, "prediction_parsed_k_strict": 285.96, "absolute_error_k_vs_c_conv_pred": 2.5400000000000205, "score_vs_c_conv_pred": 17.609095923295158, "absolute_error_k_vs_strict_pred": 2.5400000000000205, "score_vs_pred_strict": 17.609095923295158, "truth_raw_assistant_text": "The value is 288.50", "prediction_raw_text": "The value is 285.96."}
{"id": "line_390", "truth_parsed_k": 289.17, "prediction_parsed_k_with_c_conv": 285.74, "prediction_parsed_k_strict": 285.74, "absolute_error_k_vs_c_conv_pred": 3.430000000000007, "score_vs_c_conv_pred": 9.849693373363877, "absolute_error_k_vs_strict_pred": 3.430000000000007, "score_vs_pred_strict": 9.849693373363877, "truth_raw_assistant_text": "The value is 289.17", "prediction_raw_text": "The value is 285.74."}
{"id": "line_391", "truth_parsed_k": 289.49, "prediction_parsed_k_with_c_conv": 285.72, "prediction_parsed_k_strict": 285.72, "absolute_error_k_vs_c_conv_pred": 3.769999999999982, "score_vs_c_conv_pred": 7.390104211585157, "absolute_error_k_vs_strict_pred": 3.769999999999982, "score_vs_pred_strict": 7.390104211585157, "truth_raw_assistant_text": "The value is 289.49", "prediction_raw_text": "The value is 285.72\u0e2a\u0e34\u0e17\u0e18\u0e34\u0e4c"}
{"id": "line_392", "truth_parsed_k": 289.14, "prediction_parsed_k_with_c_conv": 286.69, "prediction_parsed_k_strict": 286.69, "absolute_error_k_vs_c_conv_pred": 2.4499999999999886, "score_vs_c_conv_pred": 18.534203148495976, "absolute_error_k_vs_strict_pred": 2.4499999999999886, "score_vs_pred_strict": 18.534203148495976, "truth_raw_assistant_text": "The value is 289.14", "prediction_raw_text": "The value is 286.69\u0e1e\u0e19\u0e31\u0e01\u0e07\u0e32\u0e19"}
{"id": "line_393", "truth_parsed_k": 288.63, "prediction_parsed_k_with_c_conv": 285.91, "prediction_parsed_k_strict": 285.91, "absolute_error_k_vs_c_conv_pred": 2.7199999999999704, "score_vs_c_conv_pred": 15.849092110619134, "absolute_error_k_vs_strict_pred": 2.7199999999999704, "score_vs_pred_strict": 15.849092110619134, "truth_raw_assistant_text": "The value is 288.63", "prediction_raw_text": "The value is 285.91\u0e22\u0e48\u0e32\u0e19"}
{"id": "line_394", "truth_parsed_k": 287.75, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 2.7799999999999727, "score_vs_c_conv_pred": 15.287090825853355, "absolute_error_k_vs_strict_pred": 2.7799999999999727, "score_vs_pred_strict": 15.287090825853355, "truth_raw_assistant_text": "The value is 287.75", "prediction_raw_text": "The value is 284.97."}
{"id": "line_395", "truth_parsed_k": 286.98, "prediction_parsed_k_with_c_conv": 285.81, "prediction_parsed_k_strict": 285.81, "absolute_error_k_vs_c_conv_pred": 1.170000000000016, "score_vs_c_conv_pred": 37.043035793130116, "absolute_error_k_vs_strict_pred": 1.170000000000016, "score_vs_pred_strict": 37.043035793130116, "truth_raw_assistant_text": "The value is 286.98", "prediction_raw_text": "The value is 285.81."}
{"id": "line_396", "truth_parsed_k": 286.2, "prediction_parsed_k_with_c_conv": 284.77, "prediction_parsed_k_strict": 284.77, "absolute_error_k_vs_c_conv_pred": 1.4300000000000068, "score_vs_c_conv_pred": 32.11611241065043, "absolute_error_k_vs_strict_pred": 1.4300000000000068, "score_vs_pred_strict": 32.11611241065043, "truth_raw_assistant_text": "The value is 286.20", "prediction_raw_text": "The value is 284.77\u0e42\u0e17\u0e23\u0e28\u0e31\u0e1e\u0e17\u0e4c"}
{"id": "line_397", "truth_parsed_k": 286.08, "prediction_parsed_k_with_c_conv": 285.57, "prediction_parsed_k_strict": 285.57, "absolute_error_k_vs_c_conv_pred": 0.5099999999999909, "score_vs_c_conv_pred": 56.2332107093644, "absolute_error_k_vs_strict_pred": 0.5099999999999909, "score_vs_pred_strict": 56.2332107093644, "truth_raw_assistant_text": "The value is 286.08", "prediction_raw_text": "The value is 285.57\u0e04\u0e23\u0e2d\u0e1a\u0e04\u0e23\u0e31\u0e27"}
{"id": "line_398", "truth_parsed_k": 286.19, "prediction_parsed_k_with_c_conv": 285.55, "prediction_parsed_k_strict": 285.55, "absolute_error_k_vs_c_conv_pred": 0.6399999999999864, "score_vs_c_conv_pred": 51.217801513357244, "absolute_error_k_vs_strict_pred": 0.6399999999999864, "score_vs_pred_strict": 51.217801513357244, "truth_raw_assistant_text": "The value is 286.19", "prediction_raw_text": "The value is 285.55."}
{"id": "line_399", "truth_parsed_k": 286.91, "prediction_parsed_k_with_c_conv": 285.4, "prediction_parsed_k_strict": 285.4, "absolute_error_k_vs_c_conv_pred": 1.5100000000000477, "score_vs_c_conv_pred": 30.765195885619434, "absolute_error_k_vs_strict_pred": 1.5100000000000477, "score_vs_pred_strict": 30.765195885619434, "truth_raw_assistant_text": "The value is 286.91", "prediction_raw_text": "The value is 285.40\u0e2a\u0e19\u0e38"}
{"id": "line_400", "truth_parsed_k": 287.73, "prediction_parsed_k_with_c_conv": 285.57, "prediction_parsed_k_strict": 285.57, "absolute_error_k_vs_c_conv_pred": 2.160000000000025, "score_vs_c_conv_pred": 21.751685066803127, "absolute_error_k_vs_strict_pred": 2.160000000000025, "score_vs_pred_strict": 21.751685066803127, "truth_raw_assistant_text": "The value is 287.73", "prediction_raw_text": "The value is 285.57 cu\u0309"}
{"id": "line_401", "truth_parsed_k": 288.63, "prediction_parsed_k_with_c_conv": 285.95, "prediction_parsed_k_strict": 285.95, "absolute_error_k_vs_c_conv_pred": 2.680000000000007, "score_vs_c_conv_pred": 16.230383448158403, "absolute_error_k_vs_strict_pred": 2.680000000000007, "score_vs_pred_strict": 16.230383448158403, "truth_raw_assistant_text": "The value is 288.63", "prediction_raw_text": "The value is 285.95."}
{"id": "line_402", "truth_parsed_k": 289.28, "prediction_parsed_k_with_c_conv": 285.34, "prediction_parsed_k_strict": 285.34, "absolute_error_k_vs_c_conv_pred": 3.9399999999999977, "score_vs_c_conv_pred": 6.239727622422064, "absolute_error_k_vs_strict_pred": 3.9399999999999977, "score_vs_pred_strict": 6.239727622422064, "truth_raw_assistant_text": "The value is 289.28", "prediction_raw_text": "The value is 285.34."}
{"id": "line_403", "truth_parsed_k": 289.54, "prediction_parsed_k_with_c_conv": 285.79, "prediction_parsed_k_strict": 285.79, "absolute_error_k_vs_c_conv_pred": 3.75, "score_vs_c_conv_pred": 7.528731291521307, "absolute_error_k_vs_strict_pred": 3.75, "score_vs_pred_strict": 7.528731291521307, "truth_raw_assistant_text": "The value is 289.54", "prediction_raw_text": "The value is 285.79\u0e2d\u0e19\u0e38\u0e0d\u0e32\u0e15"}
{"id": "line_404", "truth_parsed_k": 289.34, "prediction_parsed_k_with_c_conv": 285.8, "prediction_parsed_k_strict": 285.8, "absolute_error_k_vs_c_conv_pred": 3.5399999999999636, "score_vs_c_conv_pred": 9.02910145980924, "absolute_error_k_vs_strict_pred": 3.5399999999999636, "score_vs_pred_strict": 9.02910145980924, "truth_raw_assistant_text": "The value is 289.34", "prediction_raw_text": "The value is 285.80"}
{"id": "line_405", "truth_parsed_k": 288.81, "prediction_parsed_k_with_c_conv": 285.38, "prediction_parsed_k_strict": 285.38, "absolute_error_k_vs_c_conv_pred": 3.430000000000007, "score_vs_c_conv_pred": 9.849693373363877, "absolute_error_k_vs_strict_pred": 3.430000000000007, "score_vs_pred_strict": 9.849693373363877, "truth_raw_assistant_text": "The value is 288.81", "prediction_raw_text": "The value is 285.38\u0e2a\u0e16\u0e32\u0e19\u0e01\u0e32\u0e23\u0e13\u0e4c"}
{"id": "line_406", "truth_parsed_k": 288.15, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 3.329999999999984, "score_vs_c_conv_pred": 10.618026544851222, "absolute_error_k_vs_strict_pred": 3.329999999999984, "score_vs_pred_strict": 10.618026544851222, "truth_raw_assistant_text": "The value is 288.15", "prediction_raw_text": "The value is 284.82\u0e16\u0e37\u0e2d\u0e40\u0e1b\u0e47\u0e19"}
{"id": "line_407", "truth_parsed_k": 287.3, "prediction_parsed_k_with_c_conv": 284.91, "prediction_parsed_k_strict": 284.91, "absolute_error_k_vs_c_conv_pred": 2.3899999999999864, "score_vs_c_conv_pred": 19.169083262319152, "absolute_error_k_vs_strict_pred": 2.3899999999999864, "score_vs_pred_strict": 19.169083262319152, "truth_raw_assistant_text": "The value is 287.30", "prediction_raw_text": "The value is 284.91\ufb2a"}
{"id": "line_408", "truth_parsed_k": 286.57, "prediction_parsed_k_with_c_conv": 284.87, "prediction_parsed_k_strict": 284.87, "absolute_error_k_vs_c_conv_pred": 1.6999999999999886, "score_vs_c_conv_pred": 27.804779857318973, "absolute_error_k_vs_strict_pred": 1.6999999999999886, "score_vs_pred_strict": 27.804779857318973, "truth_raw_assistant_text": "The value is 286.57", "prediction_raw_text": "The value is 284.87\u0625\u0650"}
{"id": "line_409", "truth_parsed_k": 286.16, "prediction_parsed_k_with_c_conv": 285.56, "prediction_parsed_k_strict": 285.56, "absolute_error_k_vs_c_conv_pred": 0.6000000000000227, "score_vs_c_conv_pred": 52.663961114064094, "absolute_error_k_vs_strict_pred": 0.6000000000000227, "score_vs_pred_strict": 52.663961114064094, "truth_raw_assistant_text": "The value is 286.16", "prediction_raw_text": "The value is 285.56."}
{"id": "line_410", "truth_parsed_k": 286.39, "prediction_parsed_k_with_c_conv": 284.63, "prediction_parsed_k_strict": 284.63, "absolute_error_k_vs_c_conv_pred": 1.759999999999991, "score_vs_c_conv_pred": 26.93370927394322, "absolute_error_k_vs_strict_pred": 1.759999999999991, "score_vs_pred_strict": 26.93370927394322, "truth_raw_assistant_text": "The value is 286.39", "prediction_raw_text": "The value is 284.63."}
{"id": "line_411", "truth_parsed_k": 286.84, "prediction_parsed_k_with_c_conv": 285.67, "prediction_parsed_k_strict": 285.67, "absolute_error_k_vs_c_conv_pred": 1.169999999999959, "score_vs_c_conv_pred": 37.04303579313131, "absolute_error_k_vs_strict_pred": 1.169999999999959, "score_vs_pred_strict": 37.04303579313131, "truth_raw_assistant_text": "The value is 286.84", "prediction_raw_text": "The value is 285.67\u0e19\u0e30\u0e04\u0e23\u0e31\u0e1a"}
{"id": "line_412", "truth_parsed_k": 287.9, "prediction_parsed_k_with_c_conv": 285.56, "prediction_parsed_k_strict": 285.56, "absolute_error_k_vs_c_conv_pred": 2.339999999999975, "score_vs_c_conv_pred": 19.709829360586617, "absolute_error_k_vs_strict_pred": 2.339999999999975, "score_vs_pred_strict": 19.709829360586617, "truth_raw_assistant_text": "The value is 287.90", "prediction_raw_text": "The value is 285.56."}
{"id": "line_413", "truth_parsed_k": 288.75, "prediction_parsed_k_with_c_conv": 284.8, "prediction_parsed_k_strict": 284.8, "absolute_error_k_vs_c_conv_pred": 3.9499999999999886, "score_vs_c_conv_pred": 6.173564810743271, "absolute_error_k_vs_strict_pred": 3.9499999999999886, "score_vs_pred_strict": 6.173564810743271, "truth_raw_assistant_text": "The value is 288.75", "prediction_raw_text": "The value is 284.80\u0e1e\u0e25\u0e31\u0e07"}
{"id": "line_414", "truth_parsed_k": 289.5, "prediction_parsed_k_with_c_conv": 285.6, "prediction_parsed_k_strict": 285.6, "absolute_error_k_vs_c_conv_pred": 3.8999999999999773, "score_vs_c_conv_pred": 6.50601657418135, "absolute_error_k_vs_strict_pred": 3.8999999999999773, "score_vs_pred_strict": 6.50601657418135, "truth_raw_assistant_text": "The value is 289.50", "prediction_raw_text": "The value is 285.60\u0e01\u0e48\u0e2d"}
{"id": "line_415", "truth_parsed_k": 289.6, "prediction_parsed_k_with_c_conv": 285.55, "prediction_parsed_k_strict": 285.55, "absolute_error_k_vs_c_conv_pred": 4.050000000000011, "score_vs_c_conv_pred": 5.520726756855332, "absolute_error_k_vs_strict_pred": 4.050000000000011, "score_vs_pred_strict": 5.520726756855332, "truth_raw_assistant_text": "The value is 289.60", "prediction_raw_text": "The value is 285.55\u0e19\u0e48\u0e32\u0e08\u0e30"}
{"id": "line_416", "truth_parsed_k": 289.35, "prediction_parsed_k_with_c_conv": 285.93, "prediction_parsed_k_strict": 285.93, "absolute_error_k_vs_c_conv_pred": 3.420000000000016, "score_vs_c_conv_pred": 9.925547651145816, "absolute_error_k_vs_strict_pred": 3.420000000000016, "score_vs_pred_strict": 9.925547651145816, "truth_raw_assistant_text": "The value is 289.35", "prediction_raw_text": "The value is 285.93\u0e40\u0e14\u0e37\u0e2d\u0e19"}
{"id": "line_417", "truth_parsed_k": 288.77, "prediction_parsed_k_with_c_conv": 284.89, "prediction_parsed_k_strict": 284.89, "absolute_error_k_vs_c_conv_pred": 3.8799999999999955, "score_vs_c_conv_pred": 6.640155087201249, "absolute_error_k_vs_strict_pred": 3.8799999999999955, "score_vs_pred_strict": 6.640155087201249, "truth_raw_assistant_text": "The value is 288.77", "prediction_raw_text": "The value is 284.89."}
{"id": "line_418", "truth_parsed_k": 288.05, "prediction_parsed_k_with_c_conv": 284.57, "prediction_parsed_k_strict": 284.57, "absolute_error_k_vs_c_conv_pred": 3.480000000000018, "score_vs_c_conv_pred": 9.473594371356187, "absolute_error_k_vs_strict_pred": 3.480000000000018, "score_vs_pred_strict": 9.473594371356187, "truth_raw_assistant_text": "The value is 288.05", "prediction_raw_text": "The value is 284.57"}
{"id": "line_419", "truth_parsed_k": 287.17, "prediction_parsed_k_with_c_conv": 284.95, "prediction_parsed_k_strict": 284.95, "absolute_error_k_vs_c_conv_pred": 2.2200000000000273, "score_vs_c_conv_pred": 21.05372156733405, "absolute_error_k_vs_strict_pred": 2.2200000000000273, "score_vs_pred_strict": 21.05372156733405, "truth_raw_assistant_text": "The value is 287.17", "prediction_raw_text": "The value is 284.95"}
{"id": "line_420", "truth_parsed_k": 286.55, "prediction_parsed_k_with_c_conv": 285.87, "prediction_parsed_k_strict": 285.87, "absolute_error_k_vs_c_conv_pred": 0.6800000000000068, "score_vs_c_conv_pred": 49.845364337598184, "absolute_error_k_vs_strict_pred": 0.6800000000000068, "score_vs_pred_strict": 49.845364337598184, "truth_raw_assistant_text": "The value is 286.55", "prediction_raw_text": "The value is 285.87\u0e1d\u0e48\u0e32\u0e22"}
{"id": "line_421", "truth_parsed_k": 286.17, "prediction_parsed_k_with_c_conv": 284.86, "prediction_parsed_k_strict": 284.86, "absolute_error_k_vs_c_conv_pred": 1.3100000000000023, "score_vs_c_conv_pred": 34.278738986277645, "absolute_error_k_vs_strict_pred": 1.3100000000000023, "score_vs_pred_strict": 34.278738986277645, "truth_raw_assistant_text": "The value is 286.17", "prediction_raw_text": "The value is 284.86"}
{"id": "line_422", "truth_parsed_k": 286.29, "prediction_parsed_k_with_c_conv": 284.88, "prediction_parsed_k_strict": 284.88, "absolute_error_k_vs_c_conv_pred": 1.410000000000025, "score_vs_c_conv_pred": 32.46470304951512, "absolute_error_k_vs_strict_pred": 1.410000000000025, "score_vs_pred_strict": 32.46470304951512, "truth_raw_assistant_text": "The value is 286.29", "prediction_raw_text": "The value is 284.88"}
{"id": "line_423", "truth_parsed_k": 286.68, "prediction_parsed_k_with_c_conv": 285.32, "prediction_parsed_k_strict": 285.32, "absolute_error_k_vs_c_conv_pred": 1.3600000000000136, "score_vs_c_conv_pred": 33.35644846847218, "absolute_error_k_vs_strict_pred": 1.3600000000000136, "score_vs_pred_strict": 33.35644846847218, "truth_raw_assistant_text": "The value is 286.68", "prediction_raw_text": "The value is 285.32\ufa00"}
{"id": "line_424", "truth_parsed_k": 287.64, "prediction_parsed_k_with_c_conv": 284.87, "prediction_parsed_k_strict": 284.87, "absolute_error_k_vs_c_conv_pred": 2.769999999999982, "score_vs_c_conv_pred": 15.379946955126556, "absolute_error_k_vs_strict_pred": 2.769999999999982, "score_vs_pred_strict": 15.379946955126556, "truth_raw_assistant_text": "The value is 287.64", "prediction_raw_text": "The value is 284.87\u0e40\u0e19\u0e35\u0e22"}
{"id": "line_425", "truth_parsed_k": 288.68, "prediction_parsed_k_with_c_conv": 285.92, "prediction_parsed_k_strict": 285.92, "absolute_error_k_vs_c_conv_pred": 2.759999999999991, "score_vs_c_conv_pred": 15.473124386525205, "absolute_error_k_vs_strict_pred": 2.759999999999991, "score_vs_pred_strict": 15.473124386525205, "truth_raw_assistant_text": "The value is 288.68", "prediction_raw_text": "The value is 285.92\u0e40\u0e22\u0e37\u0e2d\u0e19"}
{"id": "line_426", "truth_parsed_k": 289.3, "prediction_parsed_k_with_c_conv": 285.98, "prediction_parsed_k_strict": 285.98, "absolute_error_k_vs_c_conv_pred": 3.319999999999993, "score_vs_c_conv_pred": 10.696079500386857, "absolute_error_k_vs_strict_pred": 3.319999999999993, "score_vs_pred_strict": 10.696079500386857, "truth_raw_assistant_text": "The value is 289.30", "prediction_raw_text": "The value is 285.98 \u0e1c\u0e39\u0e49"}
{"id": "line_427", "truth_parsed_k": 289.57, "prediction_parsed_k_with_c_conv": 286.51, "prediction_parsed_k_strict": 286.51, "absolute_error_k_vs_c_conv_pred": 3.0600000000000023, "score_vs_c_conv_pred": 12.809182842181798, "absolute_error_k_vs_strict_pred": 3.0600000000000023, "score_vs_pred_strict": 12.809182842181798, "truth_raw_assistant_text": "The value is 289.57", "prediction_raw_text": "The value is 286.51"}
{"id": "line_428", "truth_parsed_k": 289.32, "prediction_parsed_k_with_c_conv": 285.44, "prediction_parsed_k_strict": 285.44, "absolute_error_k_vs_c_conv_pred": 3.8799999999999955, "score_vs_c_conv_pred": 6.640155087201249, "absolute_error_k_vs_strict_pred": 3.8799999999999955, "score_vs_pred_strict": 6.640155087201249, "truth_raw_assistant_text": "The value is 289.32", "prediction_raw_text": "The value is 285.44\u0e22\u0e31\u0e07\u0e44"}
{"id": "line_429", "truth_parsed_k": 288.86, "prediction_parsed_k_with_c_conv": 284.87, "prediction_parsed_k_strict": 284.87, "absolute_error_k_vs_c_conv_pred": 3.990000000000009, "score_vs_c_conv_pred": 5.910527302361723, "absolute_error_k_vs_strict_pred": 3.990000000000009, "score_vs_pred_strict": 5.910527302361723, "truth_raw_assistant_text": "The value is 288.86", "prediction_raw_text": "The value is 284.87\u0e0d\u0e35\u0e48"}
{"id": "line_430", "truth_parsed_k": 288.02, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 3.079999999999984, "score_vs_c_conv_pred": 12.640617341470882, "absolute_error_k_vs_strict_pred": 3.079999999999984, "score_vs_pred_strict": 12.640617341470882, "truth_raw_assistant_text": "The value is 288.02", "prediction_raw_text": "The value is 284.94."}
{"id": "line_431", "truth_parsed_k": 287.11, "prediction_parsed_k_with_c_conv": 286.75, "prediction_parsed_k_strict": 286.75, "absolute_error_k_vs_c_conv_pred": 0.36000000000001364, "score_vs_c_conv_pred": 63.489730930724164, "absolute_error_k_vs_strict_pred": 0.36000000000001364, "score_vs_pred_strict": 63.489730930724164, "truth_raw_assistant_text": "The value is 287.11", "prediction_raw_text": "The value is 286.75."}
{"id": "line_432", "truth_parsed_k": 286.42, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 1.4800000000000182, "score_vs_c_conv_pred": 31.263881448592514, "absolute_error_k_vs_strict_pred": 1.4800000000000182, "score_vs_pred_strict": 31.263881448592514, "truth_raw_assistant_text": "The value is 286.42", "prediction_raw_text": "The value is 284.94."}
{"id": "line_433", "truth_parsed_k": 286.12, "prediction_parsed_k_with_c_conv": 284.76, "prediction_parsed_k_strict": 284.76, "absolute_error_k_vs_c_conv_pred": 1.3600000000000136, "score_vs_c_conv_pred": 33.35644846847218, "absolute_error_k_vs_strict_pred": 1.3600000000000136, "score_vs_pred_strict": 33.35644846847218, "truth_raw_assistant_text": "The value is 286.12", "prediction_raw_text": "The value is 284.76\u0e23\u0e39\u0e49\u0e08\u0e31\u0e01"}
{"id": "line_434", "truth_parsed_k": 286.24, "prediction_parsed_k_with_c_conv": 285.64, "prediction_parsed_k_strict": 285.64, "absolute_error_k_vs_c_conv_pred": 0.6000000000000227, "score_vs_c_conv_pred": 52.663961114064094, "absolute_error_k_vs_strict_pred": 0.6000000000000227, "score_vs_pred_strict": 52.663961114064094, "truth_raw_assistant_text": "The value is 286.24", "prediction_raw_text": "The value is 285.64"}
{"id": "line_435", "truth_parsed_k": 286.67, "prediction_parsed_k_with_c_conv": 285.39, "prediction_parsed_k_strict": 285.39, "absolute_error_k_vs_c_conv_pred": 1.2800000000000296, "score_vs_c_conv_pred": 34.84766685535777, "absolute_error_k_vs_strict_pred": 1.2800000000000296, "score_vs_pred_strict": 34.84766685535777, "truth_raw_assistant_text": "The value is 286.67", "prediction_raw_text": "The value is 285.39"}
{"id": "line_436", "truth_parsed_k": 287.75, "prediction_parsed_k_with_c_conv": 284.72, "prediction_parsed_k_strict": 284.72, "absolute_error_k_vs_c_conv_pred": 3.0299999999999727, "score_vs_c_conv_pred": 13.064026234499714, "absolute_error_k_vs_strict_pred": 3.0299999999999727, "score_vs_pred_strict": 13.064026234499714, "truth_raw_assistant_text": "The value is 287.75", "prediction_raw_text": "The value is 284.72\u05d5\ufffd"}
{"id": "line_437", "truth_parsed_k": 288.76, "prediction_parsed_k_with_c_conv": 285.77, "prediction_parsed_k_strict": 285.77, "absolute_error_k_vs_c_conv_pred": 2.990000000000009, "score_vs_c_conv_pred": 13.40761304322623, "absolute_error_k_vs_strict_pred": 2.990000000000009, "score_vs_pred_strict": 13.40761304322623, "truth_raw_assistant_text": "The value is 288.76", "prediction_raw_text": "The value is 285.77\u0e01\u0e38\u0e25"}
{"id": "line_438", "truth_parsed_k": 289.21, "prediction_parsed_k_with_c_conv": 287.78, "prediction_parsed_k_strict": 287.78, "absolute_error_k_vs_c_conv_pred": 1.4300000000000068, "score_vs_c_conv_pred": 32.11611241065043, "absolute_error_k_vs_strict_pred": 1.4300000000000068, "score_vs_pred_strict": 32.11611241065043, "truth_raw_assistant_text": "The value is 289.21", "prediction_raw_text": "The value is 287.78\u0e23\u0e30\u0e2b\u0e27\u0e48\u0e32\u0e07"}
{"id": "line_439", "truth_parsed_k": 289.49, "prediction_parsed_k_with_c_conv": 285.88, "prediction_parsed_k_strict": 285.88, "absolute_error_k_vs_c_conv_pred": 3.6100000000000136, "score_vs_c_conv_pred": 8.519632869827898, "absolute_error_k_vs_strict_pred": 3.6100000000000136, "score_vs_pred_strict": 8.519632869827898, "truth_raw_assistant_text": "The value is 289.49", "prediction_raw_text": "The value is 285.88\uf96c"}
{"id": "line_440", "truth_parsed_k": 289.41, "prediction_parsed_k_with_c_conv": 284.58, "prediction_parsed_k_strict": 284.58, "absolute_error_k_vs_c_conv_pred": 4.830000000000041, "score_vs_c_conv_pred": 0.9083803043695826, "absolute_error_k_vs_strict_pred": 4.830000000000041, "score_vs_pred_strict": 0.9083803043695826, "truth_raw_assistant_text": "The value is 289.41", "prediction_raw_text": "The value is 284.58\u0e27\u0e48\u0e32\u0e08\u0e30"}
{"id": "line_441", "truth_parsed_k": 288.73, "prediction_parsed_k_with_c_conv": 284.89, "prediction_parsed_k_strict": 284.89, "absolute_error_k_vs_c_conv_pred": 3.840000000000032, "score_vs_c_conv_pred": 6.910453475200773, "absolute_error_k_vs_strict_pred": 3.840000000000032, "score_vs_pred_strict": 6.910453475200773, "truth_raw_assistant_text": "The value is 288.73", "prediction_raw_text": "The value is 284.89\uf906"}
{"id": "line_442", "truth_parsed_k": 288.02, "prediction_parsed_k_with_c_conv": 285.99, "prediction_parsed_k_strict": 285.99, "absolute_error_k_vs_c_conv_pred": 2.0299999999999727, "score_vs_c_conv_pred": 23.329015457551794, "absolute_error_k_vs_strict_pred": 2.0299999999999727, "score_vs_pred_strict": 23.329015457551794, "truth_raw_assistant_text": "The value is 288.02", "prediction_raw_text": "The value is 285.99\u0e42\u0e23\u0e07\u0e40\u0e23\u0e35\u0e22\u0e19"}
{"id": "line_443", "truth_parsed_k": 287.2, "prediction_parsed_k_with_c_conv": 285.95, "prediction_parsed_k_strict": 285.95, "absolute_error_k_vs_c_conv_pred": 1.25, "score_vs_c_conv_pred": 35.42887468651077, "absolute_error_k_vs_strict_pred": 1.25, "score_vs_pred_strict": 35.42887468651077, "truth_raw_assistant_text": "The value is 287.20", "prediction_raw_text": "The value is 285.95"}
{"id": "line_444", "truth_parsed_k": 286.54, "prediction_parsed_k_with_c_conv": 285.62, "prediction_parsed_k_strict": 285.62, "absolute_error_k_vs_c_conv_pred": 0.9200000000000159, "score_vs_c_conv_pred": 42.818978898095395, "absolute_error_k_vs_strict_pred": 0.9200000000000159, "score_vs_pred_strict": 42.818978898095395, "truth_raw_assistant_text": "The value is 286.54", "prediction_raw_text": "The value is 285.62\u0e2a\u0e16\u0e32\u0e1a\u0e31\u0e19"}
{"id": "line_445", "truth_parsed_k": 286.11, "prediction_parsed_k_with_c_conv": 286.21, "prediction_parsed_k_strict": 286.21, "absolute_error_k_vs_c_conv_pred": 0.0999999999999659, "score_vs_c_conv_pred": 84.17193326683945, "absolute_error_k_vs_strict_pred": 0.0999999999999659, "score_vs_pred_strict": 84.17193326683945, "truth_raw_assistant_text": "The value is 286.11", "prediction_raw_text": "The value is 286.21\u0e2b\u0e19\u0e31\u0e01"}
{"id": "line_446", "truth_parsed_k": 286.29, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 1.3199999999999932, "score_vs_c_conv_pred": 34.09173649304294, "absolute_error_k_vs_strict_pred": 1.3199999999999932, "score_vs_pred_strict": 34.09173649304294, "truth_raw_assistant_text": "The value is 286.29", "prediction_raw_text": "The value is 284.97\uf9ea"}
{"id": "line_447", "truth_parsed_k": 286.74, "prediction_parsed_k_with_c_conv": 285.80217193056, "prediction_parsed_k_strict": 285.80217193056, "absolute_error_k_vs_c_conv_pred": 0.9378280694400019, "score_vs_c_conv_pred": 42.363448221070485, "absolute_error_k_vs_strict_pred": 0.9378280694400019, "score_vs_pred_strict": 42.363448221070485, "truth_raw_assistant_text": "The value is 286.74", "prediction_raw_text": "The value is 285.80217193056."}
{"id": "line_448", "truth_parsed_k": 287.84, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 3.019999999999982, "score_vs_c_conv_pred": 13.149512816791553, "absolute_error_k_vs_strict_pred": 3.019999999999982, "score_vs_pred_strict": 13.149512816791553, "truth_raw_assistant_text": "The value is 287.84", "prediction_raw_text": "The value is 284.82\u0e22\u0e2d\u0e14\u0e40\u0e22\u0e35\u0e48\u0e22\u0e21"}
{"id": "line_449", "truth_parsed_k": 288.73, "prediction_parsed_k_with_c_conv": 284.73, "prediction_parsed_k_strict": 284.73, "absolute_error_k_vs_c_conv_pred": 4.0, "score_vs_c_conv_pred": 5.845167438219834, "absolute_error_k_vs_strict_pred": 4.0, "score_vs_pred_strict": 5.845167438219834, "truth_raw_assistant_text": "The value is 288.73", "prediction_raw_text": "The value is 284.73\u0e40\u0e17\u0e48\u0e32\u0e44"}
{"id": "line_450", "truth_parsed_k": 289.43, "prediction_parsed_k_with_c_conv": 284.86, "prediction_parsed_k_strict": 284.86, "absolute_error_k_vs_c_conv_pred": 4.569999999999993, "score_vs_c_conv_pred": 2.359787578917749, "absolute_error_k_vs_strict_pred": 4.569999999999993, "score_vs_pred_strict": 2.359787578917749, "truth_raw_assistant_text": "The value is 289.43", "prediction_raw_text": "The value is 284.86"}
{"id": "line_451", "truth_parsed_k": 289.72, "prediction_parsed_k_with_c_conv": 285.81, "prediction_parsed_k_strict": 285.81, "absolute_error_k_vs_c_conv_pred": 3.910000000000025, "score_vs_c_conv_pred": 6.439197058986112, "absolute_error_k_vs_strict_pred": 3.910000000000025, "score_vs_pred_strict": 6.439197058986112, "truth_raw_assistant_text": "The value is 289.72", "prediction_raw_text": "The value is 285.81."}
{"id": "line_452", "truth_parsed_k": 289.63, "prediction_parsed_k_with_c_conv": 285.52, "prediction_parsed_k_strict": 285.52, "absolute_error_k_vs_c_conv_pred": 4.110000000000014, "score_vs_c_conv_pred": 5.136488359299129, "absolute_error_k_vs_strict_pred": 4.110000000000014, "score_vs_pred_strict": 5.136488359299129, "truth_raw_assistant_text": "The value is 289.63", "prediction_raw_text": "The value is 285.52"}
{"id": "line_453", "truth_parsed_k": 288.84, "prediction_parsed_k_with_c_conv": 285.92, "prediction_parsed_k_strict": 285.92, "absolute_error_k_vs_c_conv_pred": 2.919999999999959, "score_vs_c_conv_pred": 14.019645088634825, "absolute_error_k_vs_strict_pred": 2.919999999999959, "score_vs_pred_strict": 14.019645088634825, "truth_raw_assistant_text": "The value is 288.84", "prediction_raw_text": "The value is 285.92"}
{"id": "line_454", "truth_parsed_k": 288.23, "prediction_parsed_k_with_c_conv": 285.66, "prediction_parsed_k_strict": 285.66, "absolute_error_k_vs_c_conv_pred": 2.569999999999993, "score_vs_c_conv_pred": 17.307657132231746, "absolute_error_k_vs_strict_pred": 2.569999999999993, "score_vs_pred_strict": 17.307657132231746, "truth_raw_assistant_text": "The value is 288.23", "prediction_raw_text": "The value is 285.66."}
{"id": "line_455", "truth_parsed_k": 287.35, "prediction_parsed_k_with_c_conv": 284.98, "prediction_parsed_k_strict": 284.98, "absolute_error_k_vs_c_conv_pred": 2.3700000000000045, "score_vs_c_conv_pred": 19.3840804188251, "absolute_error_k_vs_strict_pred": 2.3700000000000045, "score_vs_pred_strict": 19.3840804188251, "truth_raw_assistant_text": "The value is 287.35", "prediction_raw_text": "The value is 284.98"}
{"id": "line_456", "truth_parsed_k": 286.73, "prediction_parsed_k_with_c_conv": 284.96, "prediction_parsed_k_strict": 284.96, "absolute_error_k_vs_c_conv_pred": 1.7700000000000387, "score_vs_c_conv_pred": 26.79123142406169, "absolute_error_k_vs_strict_pred": 1.7700000000000387, "score_vs_pred_strict": 26.79123142406169, "truth_raw_assistant_text": "The value is 286.73", "prediction_raw_text": "The value is 284.96\u232a"}
{"id": "line_457", "truth_parsed_k": 286.42, "prediction_parsed_k_with_c_conv": 284.85, "prediction_parsed_k_strict": 284.85, "absolute_error_k_vs_c_conv_pred": 1.5699999999999932, "score_vs_c_conv_pred": 29.794703436952663, "absolute_error_k_vs_strict_pred": 1.5699999999999932, "score_vs_pred_strict": 29.794703436952663, "truth_raw_assistant_text": "The value is 286.42", "prediction_raw_text": "The value is 284.85\u0e40\u0e21\u0e47\u0e14"}
{"id": "line_458", "truth_parsed_k": 286.53, "prediction_parsed_k_with_c_conv": 285.99, "prediction_parsed_k_strict": 285.99, "absolute_error_k_vs_c_conv_pred": 0.5399999999999636, "score_vs_c_conv_pred": 54.99014767102973, "absolute_error_k_vs_strict_pred": 0.5399999999999636, "score_vs_pred_strict": 54.99014767102973, "truth_raw_assistant_text": "The value is 286.53", "prediction_raw_text": "The value is 285.99\u0e04\u0e38\u0e49\u0e21"}
{"id": "line_459", "truth_parsed_k": 287.18, "prediction_parsed_k_with_c_conv": 285.74, "prediction_parsed_k_strict": 285.74, "absolute_error_k_vs_c_conv_pred": 1.4399999999999977, "score_vs_c_conv_pred": 31.943494827208674, "absolute_error_k_vs_strict_pred": 1.4399999999999977, "score_vs_pred_strict": 31.943494827208674, "truth_raw_assistant_text": "The value is 287.18", "prediction_raw_text": "The value is 285.74."}
{"id": "line_460", "truth_parsed_k": 287.98, "prediction_parsed_k_with_c_conv": 286.92, "prediction_parsed_k_strict": 286.92, "absolute_error_k_vs_c_conv_pred": 1.0600000000000023, "score_vs_c_conv_pred": 39.433400621654805, "absolute_error_k_vs_strict_pred": 1.0600000000000023, "score_vs_pred_strict": 39.433400621654805, "truth_raw_assistant_text": "The value is 287.98", "prediction_raw_text": "The value is 286.92<|fim_middle|>"}
{"id": "line_461", "truth_parsed_k": 288.88, "prediction_parsed_k_with_c_conv": 285.88, "prediction_parsed_k_strict": 285.88, "absolute_error_k_vs_c_conv_pred": 3.0, "score_vs_c_conv_pred": 13.32130447316555, "absolute_error_k_vs_strict_pred": 3.0, "score_vs_pred_strict": 13.32130447316555, "truth_raw_assistant_text": "The value is 288.88", "prediction_raw_text": "The value is 285.88\ufa06"}
{"id": "line_462", "truth_parsed_k": 289.55, "prediction_parsed_k_with_c_conv": 285.97, "prediction_parsed_k_strict": 285.97, "absolute_error_k_vs_c_conv_pred": 3.579999999999984, "score_vs_c_conv_pred": 8.736797359755133, "absolute_error_k_vs_strict_pred": 3.579999999999984, "score_vs_pred_strict": 8.736797359755133, "truth_raw_assistant_text": "The value is 289.55", "prediction_raw_text": "The value is 285.97\ufa61"}
{"id": "line_463", "truth_parsed_k": 289.66, "prediction_parsed_k_with_c_conv": 285.57, "prediction_parsed_k_strict": 285.57, "absolute_error_k_vs_c_conv_pred": 4.090000000000032, "score_vs_c_conv_pred": 5.2639596070672035, "absolute_error_k_vs_strict_pred": 4.090000000000032, "score_vs_pred_strict": 5.2639596070672035, "truth_raw_assistant_text": "The value is 289.66", "prediction_raw_text": "The value is 285.57\u0e41\u0e17\u0e49"}
{"id": "line_464", "truth_parsed_k": 289.53, "prediction_parsed_k_with_c_conv": 286.64, "prediction_parsed_k_strict": 286.64, "absolute_error_k_vs_c_conv_pred": 2.8899999999999864, "score_vs_c_conv_pred": 14.28626361130939, "absolute_error_k_vs_strict_pred": 2.8899999999999864, "score_vs_pred_strict": 14.28626361130939, "truth_raw_assistant_text": "The value is 289.53", "prediction_raw_text": "The value is 286.64."}
{"id": "line_465", "truth_parsed_k": 288.9, "prediction_parsed_k_with_c_conv": 286.75, "prediction_parsed_k_strict": 286.75, "absolute_error_k_vs_c_conv_pred": 2.1499999999999773, "score_vs_c_conv_pred": 21.869791619321642, "absolute_error_k_vs_strict_pred": 2.1499999999999773, "score_vs_pred_strict": 21.869791619321642, "truth_raw_assistant_text": "The value is 288.90", "prediction_raw_text": "The value is 286.75."}
{"id": "line_466", "truth_parsed_k": 288.22, "prediction_parsed_k_with_c_conv": 285.78, "prediction_parsed_k_strict": 285.78, "absolute_error_k_vs_c_conv_pred": 2.4400000000000546, "score_vs_c_conv_pred": 18.638982466711663, "absolute_error_k_vs_strict_pred": 2.4400000000000546, "score_vs_pred_strict": 18.638982466711663, "truth_raw_assistant_text": "The value is 288.22", "prediction_raw_text": "The value is 285.78"}
{"id": "line_467", "truth_parsed_k": 287.36, "prediction_parsed_k_with_c_conv": 284.91, "prediction_parsed_k_strict": 284.91, "absolute_error_k_vs_c_conv_pred": 2.4499999999999886, "score_vs_c_conv_pred": 18.534203148495976, "absolute_error_k_vs_strict_pred": 2.4499999999999886, "score_vs_pred_strict": 18.534203148495976, "truth_raw_assistant_text": "The value is 287.36", "prediction_raw_text": "The value is 284.91"}
{"id": "line_468", "truth_parsed_k": 286.61, "prediction_parsed_k_with_c_conv": 285.88, "prediction_parsed_k_strict": 285.88, "absolute_error_k_vs_c_conv_pred": 0.7300000000000182, "score_vs_c_conv_pred": 48.222689715002595, "absolute_error_k_vs_strict_pred": 0.7300000000000182, "score_vs_pred_strict": 48.222689715002595, "truth_raw_assistant_text": "The value is 286.61", "prediction_raw_text": "The value is 285.88."}
{"id": "line_469", "truth_parsed_k": 286.38, "prediction_parsed_k_with_c_conv": 286.51, "prediction_parsed_k_strict": 286.51, "absolute_error_k_vs_c_conv_pred": 0.12999999999999545, "score_vs_c_conv_pred": 80.80150876164818, "absolute_error_k_vs_strict_pred": 0.12999999999999545, "score_vs_pred_strict": 80.80150876164818, "truth_raw_assistant_text": "The value is 286.38", "prediction_raw_text": "The value is 286.51\u0e27\u0e4c"}
{"id": "line_470", "truth_parsed_k": 286.64, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 1.8199999999999932, "score_vs_c_conv_pred": 26.08993525777755, "absolute_error_k_vs_strict_pred": 1.8199999999999932, "score_vs_pred_strict": 26.08993525777755, "truth_raw_assistant_text": "The value is 286.64", "prediction_raw_text": "The value is 284.82\u0e25\u0e34\u0e19"}
{"id": "line_471", "truth_parsed_k": 287.01, "prediction_parsed_k_with_c_conv": 285.62, "prediction_parsed_k_strict": 285.62, "absolute_error_k_vs_c_conv_pred": 1.3899999999999864, "score_vs_c_conv_pred": 32.817865499099476, "absolute_error_k_vs_strict_pred": 1.3899999999999864, "score_vs_pred_strict": 32.817865499099476, "truth_raw_assistant_text": "The value is 287.01", "prediction_raw_text": "The value is 285.62."}
{"id": "line_472", "truth_parsed_k": 287.96, "prediction_parsed_k_with_c_conv": 285.96, "prediction_parsed_k_strict": 285.96, "absolute_error_k_vs_c_conv_pred": 2.0, "score_vs_c_conv_pred": 23.70652048310418, "absolute_error_k_vs_strict_pred": 2.0, "score_vs_pred_strict": 23.70652048310418, "truth_raw_assistant_text": "The value is 287.96", "prediction_raw_text": "The value is 285.96\u0e40\u0e25\u0e22\u0e17\u0e35"}
{"id": "line_473", "truth_parsed_k": 288.81, "prediction_parsed_k_with_c_conv": 286.34, "prediction_parsed_k_strict": 286.34, "absolute_error_k_vs_c_conv_pred": 2.4700000000000273, "score_vs_c_conv_pred": 18.325859743192574, "absolute_error_k_vs_strict_pred": 2.4700000000000273, "score_vs_pred_strict": 18.325859743192574, "truth_raw_assistant_text": "The value is 288.81", "prediction_raw_text": "The value is 286.34\u0e19\u0e31\u0e48\u0e19"}
{"id": "line_474", "truth_parsed_k": 289.35, "prediction_parsed_k_with_c_conv": 285.98, "prediction_parsed_k_strict": 285.98, "absolute_error_k_vs_c_conv_pred": 3.3700000000000045, "score_vs_c_conv_pred": 10.308057645382362, "absolute_error_k_vs_strict_pred": 3.3700000000000045, "score_vs_pred_strict": 10.308057645382362, "truth_raw_assistant_text": "The value is 289.35", "prediction_raw_text": "The value is 285.98\u0e1a\u0e31\u0e19\u0e40\u0e17\u0e34\u0e07"}
{"id": "line_475", "truth_parsed_k": 289.6, "prediction_parsed_k_with_c_conv": 285.76, "prediction_parsed_k_strict": 285.76, "absolute_error_k_vs_c_conv_pred": 3.840000000000032, "score_vs_c_conv_pred": 6.910453475200773, "absolute_error_k_vs_strict_pred": 3.840000000000032, "score_vs_pred_strict": 6.910453475200773, "truth_raw_assistant_text": "The value is 289.60", "prediction_raw_text": "The value is 285.76."}
{"id": "line_476", "truth_parsed_k": 289.44, "prediction_parsed_k_with_c_conv": 284.95, "prediction_parsed_k_strict": 284.95, "absolute_error_k_vs_c_conv_pred": 4.490000000000009, "score_vs_c_conv_pred": 2.822583086765784, "absolute_error_k_vs_strict_pred": 4.490000000000009, "score_vs_pred_strict": 2.822583086765784, "truth_raw_assistant_text": "The value is 289.44", "prediction_raw_text": "The value is 284.95\u0e17\u0e35\u0e48\u0e40\u0e1b\u0e47\u0e19"}
{"id": "line_477", "truth_parsed_k": 288.72, "prediction_parsed_k_with_c_conv": 284.7, "prediction_parsed_k_strict": 284.7, "absolute_error_k_vs_c_conv_pred": 4.020000000000039, "score_vs_c_conv_pred": 5.714921714863896, "absolute_error_k_vs_strict_pred": 4.020000000000039, "score_vs_pred_strict": 5.714921714863896, "truth_raw_assistant_text": "The value is 288.72", "prediction_raw_text": "The value is 284.70\u0e2a\u0e32\u0e40\u0e2b\u0e15\u0e38"}
{"id": "line_478", "truth_parsed_k": 287.76, "prediction_parsed_k_with_c_conv": 284.98, "prediction_parsed_k_strict": 284.98, "absolute_error_k_vs_c_conv_pred": 2.7799999999999727, "score_vs_c_conv_pred": 15.287090825853355, "absolute_error_k_vs_strict_pred": 2.7799999999999727, "score_vs_pred_strict": 15.287090825853355, "truth_raw_assistant_text": "The value is 287.76", "prediction_raw_text": "The value is 284.98"}
{"id": "line_479", "truth_parsed_k": 286.97, "prediction_parsed_k_with_c_conv": 286.78, "prediction_parsed_k_strict": 286.78, "absolute_error_k_vs_c_conv_pred": 0.19000000000005457, "score_vs_c_conv_pred": 75.11132448969116, "absolute_error_k_vs_strict_pred": 0.19000000000005457, "score_vs_pred_strict": 75.11132448969116, "truth_raw_assistant_text": "The value is 286.97", "prediction_raw_text": "The value is 286.78"}
{"id": "line_480", "truth_parsed_k": 286.43, "prediction_parsed_k_with_c_conv": 284.84, "prediction_parsed_k_strict": 284.84, "absolute_error_k_vs_c_conv_pred": 1.5900000000000318, "score_vs_c_conv_pred": 29.478825905163642, "absolute_error_k_vs_strict_pred": 1.5900000000000318, "score_vs_pred_strict": 29.478825905163642, "truth_raw_assistant_text": "The value is 286.43", "prediction_raw_text": "The value is 284.84\uf98c"}
{"id": "line_481", "truth_parsed_k": 286.06, "prediction_parsed_k_with_c_conv": 284.96, "prediction_parsed_k_strict": 284.96, "absolute_error_k_vs_c_conv_pred": 1.1000000000000227, "score_vs_c_conv_pred": 38.53943468230319, "absolute_error_k_vs_strict_pred": 1.1000000000000227, "score_vs_pred_strict": 38.53943468230319, "truth_raw_assistant_text": "The value is 286.06", "prediction_raw_text": "The value is 284.96\uf9af"}
{"id": "line_482", "truth_parsed_k": 286.01, "prediction_parsed_k_with_c_conv": 285.88, "prediction_parsed_k_strict": 285.88, "absolute_error_k_vs_c_conv_pred": 0.12999999999999545, "score_vs_c_conv_pred": 80.80150876164818, "absolute_error_k_vs_strict_pred": 0.12999999999999545, "score_vs_pred_strict": 80.80150876164818, "truth_raw_assistant_text": "The value is 286.01", "prediction_raw_text": "The value is 285.88\u0e40\u0e19\u0e35\u0e22"}
{"id": "line_483", "truth_parsed_k": 286.65, "prediction_parsed_k_with_c_conv": 285.88, "prediction_parsed_k_strict": 285.88, "absolute_error_k_vs_c_conv_pred": 0.7699999999999818, "score_vs_c_conv_pred": 46.99146951739295, "absolute_error_k_vs_strict_pred": 0.7699999999999818, "score_vs_pred_strict": 46.99146951739295, "truth_raw_assistant_text": "The value is 286.65", "prediction_raw_text": "The value is 285.88\u0e40\u0e23\u0e35\u0e22\u0e19\u0e23\u0e39\u0e49"}
{"id": "line_484", "truth_parsed_k": 287.66, "prediction_parsed_k_with_c_conv": 285.71, "prediction_parsed_k_strict": 285.71, "absolute_error_k_vs_c_conv_pred": 1.9500000000000455, "score_vs_c_conv_pred": 24.347699602992044, "absolute_error_k_vs_strict_pred": 1.9500000000000455, "score_vs_pred_strict": 24.347699602992044, "truth_raw_assistant_text": "The value is 287.66", "prediction_raw_text": "The value is 285.71"}
{"id": "line_485", "truth_parsed_k": 288.46, "prediction_parsed_k_with_c_conv": 285.62, "prediction_parsed_k_strict": 285.62, "absolute_error_k_vs_c_conv_pred": 2.839999999999975, "score_vs_c_conv_pred": 14.73657930275375, "absolute_error_k_vs_strict_pred": 2.839999999999975, "score_vs_pred_strict": 14.73657930275375, "truth_raw_assistant_text": "The value is 288.46", "prediction_raw_text": "The value is 285.62."}
{"id": "line_486", "truth_parsed_k": 289.22, "prediction_parsed_k_with_c_conv": 285.95, "prediction_parsed_k_strict": 285.95, "absolute_error_k_vs_c_conv_pred": 3.2700000000000387, "score_vs_c_conv_pred": 11.08977439016785, "absolute_error_k_vs_strict_pred": 3.2700000000000387, "score_vs_pred_strict": 11.08977439016785, "truth_raw_assistant_text": "The value is 289.22", "prediction_raw_text": "The value is 285.95\u0e40\u0e2d\u0e47\u0e21"}
{"id": "line_487", "truth_parsed_k": 289.39, "prediction_parsed_k_with_c_conv": 284.95, "prediction_parsed_k_strict": 284.95, "absolute_error_k_vs_c_conv_pred": 4.439999999999998, "score_vs_c_conv_pred": 3.115922052920661, "absolute_error_k_vs_strict_pred": 4.439999999999998, "score_vs_pred_strict": 3.115922052920661, "truth_raw_assistant_text": "The value is 289.39", "prediction_raw_text": "The value is 284.95"}
{"id": "line_488", "truth_parsed_k": 289.19, "prediction_parsed_k_with_c_conv": 284.86, "prediction_parsed_k_strict": 284.86, "absolute_error_k_vs_c_conv_pred": 4.329999999999984, "score_vs_c_conv_pred": 3.772741220181597, "absolute_error_k_vs_strict_pred": 4.329999999999984, "score_vs_pred_strict": 3.772741220181597, "truth_raw_assistant_text": "The value is 289.19", "prediction_raw_text": "The value is 284.86\u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e38\u0e02"}
{"id": "line_489", "truth_parsed_k": 288.51, "prediction_parsed_k_with_c_conv": 285.81, "prediction_parsed_k_strict": 285.81, "absolute_error_k_vs_c_conv_pred": 2.6999999999999886, "score_vs_c_conv_pred": 16.039062921826364, "absolute_error_k_vs_strict_pred": 2.6999999999999886, "score_vs_pred_strict": 16.039062921826364, "truth_raw_assistant_text": "The value is 288.51", "prediction_raw_text": "The value is 285.81."}
{"id": "line_490", "truth_parsed_k": 287.82, "prediction_parsed_k_with_c_conv": 284.8, "prediction_parsed_k_strict": 284.8, "absolute_error_k_vs_c_conv_pred": 3.019999999999982, "score_vs_c_conv_pred": 13.149512816791553, "absolute_error_k_vs_strict_pred": 3.019999999999982, "score_vs_pred_strict": 13.149512816791553, "truth_raw_assistant_text": "The value is 287.82", "prediction_raw_text": "The value is 284.80"}
{"id": "line_491", "truth_parsed_k": 286.83, "prediction_parsed_k_with_c_conv": 284.73, "prediction_parsed_k_strict": 284.73, "absolute_error_k_vs_c_conv_pred": 2.099999999999966, "score_vs_c_conv_pred": 22.468221820366264, "absolute_error_k_vs_strict_pred": 2.099999999999966, "score_vs_pred_strict": 22.468221820366264, "truth_raw_assistant_text": "The value is 286.83", "prediction_raw_text": "The value is 284.73"}
{"id": "line_492", "truth_parsed_k": 285.92, "prediction_parsed_k_with_c_conv": 285.92, "prediction_parsed_k_strict": 285.92, "absolute_error_k_vs_c_conv_pred": 0.0, "score_vs_c_conv_pred": 100.0, "absolute_error_k_vs_strict_pred": 0.0, "score_vs_pred_strict": 100.0, "truth_raw_assistant_text": "The value is 285.92", "prediction_raw_text": "The value is 285.92"}
{"id": "line_493", "truth_parsed_k": 285.86, "prediction_parsed_k_with_c_conv": 284.95, "prediction_parsed_k_strict": 284.95, "absolute_error_k_vs_c_conv_pred": 0.910000000000025, "score_vs_c_conv_pred": 43.077906381572674, "absolute_error_k_vs_strict_pred": 0.910000000000025, "score_vs_pred_strict": 43.077906381572674, "truth_raw_assistant_text": "The value is 285.86", "prediction_raw_text": "The value is 284.95\u0e2a\u0e48\u0e27\u0e19\u0e25\u0e14"}
{"id": "line_494", "truth_parsed_k": 285.88, "prediction_parsed_k_with_c_conv": 285.58, "prediction_parsed_k_strict": 285.58, "absolute_error_k_vs_c_conv_pred": 0.30000000000001137, "score_vs_c_conv_pred": 67.04586824652068, "absolute_error_k_vs_strict_pred": 0.30000000000001137, "score_vs_pred_strict": 67.04586824652068, "truth_raw_assistant_text": "The value is 285.88", "prediction_raw_text": "The value is 285.58\ufb2f"}
{"id": "line_495", "truth_parsed_k": 286.52, "prediction_parsed_k_with_c_conv": 284.93101241097, "prediction_parsed_k_strict": 284.93101241097, "absolute_error_k_vs_c_conv_pred": 1.588987589029955, "score_vs_c_conv_pred": 29.494727075177096, "absolute_error_k_vs_strict_pred": 1.588987589029955, "score_vs_pred_strict": 29.494727075177096, "truth_raw_assistant_text": "The value is 286.52", "prediction_raw_text": "The value is 284.93101241097."}
{"id": "line_496", "truth_parsed_k": 287.54, "prediction_parsed_k_with_c_conv": 285.35, "prediction_parsed_k_strict": 285.35, "absolute_error_k_vs_c_conv_pred": 2.1899999999999977, "score_vs_c_conv_pred": 21.400442032933764, "absolute_error_k_vs_strict_pred": 2.1899999999999977, "score_vs_pred_strict": 21.400442032933764, "truth_raw_assistant_text": "The value is 287.54", "prediction_raw_text": "The value is 285.35\u0e21\u0e37\u0e2d\u0e16\u0e37\u0e2d"}
{"id": "line_497", "truth_parsed_k": 288.5, "prediction_parsed_k_with_c_conv": 285.94, "prediction_parsed_k_strict": 285.94, "absolute_error_k_vs_c_conv_pred": 2.5600000000000023, "score_vs_c_conv_pred": 17.407762269101447, "absolute_error_k_vs_strict_pred": 2.5600000000000023, "score_vs_pred_strict": 17.407762269101447, "truth_raw_assistant_text": "The value is 288.50", "prediction_raw_text": "The value is 285.94."}
{"id": "line_498", "truth_parsed_k": 289.11, "prediction_parsed_k_with_c_conv": 285.97, "prediction_parsed_k_strict": 285.97, "absolute_error_k_vs_c_conv_pred": 3.1399999999999864, "score_vs_c_conv_pred": 12.141160968399278, "absolute_error_k_vs_strict_pred": 3.1399999999999864, "score_vs_pred_strict": 12.141160968399278, "truth_raw_assistant_text": "The value is 289.11", "prediction_raw_text": "The value is 285.97\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23"}
{"id": "line_499", "truth_parsed_k": 289.5, "prediction_parsed_k_with_c_conv": 285.6, "prediction_parsed_k_strict": 285.6, "absolute_error_k_vs_c_conv_pred": 3.8999999999999773, "score_vs_c_conv_pred": 6.50601657418135, "absolute_error_k_vs_strict_pred": 3.8999999999999773, "score_vs_pred_strict": 6.50601657418135, "truth_raw_assistant_text": "The value is 289.50", "prediction_raw_text": "The value is 285.60\u0e0a\u0e35\u0e1e"}
{"id": "line_500", "truth_parsed_k": 289.36, "prediction_parsed_k_with_c_conv": 285.47, "prediction_parsed_k_strict": 285.47, "absolute_error_k_vs_c_conv_pred": 3.8899999999999864, "score_vs_c_conv_pred": 6.573002307246901, "absolute_error_k_vs_strict_pred": 3.8899999999999864, "score_vs_pred_strict": 6.573002307246901, "truth_raw_assistant_text": "The value is 289.36", "prediction_raw_text": "The value is 285.47\u0e2a\u0e39\u0e07\u0e2a\u0e38\u0e14"}
{"id": "line_501", "truth_parsed_k": 288.72, "prediction_parsed_k_with_c_conv": 285.84, "prediction_parsed_k_strict": 285.84, "absolute_error_k_vs_c_conv_pred": 2.8800000000000523, "score_vs_c_conv_pred": 14.375726320859128, "absolute_error_k_vs_strict_pred": 2.8800000000000523, "score_vs_pred_strict": 14.375726320859128, "truth_raw_assistant_text": "The value is 288.72", "prediction_raw_text": "The value is 285.84."}
{"id": "line_502", "truth_parsed_k": 287.86, "prediction_parsed_k_with_c_conv": 285.64, "prediction_parsed_k_strict": 285.64, "absolute_error_k_vs_c_conv_pred": 2.2200000000000273, "score_vs_c_conv_pred": 21.05372156733405, "absolute_error_k_vs_strict_pred": 2.2200000000000273, "score_vs_pred_strict": 21.05372156733405, "truth_raw_assistant_text": "The value is 287.86", "prediction_raw_text": "The value is 285.64"}
{"id": "line_503", "truth_parsed_k": 287.13, "prediction_parsed_k_with_c_conv": 284.96, "prediction_parsed_k_strict": 284.96, "absolute_error_k_vs_c_conv_pred": 2.170000000000016, "score_vs_c_conv_pred": 21.634094265065652, "absolute_error_k_vs_strict_pred": 2.170000000000016, "score_vs_pred_strict": 21.634094265065652, "truth_raw_assistant_text": "The value is 287.13", "prediction_raw_text": "The value is 284.96\u0e08\u0e34"}
{"id": "line_504", "truth_parsed_k": 286.29, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 1.4700000000000273, "score_vs_c_conv_pred": 31.432183563992467, "absolute_error_k_vs_strict_pred": 1.4700000000000273, "score_vs_pred_strict": 31.432183563992467, "truth_raw_assistant_text": "The value is 286.29", "prediction_raw_text": "The value is 284.82\u0e15\u0e31\u0e19"}
{"id": "line_505", "truth_parsed_k": 285.78, "prediction_parsed_k_with_c_conv": 284.88, "prediction_parsed_k_strict": 284.88, "absolute_error_k_vs_c_conv_pred": 0.8999999999999773, "score_vs_c_conv_pred": 43.33934776341783, "absolute_error_k_vs_strict_pred": 0.8999999999999773, "score_vs_pred_strict": 43.33934776341783, "truth_raw_assistant_text": "The value is 285.78", "prediction_raw_text": "The value is 284.88"}
{"id": "line_506", "truth_parsed_k": 286.15, "prediction_parsed_k_with_c_conv": 285.99, "prediction_parsed_k_strict": 285.99, "absolute_error_k_vs_c_conv_pred": 0.15999999999996817, "score_vs_c_conv_pred": 77.8063969632971, "absolute_error_k_vs_strict_pred": 0.15999999999996817, "score_vs_pred_strict": 77.8063969632971, "truth_raw_assistant_text": "The value is 286.15", "prediction_raw_text": "The value is 285.99\uf9b5"}
{"id": "line_507", "truth_parsed_k": 286.58, "prediction_parsed_k_with_c_conv": 284.87, "prediction_parsed_k_strict": 284.87, "absolute_error_k_vs_c_conv_pred": 1.7099999999999795, "score_vs_c_conv_pred": 27.657630579644565, "absolute_error_k_vs_strict_pred": 1.7099999999999795, "score_vs_pred_strict": 27.657630579644565, "truth_raw_assistant_text": "The value is 286.58", "prediction_raw_text": "The value is 284.87\u0e27\u0e34\u0e40\u0e04\u0e23"}
{"id": "line_508", "truth_parsed_k": 287.68, "prediction_parsed_k_with_c_conv": 284.99, "prediction_parsed_k_strict": 284.99, "absolute_error_k_vs_c_conv_pred": 2.6899999999999977, "score_vs_c_conv_pred": 16.134553272977538, "absolute_error_k_vs_strict_pred": 2.6899999999999977, "score_vs_pred_strict": 16.134553272977538, "truth_raw_assistant_text": "The value is 287.68", "prediction_raw_text": "The value is 284.99\u0e17\u0e35\u0e48\u0e44\u0e21\u0e48"}
{"id": "line_509", "truth_parsed_k": 288.7, "prediction_parsed_k_with_c_conv": 285.96, "prediction_parsed_k_strict": 285.96, "absolute_error_k_vs_c_conv_pred": 2.740000000000009, "score_vs_c_conv_pred": 15.660452104108114, "absolute_error_k_vs_strict_pred": 2.740000000000009, "score_vs_pred_strict": 15.660452104108114, "truth_raw_assistant_text": "The value is 288.70", "prediction_raw_text": "The value is 285.96\u0e08\u0e31\u0e07\u0e2b\u0e27"}
{"id": "line_510", "truth_parsed_k": 289.41, "prediction_parsed_k_with_c_conv": 285.48, "prediction_parsed_k_strict": 285.48, "absolute_error_k_vs_c_conv_pred": 3.930000000000007, "score_vs_c_conv_pred": 6.306053396855771, "absolute_error_k_vs_strict_pred": 3.930000000000007, "score_vs_pred_strict": 6.306053396855771, "truth_raw_assistant_text": "The value is 289.41", "prediction_raw_text": "The value is 285.48"}
{"id": "line_511", "truth_parsed_k": 289.61, "prediction_parsed_k_with_c_conv": 284.87, "prediction_parsed_k_strict": 284.87, "absolute_error_k_vs_c_conv_pred": 4.740000000000009, "score_vs_c_conv_pred": 1.4019873170983632, "absolute_error_k_vs_strict_pred": 4.740000000000009, "score_vs_pred_strict": 1.4019873170983632, "truth_raw_assistant_text": "The value is 289.61", "prediction_raw_text": "The value is 284.87\u0e01\u0e48\u0e2d\u0e19\u0e2b\u0e19\u0e49\u0e32\u0e19\u0e35\u0e49"}
{"id": "line_512", "truth_parsed_k": 289.45, "prediction_parsed_k_with_c_conv": 284.88, "prediction_parsed_k_strict": 284.88, "absolute_error_k_vs_c_conv_pred": 4.569999999999993, "score_vs_c_conv_pred": 2.359787578917749, "absolute_error_k_vs_strict_pred": 4.569999999999993, "score_vs_pred_strict": 2.359787578917749, "truth_raw_assistant_text": "The value is 289.45", "prediction_raw_text": "The value is 284.88\u0e40\u0e1e\u0e34\u0e48\u0e07"}
{"id": "line_513", "truth_parsed_k": 288.92, "prediction_parsed_k_with_c_conv": 285.95, "prediction_parsed_k_strict": 285.95, "absolute_error_k_vs_c_conv_pred": 2.9700000000000273, "score_vs_c_conv_pred": 13.58106453378981, "absolute_error_k_vs_strict_pred": 2.9700000000000273, "score_vs_pred_strict": 13.58106453378981, "truth_raw_assistant_text": "The value is 288.92", "prediction_raw_text": "The value is 285.95\u0e0a\u0e39"}
{"id": "line_514", "truth_parsed_k": 288.16, "prediction_parsed_k_with_c_conv": 284.91, "prediction_parsed_k_strict": 284.91, "absolute_error_k_vs_c_conv_pred": 3.25, "score_vs_c_conv_pred": 11.248878255127204, "absolute_error_k_vs_strict_pred": 3.25, "score_vs_pred_strict": 11.248878255127204, "truth_raw_assistant_text": "The value is 288.16", "prediction_raw_text": "The value is 284.91 \u0e19\u0e2d\u0e01\u0e08\u0e32\u0e01\u0e19\u0e35\u0e49"}
{"id": "line_515", "truth_parsed_k": 287.24, "prediction_parsed_k_with_c_conv": 285.92, "prediction_parsed_k_strict": 285.92, "absolute_error_k_vs_c_conv_pred": 1.3199999999999932, "score_vs_c_conv_pred": 34.09173649304294, "absolute_error_k_vs_strict_pred": 1.3199999999999932, "score_vs_pred_strict": 34.09173649304294, "truth_raw_assistant_text": "The value is 287.24", "prediction_raw_text": "The value is 285.92\u0e04\u0e23\u0e31\u0e49\u0e07\u0e19\u0e35\u0e49"}
{"id": "line_516", "truth_parsed_k": 286.47, "prediction_parsed_k_with_c_conv": 284.66, "prediction_parsed_k_strict": 284.66, "absolute_error_k_vs_c_conv_pred": 1.8100000000000023, "score_vs_c_conv_pred": 26.228740981648546, "absolute_error_k_vs_strict_pred": 1.8100000000000023, "score_vs_pred_strict": 26.228740981648546, "truth_raw_assistant_text": "The value is 286.47", "prediction_raw_text": "The value is 284.66."}
{"id": "line_517", "truth_parsed_k": 286.16, "prediction_parsed_k_with_c_conv": 285.58, "prediction_parsed_k_strict": 285.58, "absolute_error_k_vs_c_conv_pred": 0.5800000000000409, "score_vs_c_conv_pred": 53.4172480178835, "absolute_error_k_vs_strict_pred": 0.5800000000000409, "score_vs_pred_strict": 53.4172480178835, "truth_raw_assistant_text": "The value is 286.16", "prediction_raw_text": "The value is 285.58\u0e1b\u0e31\u0e08\u0e08\u0e31\u0e22"}
{"id": "line_518", "truth_parsed_k": 286.43, "prediction_parsed_k_with_c_conv": 284.88, "prediction_parsed_k_strict": 284.88, "absolute_error_k_vs_c_conv_pred": 1.5500000000000114, "score_vs_c_conv_pred": 30.11433034447373, "absolute_error_k_vs_strict_pred": 1.5500000000000114, "score_vs_pred_strict": 30.11433034447373, "truth_raw_assistant_text": "The value is 286.43", "prediction_raw_text": "The value is 284.88\u0e01\u0e32\u0e23\u0e40\u0e21\u0e37\u0e2d\u0e07"}
{"id": "line_519", "truth_parsed_k": 287.03, "prediction_parsed_k_with_c_conv": 285.48, "prediction_parsed_k_strict": 285.48, "absolute_error_k_vs_c_conv_pred": 1.5499999999999545, "score_vs_c_conv_pred": 30.114330344474638, "absolute_error_k_vs_strict_pred": 1.5499999999999545, "score_vs_pred_strict": 30.114330344474638, "truth_raw_assistant_text": "The value is 287.03", "prediction_raw_text": "The value is 285.48\uf92d"}
{"id": "line_520", "truth_parsed_k": 287.85, "prediction_parsed_k_with_c_conv": 285.74, "prediction_parsed_k_strict": 285.74, "absolute_error_k_vs_c_conv_pred": 2.1100000000000136, "score_vs_c_conv_pred": 22.347467130089584, "absolute_error_k_vs_strict_pred": 2.1100000000000136, "score_vs_pred_strict": 22.347467130089584, "truth_raw_assistant_text": "The value is 287.85", "prediction_raw_text": "The value is 285.74."}
{"id": "line_521", "truth_parsed_k": 288.89, "prediction_parsed_k_with_c_conv": 286.21, "prediction_parsed_k_strict": 286.21, "absolute_error_k_vs_c_conv_pred": 2.680000000000007, "score_vs_c_conv_pred": 16.230383448158403, "absolute_error_k_vs_strict_pred": 2.680000000000007, "score_vs_pred_strict": 16.230383448158403, "truth_raw_assistant_text": "The value is 288.89", "prediction_raw_text": "The value is 286.21."}
{"id": "line_522", "truth_parsed_k": 289.47, "prediction_parsed_k_with_c_conv": 284.64, "prediction_parsed_k_strict": 284.64, "absolute_error_k_vs_c_conv_pred": 4.830000000000041, "score_vs_c_conv_pred": 0.9083803043695826, "absolute_error_k_vs_strict_pred": 4.830000000000041, "score_vs_pred_strict": 0.9083803043695826, "truth_raw_assistant_text": "The value is 289.47", "prediction_raw_text": "The value is 284.64"}
{"id": "line_523", "truth_parsed_k": 289.71, "prediction_parsed_k_with_c_conv": 285.52, "prediction_parsed_k_strict": 285.52, "absolute_error_k_vs_c_conv_pred": 4.189999999999998, "score_vs_c_conv_pred": 4.632553169804399, "absolute_error_k_vs_strict_pred": 4.189999999999998, "score_vs_pred_strict": 4.632553169804399, "truth_raw_assistant_text": "The value is 289.71", "prediction_raw_text": "The value is 285.52\u0e40\u0e14\u0e2d\u0e23\u0e4c"}
{"id": "line_524", "truth_parsed_k": 289.6, "prediction_parsed_k_with_c_conv": 285.85, "prediction_parsed_k_strict": 285.85, "absolute_error_k_vs_c_conv_pred": 3.75, "score_vs_c_conv_pred": 7.528731291521307, "absolute_error_k_vs_strict_pred": 3.75, "score_vs_pred_strict": 7.528731291521307, "truth_raw_assistant_text": "The value is 289.60", "prediction_raw_text": "The value is 285.85\u01a1\u0301i"}
{"id": "line_525", "truth_parsed_k": 289.04, "prediction_parsed_k_with_c_conv": 286.6, "prediction_parsed_k_strict": 286.6, "absolute_error_k_vs_c_conv_pred": 2.4399999999999977, "score_vs_c_conv_pred": 18.63898246671226, "absolute_error_k_vs_strict_pred": 2.4399999999999977, "score_vs_pred_strict": 18.63898246671226, "truth_raw_assistant_text": "The value is 289.04", "prediction_raw_text": "The value is 286.60\u0e2d\u0e35\u0e40\u0e21"}
{"id": "line_526", "truth_parsed_k": 288.14, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 3.1999999999999886, "score_vs_c_conv_pred": 11.65079990761112, "absolute_error_k_vs_strict_pred": 3.1999999999999886, "score_vs_pred_strict": 11.65079990761112, "truth_raw_assistant_text": "The value is 288.14", "prediction_raw_text": "The value is 284.94\u0e41\u0e2b\u0e25\u0e48\u0e07"}
{"id": "line_527", "truth_parsed_k": 287.26, "prediction_parsed_k_with_c_conv": 284.65, "prediction_parsed_k_strict": 284.65, "absolute_error_k_vs_c_conv_pred": 2.6100000000000136, "score_vs_c_conv_pred": 16.91091697886058, "absolute_error_k_vs_strict_pred": 2.6100000000000136, "score_vs_pred_strict": 16.91091697886058, "truth_raw_assistant_text": "The value is 287.26", "prediction_raw_text": "The value is 284.65."}
{"id": "line_528", "truth_parsed_k": 286.39, "prediction_parsed_k_with_c_conv": 284.74, "prediction_parsed_k_strict": 284.74, "absolute_error_k_vs_c_conv_pred": 1.6499999999999773, "score_vs_c_conv_pred": 28.55283492688805, "absolute_error_k_vs_strict_pred": 1.6499999999999773, "score_vs_pred_strict": 28.55283492688805, "truth_raw_assistant_text": "The value is 286.39", "prediction_raw_text": "The value is 284.74 cu\u0309"}
{"id": "line_529", "truth_parsed_k": 286.11, "prediction_parsed_k_with_c_conv": 285.5, "prediction_parsed_k_strict": 285.5, "absolute_error_k_vs_c_conv_pred": 0.6100000000000136, "score_vs_c_conv_pred": 52.29507519742948, "absolute_error_k_vs_strict_pred": 0.6100000000000136, "score_vs_pred_strict": 52.29507519742948, "truth_raw_assistant_text": "The value is 286.11", "prediction_raw_text": "The value is 285.50\u0e44\u0e21\u0e48\u0e15\u0e49\u0e2d\u0e07"}
{"id": "line_530", "truth_parsed_k": 286.27, "prediction_parsed_k_with_c_conv": 284.84, "prediction_parsed_k_strict": 284.84, "absolute_error_k_vs_c_conv_pred": 1.4300000000000068, "score_vs_c_conv_pred": 32.11611241065043, "absolute_error_k_vs_strict_pred": 1.4300000000000068, "score_vs_pred_strict": 32.11611241065043, "truth_raw_assistant_text": "The value is 286.27", "prediction_raw_text": "The value is 284.84"}
{"id": "line_531", "truth_parsed_k": 286.96, "prediction_parsed_k_with_c_conv": 284.99, "prediction_parsed_k_strict": 284.99, "absolute_error_k_vs_c_conv_pred": 1.9699999999999704, "score_vs_c_conv_pred": 24.089393058266328, "absolute_error_k_vs_strict_pred": 1.9699999999999704, "score_vs_pred_strict": 24.089393058266328, "truth_raw_assistant_text": "The value is 286.96", "prediction_raw_text": "The value is 284.99\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e2a\u0e23\u0e23"}
{"id": "line_532", "truth_parsed_k": 287.99, "prediction_parsed_k_with_c_conv": 284.92, "prediction_parsed_k_strict": 284.92, "absolute_error_k_vs_c_conv_pred": 3.069999999999993, "score_vs_c_conv_pred": 12.724768193723124, "absolute_error_k_vs_strict_pred": 3.069999999999993, "score_vs_pred_strict": 12.724768193723124, "truth_raw_assistant_text": "The value is 287.99", "prediction_raw_text": "The value is 284.92\u0e2a\u0e37\u0e48\u0e2d\u0e2a\u0e32\u0e23"}
{"id": "line_533", "truth_parsed_k": 288.89, "prediction_parsed_k_with_c_conv": 285.69, "prediction_parsed_k_strict": 285.69, "absolute_error_k_vs_c_conv_pred": 3.1999999999999886, "score_vs_c_conv_pred": 11.65079990761112, "absolute_error_k_vs_strict_pred": 3.1999999999999886, "score_vs_pred_strict": 11.65079990761112, "truth_raw_assistant_text": "The value is 288.89", "prediction_raw_text": "The value is 285.69."}
{"id": "line_534", "truth_parsed_k": 289.44, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 4.46999999999997, "score_vs_c_conv_pred": 2.939535499945378, "absolute_error_k_vs_strict_pred": 4.46999999999997, "score_vs_pred_strict": 2.939535499945378, "truth_raw_assistant_text": "The value is 289.44", "prediction_raw_text": "The value is 284.97\uf942"}
{"id": "line_535", "truth_parsed_k": 289.88, "prediction_parsed_k_with_c_conv": 286.94, "prediction_parsed_k_strict": 286.94, "absolute_error_k_vs_c_conv_pred": 2.9399999999999977, "score_vs_c_conv_pred": 13.843354764689742, "absolute_error_k_vs_strict_pred": 2.9399999999999977, "score_vs_pred_strict": 13.843354764689742, "truth_raw_assistant_text": "The value is 289.88", "prediction_raw_text": "The value is 286.94\u0e2d\u0e31\u0e15\u0e42\u0e19"}
{"id": "line_536", "truth_parsed_k": 289.68, "prediction_parsed_k_with_c_conv": 284.78, "prediction_parsed_k_strict": 284.78, "absolute_error_k_vs_c_conv_pred": 4.900000000000034, "score_vs_c_conv_pred": 0.5306230961827918, "absolute_error_k_vs_strict_pred": 4.900000000000034, "score_vs_pred_strict": 0.5306230961827918, "truth_raw_assistant_text": "The value is 289.68", "prediction_raw_text": "The value is 284.78\u0e42\u0e15\u0e49"}
{"id": "line_537", "truth_parsed_k": 289.02, "prediction_parsed_k_with_c_conv": 286.84, "prediction_parsed_k_strict": 286.84, "absolute_error_k_vs_c_conv_pred": 2.180000000000007, "score_vs_c_conv_pred": 21.517014729297866, "absolute_error_k_vs_strict_pred": 2.180000000000007, "score_vs_pred_strict": 21.517014729297866, "truth_raw_assistant_text": "The value is 289.02", "prediction_raw_text": "The value is 286.84."}
{"id": "line_538", "truth_parsed_k": 288.35, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 3.3799999999999955, "score_vs_c_conv_pred": 10.231119733596717, "absolute_error_k_vs_strict_pred": 3.3799999999999955, "score_vs_pred_strict": 10.231119733596717, "truth_raw_assistant_text": "The value is 288.35", "prediction_raw_text": "The value is 284.97\u0e08\u0e30\u0e17\u0e33\u0e43\u0e2b\u0e49"}
{"id": "line_539", "truth_parsed_k": 287.45, "prediction_parsed_k_with_c_conv": 285.64, "prediction_parsed_k_strict": 285.64, "absolute_error_k_vs_c_conv_pred": 1.8100000000000023, "score_vs_c_conv_pred": 26.228740981648546, "absolute_error_k_vs_strict_pred": 1.8100000000000023, "score_vs_pred_strict": 26.228740981648546, "truth_raw_assistant_text": "The value is 287.45", "prediction_raw_text": "The value is 285.64."}
{"id": "line_540", "truth_parsed_k": 286.5, "prediction_parsed_k_with_c_conv": 285.8, "prediction_parsed_k_strict": 285.8, "absolute_error_k_vs_c_conv_pred": 0.6999999999999886, "score_vs_c_conv_pred": 49.18451520163744, "absolute_error_k_vs_strict_pred": 0.6999999999999886, "score_vs_pred_strict": 49.18451520163744, "truth_raw_assistant_text": "The value is 286.50", "prediction_raw_text": "The value is 285.80\u0e40\u0e0a\u0e37\u0e48\u0e2d\u0e27\u0e48\u0e32"}
{"id": "line_541", "truth_parsed_k": 286.27, "prediction_parsed_k_with_c_conv": 284.64, "prediction_parsed_k_strict": 284.64, "absolute_error_k_vs_c_conv_pred": 1.6299999999999955, "score_vs_c_conv_pred": 28.85797417842264, "absolute_error_k_vs_strict_pred": 1.6299999999999955, "score_vs_pred_strict": 28.85797417842264, "truth_raw_assistant_text": "The value is 286.27", "prediction_raw_text": "The value is 284.64\u0e2a\u0e37"}
{"id": "line_542", "truth_parsed_k": 286.47, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 1.5300000000000296, "score_vs_c_conv_pred": 30.43779670603145, "absolute_error_k_vs_strict_pred": 1.5300000000000296, "score_vs_pred_strict": 30.43779670603145, "truth_raw_assistant_text": "The value is 286.47", "prediction_raw_text": "The value is 284.94"}
{"id": "line_543", "truth_parsed_k": 287.14, "prediction_parsed_k_with_c_conv": 284.7, "prediction_parsed_k_strict": 284.7, "absolute_error_k_vs_c_conv_pred": 2.4399999999999977, "score_vs_c_conv_pred": 18.63898246671226, "absolute_error_k_vs_strict_pred": 2.4399999999999977, "score_vs_pred_strict": 18.63898246671226, "truth_raw_assistant_text": "The value is 287.14", "prediction_raw_text": "The value is 284.70\u0e07\u0e48\u0e32\u0e22\u0e46"}
{"id": "line_544", "truth_parsed_k": 288.09, "prediction_parsed_k_with_c_conv": 286.65, "prediction_parsed_k_strict": 286.65, "absolute_error_k_vs_c_conv_pred": 1.4399999999999977, "score_vs_c_conv_pred": 31.943494827208674, "absolute_error_k_vs_strict_pred": 1.4399999999999977, "score_vs_pred_strict": 31.943494827208674, "truth_raw_assistant_text": "The value is 288.09", "prediction_raw_text": "The value is 286.65."}
{"id": "line_545", "truth_parsed_k": 289.04, "prediction_parsed_k_with_c_conv": 285.3, "prediction_parsed_k_strict": 285.3, "absolute_error_k_vs_c_conv_pred": 3.740000000000009, "score_vs_c_conv_pred": 7.598313373665089, "absolute_error_k_vs_strict_pred": 3.740000000000009, "score_vs_pred_strict": 7.598313373665089, "truth_raw_assistant_text": "The value is 289.04", "prediction_raw_text": "The value is 285.30"}
{"id": "line_546", "truth_parsed_k": 289.8, "prediction_parsed_k_with_c_conv": 284.81, "prediction_parsed_k_strict": 284.81, "absolute_error_k_vs_c_conv_pred": 4.990000000000009, "score_vs_c_conv_pred": 0.052594256575544396, "absolute_error_k_vs_strict_pred": 4.990000000000009, "score_vs_pred_strict": 0.052594256575544396, "truth_raw_assistant_text": "The value is 289.80", "prediction_raw_text": "The value is 284.81"}
{"id": "line_547", "truth_parsed_k": 290.04, "prediction_parsed_k_with_c_conv": 284.8, "prediction_parsed_k_strict": 284.8, "absolute_error_k_vs_c_conv_pred": 5.240000000000009, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.240000000000009, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.04", "prediction_raw_text": "The value is 284.80\u0e41\u0e15\u0e01\u0e15\u0e48\u0e32\u0e07\u0e01\u0e31\u0e19"}
{"id": "line_548", "truth_parsed_k": 289.77, "prediction_parsed_k_with_c_conv": 284.96, "prediction_parsed_k_strict": 284.96, "absolute_error_k_vs_c_conv_pred": 4.810000000000002, "score_vs_c_conv_pred": 1.017291477328286, "absolute_error_k_vs_strict_pred": 4.810000000000002, "score_vs_pred_strict": 1.017291477328286, "truth_raw_assistant_text": "The value is 289.77", "prediction_raw_text": "The value is 284.96\u0e02\u0e49\u0e32\u0e07"}
{"id": "line_549", "truth_parsed_k": 289.21, "prediction_parsed_k_with_c_conv": 285.62, "prediction_parsed_k_strict": 285.62, "absolute_error_k_vs_c_conv_pred": 3.589999999999975, "score_vs_c_conv_pred": 8.664214429539863, "absolute_error_k_vs_strict_pred": 3.589999999999975, "score_vs_pred_strict": 8.664214429539863, "truth_raw_assistant_text": "The value is 289.21", "prediction_raw_text": "The value is 285.62\u0e17\u0e35\u0e48\u0e44\u0e21\u0e48"}
{"id": "line_550", "truth_parsed_k": 288.29, "prediction_parsed_k_with_c_conv": 285.62, "prediction_parsed_k_strict": 285.62, "absolute_error_k_vs_c_conv_pred": 2.670000000000016, "score_vs_c_conv_pred": 16.326555874691216, "absolute_error_k_vs_strict_pred": 2.670000000000016, "score_vs_pred_strict": 16.326555874691216, "truth_raw_assistant_text": "The value is 288.29", "prediction_raw_text": "The value is 285.62"}
{"id": "line_551", "truth_parsed_k": 287.53, "prediction_parsed_k_with_c_conv": 286.95, "prediction_parsed_k_strict": 286.95, "absolute_error_k_vs_c_conv_pred": 0.5799999999999841, "score_vs_c_conv_pred": 53.417248017885676, "absolute_error_k_vs_strict_pred": 0.5799999999999841, "score_vs_pred_strict": 53.417248017885676, "truth_raw_assistant_text": "The value is 287.53", "prediction_raw_text": "The value is 286.95."}
{"id": "line_552", "truth_parsed_k": 286.97, "prediction_parsed_k_with_c_conv": 285.47, "prediction_parsed_k_strict": 285.47, "absolute_error_k_vs_c_conv_pred": 1.5, "score_vs_c_conv_pred": 30.93040039646092, "absolute_error_k_vs_strict_pred": 1.5, "score_vs_pred_strict": 30.93040039646092, "truth_raw_assistant_text": "The value is 286.97", "prediction_raw_text": "The value is 285.47 \u0e0b\u0e36\u0e48\u0e07\u0e40\u0e1b\u0e47\u0e19"}
{"id": "line_553", "truth_parsed_k": 286.76, "prediction_parsed_k_with_c_conv": 284.64, "prediction_parsed_k_strict": 284.64, "absolute_error_k_vs_c_conv_pred": 2.1200000000000045, "score_vs_c_conv_pred": 22.227251525046988, "absolute_error_k_vs_strict_pred": 2.1200000000000045, "score_vs_pred_strict": 22.227251525046988, "truth_raw_assistant_text": "The value is 286.76", "prediction_raw_text": "The value is 284.64"}
{"id": "line_554", "truth_parsed_k": 286.99, "prediction_parsed_k_with_c_conv": 284.64, "prediction_parsed_k_strict": 284.64, "absolute_error_k_vs_c_conv_pred": 2.3500000000000227, "score_vs_c_conv_pred": 19.60080795334589, "absolute_error_k_vs_strict_pred": 2.3500000000000227, "score_vs_pred_strict": 19.60080795334589, "truth_raw_assistant_text": "The value is 286.99", "prediction_raw_text": "The value is 284.64\u0e40\u0e0b\u0e47\u0e19"}
{"id": "line_555", "truth_parsed_k": 287.32, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 2.5, "score_vs_c_conv_pred": 18.016336211152296, "absolute_error_k_vs_strict_pred": 2.5, "score_vs_pred_strict": 18.016336211152296, "truth_raw_assistant_text": "The value is 287.32", "prediction_raw_text": "The value is 284.82\u0e01\u0e48\u0e2d"}
{"id": "line_556", "truth_parsed_k": 288.22, "prediction_parsed_k_with_c_conv": 285.72, "prediction_parsed_k_strict": 285.72, "absolute_error_k_vs_c_conv_pred": 2.5, "score_vs_c_conv_pred": 18.016336211152296, "absolute_error_k_vs_strict_pred": 2.5, "score_vs_pred_strict": 18.016336211152296, "truth_raw_assistant_text": "The value is 288.22", "prediction_raw_text": "The value is 285.72"}
{"id": "line_557", "truth_parsed_k": 289.04, "prediction_parsed_k_with_c_conv": 287.89, "prediction_parsed_k_strict": 287.89, "absolute_error_k_vs_c_conv_pred": 1.150000000000034, "score_vs_c_conv_pred": 37.46216099822974, "absolute_error_k_vs_strict_pred": 1.150000000000034, "score_vs_pred_strict": 37.46216099822974, "truth_raw_assistant_text": "The value is 289.04", "prediction_raw_text": "The value is 287.89."}
{"id": "line_558", "truth_parsed_k": 289.92, "prediction_parsed_k_with_c_conv": 285.84, "prediction_parsed_k_strict": 285.84, "absolute_error_k_vs_c_conv_pred": 4.080000000000041, "score_vs_c_conv_pred": 5.327922228129611, "absolute_error_k_vs_strict_pred": 4.080000000000041, "score_vs_pred_strict": 5.327922228129611, "truth_raw_assistant_text": "The value is 289.92", "prediction_raw_text": "The value is 285.84."}
{"id": "line_559", "truth_parsed_k": 290.21, "prediction_parsed_k_with_c_conv": 285.85, "prediction_parsed_k_strict": 285.85, "absolute_error_k_vs_c_conv_pred": 4.359999999999957, "score_vs_c_conv_pred": 3.5920140263254408, "absolute_error_k_vs_strict_pred": 4.359999999999957, "score_vs_pred_strict": 3.5920140263254408, "truth_raw_assistant_text": "The value is 290.21", "prediction_raw_text": "The value is 285.85."}
{"id": "line_560", "truth_parsed_k": 289.96, "prediction_parsed_k_with_c_conv": 284.5, "prediction_parsed_k_strict": 284.5, "absolute_error_k_vs_c_conv_pred": 5.4599999999999795, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.4599999999999795, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 289.96", "prediction_raw_text": "The value is 284.50\u0e40\u0e23\u0e37\u0e48\u0e2d\u0e22\u0e46"}
{"id": "line_561", "truth_parsed_k": 289.28, "prediction_parsed_k_with_c_conv": 284.84, "prediction_parsed_k_strict": 284.84, "absolute_error_k_vs_c_conv_pred": 4.439999999999998, "score_vs_c_conv_pred": 3.115922052920661, "absolute_error_k_vs_strict_pred": 4.439999999999998, "score_vs_pred_strict": 3.115922052920661, "truth_raw_assistant_text": "The value is 289.28", "prediction_raw_text": "The value is 284.84\u0e1b\u0e25\u0e39\u0e01"}
{"id": "line_562", "truth_parsed_k": 288.45, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 3.6299999999999955, "score_vs_c_conv_pred": 8.375823444956499, "absolute_error_k_vs_strict_pred": 3.6299999999999955, "score_vs_pred_strict": 8.375823444956499, "truth_raw_assistant_text": "The value is 288.45", "prediction_raw_text": "The value is 284.82"}
{"id": "line_563", "truth_parsed_k": 287.76, "prediction_parsed_k_with_c_conv": 285.90194094081, "prediction_parsed_k_strict": 285.90194094081, "absolute_error_k_vs_c_conv_pred": 1.8580590591899977, "score_vs_c_conv_pred": 25.568101981782632, "absolute_error_k_vs_strict_pred": 1.8580590591899977, "score_vs_pred_strict": 25.568101981782632, "truth_raw_assistant_text": "The value is 287.76", "prediction_raw_text": "The value is 285.90194094081."}
{"id": "line_564", "truth_parsed_k": 287.01, "prediction_parsed_k_with_c_conv": 284.8, "prediction_parsed_k_strict": 284.8, "absolute_error_k_vs_c_conv_pred": 2.2099999999999795, "score_vs_c_conv_pred": 21.16879973737197, "absolute_error_k_vs_strict_pred": 2.2099999999999795, "score_vs_pred_strict": 21.16879973737197, "truth_raw_assistant_text": "The value is 287.01", "prediction_raw_text": "The value is 284.80\u0645\u064e"}
{"id": "line_565", "truth_parsed_k": 286.68, "prediction_parsed_k_with_c_conv": 284.64, "prediction_parsed_k_strict": 284.64, "absolute_error_k_vs_c_conv_pred": 2.0400000000000205, "score_vs_c_conv_pred": 23.20434740707088, "absolute_error_k_vs_strict_pred": 2.0400000000000205, "score_vs_pred_strict": 23.20434740707088, "truth_raw_assistant_text": "The value is 286.68", "prediction_raw_text": "The value is 284.64\u0e28\u0e39"}
{"id": "line_566", "truth_parsed_k": 286.8, "prediction_parsed_k_with_c_conv": 284.93, "prediction_parsed_k_strict": 284.93, "absolute_error_k_vs_c_conv_pred": 1.8700000000000045, "score_vs_c_conv_pred": 25.406440422737266, "absolute_error_k_vs_strict_pred": 1.8700000000000045, "score_vs_pred_strict": 25.406440422737266, "truth_raw_assistant_text": "The value is 286.80", "prediction_raw_text": "The value is 284.93."}
{"id": "line_567", "truth_parsed_k": 287.31, "prediction_parsed_k_with_c_conv": 284.91, "prediction_parsed_k_strict": 284.91, "absolute_error_k_vs_c_conv_pred": 2.3999999999999773, "score_vs_c_conv_pred": 19.062224983973174, "absolute_error_k_vs_strict_pred": 2.3999999999999773, "score_vs_pred_strict": 19.062224983973174, "truth_raw_assistant_text": "The value is 287.31", "prediction_raw_text": "The value is 284.91"}
{"id": "line_568", "truth_parsed_k": 288.3, "prediction_parsed_k_with_c_conv": 285.6, "prediction_parsed_k_strict": 285.6, "absolute_error_k_vs_c_conv_pred": 2.6999999999999886, "score_vs_c_conv_pred": 16.039062921826364, "absolute_error_k_vs_strict_pred": 2.6999999999999886, "score_vs_pred_strict": 16.039062921826364, "truth_raw_assistant_text": "The value is 288.30", "prediction_raw_text": "The value is 285.60\u0e1b\u0e31\u0e08\u0e08\u0e38\u0e1a\u0e31\u0e19"}
{"id": "line_569", "truth_parsed_k": 289.26, "prediction_parsed_k_with_c_conv": 285.59, "prediction_parsed_k_strict": 285.59, "absolute_error_k_vs_c_conv_pred": 3.670000000000016, "score_vs_c_conv_pred": 8.090488316374921, "absolute_error_k_vs_strict_pred": 3.670000000000016, "score_vs_pred_strict": 8.090488316374921, "truth_raw_assistant_text": "The value is 289.26", "prediction_raw_text": "The value is 285.59"}
{"id": "line_570", "truth_parsed_k": 289.84, "prediction_parsed_k_with_c_conv": 284.8, "prediction_parsed_k_strict": 284.8, "absolute_error_k_vs_c_conv_pred": 5.039999999999964, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.039999999999964, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 289.84", "prediction_raw_text": "The value is 284.80\u0e25\u0e31\u0e01\u0e29\u0e13\u0e30"}
{"id": "line_571", "truth_parsed_k": 290.09, "prediction_parsed_k_with_c_conv": 285.92, "prediction_parsed_k_strict": 285.92, "absolute_error_k_vs_c_conv_pred": 4.169999999999959, "score_vs_c_conv_pred": 4.757655605295974, "absolute_error_k_vs_strict_pred": 4.169999999999959, "score_vs_pred_strict": 4.757655605295974, "truth_raw_assistant_text": "The value is 290.09", "prediction_raw_text": "The value is 285.92\u0e27\u0e34\u0e40\u0e04\u0e23\u0e32\u0e30"}
{"id": "line_572", "truth_parsed_k": 289.81, "prediction_parsed_k_with_c_conv": 284.85, "prediction_parsed_k_strict": 284.85, "absolute_error_k_vs_c_conv_pred": 4.9599999999999795, "score_vs_c_conv_pred": 0.2109961886617273, "absolute_error_k_vs_strict_pred": 4.9599999999999795, "score_vs_pred_strict": 0.2109961886617273, "truth_raw_assistant_text": "The value is 289.81", "prediction_raw_text": "The value is 284.85\u0631\u0650"}
{"id": "line_573", "truth_parsed_k": 289.23, "prediction_parsed_k_with_c_conv": 284.54, "prediction_parsed_k_strict": 284.54, "absolute_error_k_vs_c_conv_pred": 4.689999999999998, "score_vs_c_conv_pred": 1.6801742003018982, "absolute_error_k_vs_strict_pred": 4.689999999999998, "score_vs_pred_strict": 1.6801742003018982, "truth_raw_assistant_text": "The value is 289.23", "prediction_raw_text": "The value is 284.54\u0e08\u0e31\u0e07\u0e2b\u0e27"}
{"id": "line_574", "truth_parsed_k": 288.53, "prediction_parsed_k_with_c_conv": 285.72, "prediction_parsed_k_strict": 285.72, "absolute_error_k_vs_c_conv_pred": 2.8099999999999454, "score_vs_c_conv_pred": 15.010428281229727, "absolute_error_k_vs_strict_pred": 2.8099999999999454, "score_vs_pred_strict": 15.010428281229727, "truth_raw_assistant_text": "The value is 288.53", "prediction_raw_text": "The value is 285.72\uf937"}
{"id": "line_575", "truth_parsed_k": 287.65, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 2.7099999999999795, "score_vs_c_conv_pred": 15.943909993114158, "absolute_error_k_vs_strict_pred": 2.7099999999999795, "score_vs_pred_strict": 15.943909993114158, "truth_raw_assistant_text": "The value is 287.65", "prediction_raw_text": "The value is 284.94."}
{"id": "line_576", "truth_parsed_k": 286.86, "prediction_parsed_k_with_c_conv": 285.83, "prediction_parsed_k_strict": 285.83, "absolute_error_k_vs_c_conv_pred": 1.0300000000000296, "score_vs_c_conv_pred": 40.12390642449682, "absolute_error_k_vs_strict_pred": 1.0300000000000296, "score_vs_pred_strict": 40.12390642449682, "truth_raw_assistant_text": "The value is 286.86", "prediction_raw_text": "The value is 285.83\u0e40\u0e2b\u0e23\u0e35\u0e22"}
{"id": "line_577", "truth_parsed_k": 286.5, "prediction_parsed_k_with_c_conv": 285.92, "prediction_parsed_k_strict": 285.92, "absolute_error_k_vs_c_conv_pred": 0.5799999999999841, "score_vs_c_conv_pred": 53.417248017885676, "absolute_error_k_vs_strict_pred": 0.5799999999999841, "score_vs_pred_strict": 53.417248017885676, "truth_raw_assistant_text": "The value is 286.50", "prediction_raw_text": "The value is 285.92\ufa17"}
{"id": "line_578", "truth_parsed_k": 286.6, "prediction_parsed_k_with_c_conv": 285.69, "prediction_parsed_k_strict": 285.69, "absolute_error_k_vs_c_conv_pred": 0.910000000000025, "score_vs_c_conv_pred": 43.077906381572674, "absolute_error_k_vs_strict_pred": 0.910000000000025, "score_vs_pred_strict": 43.077906381572674, "truth_raw_assistant_text": "The value is 286.60", "prediction_raw_text": "The value is 285.69"}
{"id": "line_579", "truth_parsed_k": 287.12, "prediction_parsed_k_with_c_conv": 284.92, "prediction_parsed_k_strict": 284.92, "absolute_error_k_vs_c_conv_pred": 2.1999999999999886, "score_vs_c_conv_pred": 21.284371806647727, "absolute_error_k_vs_strict_pred": 2.1999999999999886, "score_vs_pred_strict": 21.284371806647727, "truth_raw_assistant_text": "The value is 287.12", "prediction_raw_text": "The value is 284.92\u0e40\u0e2d\u0e47"}
{"id": "line_580", "truth_parsed_k": 288.15, "prediction_parsed_k_with_c_conv": 284.84, "prediction_parsed_k_strict": 284.84, "absolute_error_k_vs_c_conv_pred": 3.3100000000000023, "score_vs_c_conv_pred": 10.774359354368157, "absolute_error_k_vs_strict_pred": 3.3100000000000023, "score_vs_pred_strict": 10.774359354368157, "truth_raw_assistant_text": "The value is 288.15", "prediction_raw_text": "The value is 284.84\u0e0d\u0e35\u0e48\u0e1b\u0e38"}
{"id": "line_581", "truth_parsed_k": 289.07, "prediction_parsed_k_with_c_conv": 284.85, "prediction_parsed_k_strict": 284.85, "absolute_error_k_vs_c_conv_pred": 4.21999999999997, "score_vs_c_conv_pred": 4.44598255645674, "absolute_error_k_vs_strict_pred": 4.21999999999997, "score_vs_pred_strict": 4.44598255645674, "truth_raw_assistant_text": "The value is 289.07", "prediction_raw_text": "The value is 284.85."}
{"id": "line_582", "truth_parsed_k": 289.83, "prediction_parsed_k_with_c_conv": 284.89, "prediction_parsed_k_strict": 284.89, "absolute_error_k_vs_c_conv_pred": 4.939999999999998, "score_vs_c_conv_pred": 0.3171175114497493, "absolute_error_k_vs_strict_pred": 4.939999999999998, "score_vs_pred_strict": 0.3171175114497493, "truth_raw_assistant_text": "The value is 289.83", "prediction_raw_text": "The value is 284.89\ufb2b"}
{"id": "line_583", "truth_parsed_k": 289.96, "prediction_parsed_k_with_c_conv": 285.36, "prediction_parsed_k_strict": 285.36, "absolute_error_k_vs_c_conv_pred": 4.599999999999966, "score_vs_c_conv_pred": 2.1882694779878475, "absolute_error_k_vs_strict_pred": 4.599999999999966, "score_vs_pred_strict": 2.1882694779878475, "truth_raw_assistant_text": "The value is 289.96", "prediction_raw_text": "The value is 285.36."}
{"id": "line_584", "truth_parsed_k": 289.78, "prediction_parsed_k_with_c_conv": 285.92, "prediction_parsed_k_strict": 285.92, "absolute_error_k_vs_c_conv_pred": 3.859999999999957, "score_vs_c_conv_pred": 6.774965134999689, "absolute_error_k_vs_strict_pred": 3.859999999999957, "score_vs_pred_strict": 6.774965134999689, "truth_raw_assistant_text": "The value is 289.78", "prediction_raw_text": "The value is 285.92\u0e22\u0e38\u0e04"}
{"id": "line_585", "truth_parsed_k": 289.17, "prediction_parsed_k_with_c_conv": 284.92, "prediction_parsed_k_strict": 284.92, "absolute_error_k_vs_c_conv_pred": 4.25, "score_vs_c_conv_pred": 4.260695696026007, "absolute_error_k_vs_strict_pred": 4.25, "score_vs_pred_strict": 4.260695696026007, "truth_raw_assistant_text": "The value is 289.17", "prediction_raw_text": "The value is 284.92."}
{"id": "line_586", "truth_parsed_k": 288.31, "prediction_parsed_k_with_c_conv": 284.92, "prediction_parsed_k_strict": 284.92, "absolute_error_k_vs_c_conv_pred": 3.3899999999999864, "score_vs_c_conv_pred": 10.154401018437898, "absolute_error_k_vs_strict_pred": 3.3899999999999864, "score_vs_pred_strict": 10.154401018437898, "truth_raw_assistant_text": "The value is 288.31", "prediction_raw_text": "The value is 284.92\u0e40\u0e0a\u0e35\u0e48\u0e22\u0e27"}
{"id": "line_587", "truth_parsed_k": 287.64, "prediction_parsed_k_with_c_conv": 284.54, "prediction_parsed_k_strict": 284.54, "absolute_error_k_vs_c_conv_pred": 3.099999999999966, "score_vs_c_conv_pred": 12.473100466522569, "absolute_error_k_vs_strict_pred": 3.099999999999966, "score_vs_pred_strict": 12.473100466522569, "truth_raw_assistant_text": "The value is 287.64", "prediction_raw_text": "The value is 284.54\u0e23\u0e32\u0e22\u0e44\u0e14\u0e49"}
{"id": "line_588", "truth_parsed_k": 286.97, "prediction_parsed_k_with_c_conv": 284.95, "prediction_parsed_k_strict": 284.95, "absolute_error_k_vs_c_conv_pred": 2.0200000000000387, "score_vs_c_conv_pred": 23.454263361514727, "absolute_error_k_vs_strict_pred": 2.0200000000000387, "score_vs_pred_strict": 23.454263361514727, "truth_raw_assistant_text": "The value is 286.97", "prediction_raw_text": "The value is 284.95\u0e2a\u0e35\u0e48"}
{"id": "line_589", "truth_parsed_k": 286.44, "prediction_parsed_k_with_c_conv": 284.92, "prediction_parsed_k_strict": 284.92, "absolute_error_k_vs_c_conv_pred": 1.5199999999999818, "score_vs_c_conv_pred": 30.600998725619522, "absolute_error_k_vs_strict_pred": 1.5199999999999818, "score_vs_pred_strict": 30.600998725619522, "truth_raw_assistant_text": "The value is 286.44", "prediction_raw_text": "The value is 284.92"}
{"id": "line_590", "truth_parsed_k": 286.43, "prediction_parsed_k_with_c_conv": 285.97, "prediction_parsed_k_strict": 285.97, "absolute_error_k_vs_c_conv_pred": 0.45999999999997954, "score_vs_c_conv_pred": 58.44168142671445, "absolute_error_k_vs_strict_pred": 0.45999999999997954, "score_vs_pred_strict": 58.44168142671445, "truth_raw_assistant_text": "The value is 286.43", "prediction_raw_text": "The value is 285.97\u0e40\u0e02\u0e49\u0e21"}
{"id": "line_591", "truth_parsed_k": 287.19, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 2.25, "score_vs_c_conv_pred": 20.711408684750577, "absolute_error_k_vs_strict_pred": 2.25, "score_vs_pred_strict": 20.711408684750577, "truth_raw_assistant_text": "The value is 287.19", "prediction_raw_text": "The value is 284.94."}
{"id": "line_592", "truth_parsed_k": 288.12, "prediction_parsed_k_with_c_conv": 284.96, "prediction_parsed_k_strict": 284.96, "absolute_error_k_vs_c_conv_pred": 3.160000000000025, "score_vs_c_conv_pred": 11.97671312415396, "absolute_error_k_vs_strict_pred": 3.160000000000025, "score_vs_pred_strict": 11.97671312415396, "truth_raw_assistant_text": "The value is 288.12", "prediction_raw_text": "The value is 284.96RGBO"}
{"id": "line_593", "truth_parsed_k": 288.99, "prediction_parsed_k_with_c_conv": 284.65, "prediction_parsed_k_strict": 284.65, "absolute_error_k_vs_c_conv_pred": 4.340000000000032, "score_vs_c_conv_pred": 3.7123639509263895, "absolute_error_k_vs_strict_pred": 4.340000000000032, "score_vs_pred_strict": 3.7123639509263895, "truth_raw_assistant_text": "The value is 288.99", "prediction_raw_text": "The value is 284.65."}
{"id": "line_594", "truth_parsed_k": 289.59, "prediction_parsed_k_with_c_conv": 284.95, "prediction_parsed_k_strict": 284.95, "absolute_error_k_vs_c_conv_pred": 4.639999999999986, "score_vs_c_conv_pred": 1.9612649656465808, "absolute_error_k_vs_strict_pred": 4.639999999999986, "score_vs_pred_strict": 1.9612649656465808, "truth_raw_assistant_text": "The value is 289.59", "prediction_raw_text": "The value is 284.95\ufb2e"}
{"id": "line_595", "truth_parsed_k": 289.86, "prediction_parsed_k_with_c_conv": 285.81, "prediction_parsed_k_strict": 285.81, "absolute_error_k_vs_c_conv_pred": 4.050000000000011, "score_vs_c_conv_pred": 5.520726756855332, "absolute_error_k_vs_strict_pred": 4.050000000000011, "score_vs_pred_strict": 5.520726756855332, "truth_raw_assistant_text": "The value is 289.86", "prediction_raw_text": "The value is 285.81\u0e40\u0e0a\u0e34\u0e07"}
{"id": "line_596", "truth_parsed_k": 289.65, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 4.7099999999999795, "score_vs_c_conv_pred": 1.5685543480219444, "absolute_error_k_vs_strict_pred": 4.7099999999999795, "score_vs_pred_strict": 1.5685543480219444, "truth_raw_assistant_text": "The value is 289.65", "prediction_raw_text": "The value is 284.94\u0e41\u0e1c\u0e48\u0e19"}
{"id": "line_597", "truth_parsed_k": 289.19, "prediction_parsed_k_with_c_conv": 285.44, "prediction_parsed_k_strict": 285.44, "absolute_error_k_vs_c_conv_pred": 3.75, "score_vs_c_conv_pred": 7.528731291521307, "absolute_error_k_vs_strict_pred": 3.75, "score_vs_pred_strict": 7.528731291521307, "truth_raw_assistant_text": "The value is 289.19", "prediction_raw_text": "The value is 285.44."}
{"id": "line_598", "truth_parsed_k": 288.28, "prediction_parsed_k_with_c_conv": 286.75, "prediction_parsed_k_strict": 286.75, "absolute_error_k_vs_c_conv_pred": 1.5299999999999727, "score_vs_c_conv_pred": 30.437796706032373, "absolute_error_k_vs_strict_pred": 1.5299999999999727, "score_vs_pred_strict": 30.437796706032373, "truth_raw_assistant_text": "The value is 288.28", "prediction_raw_text": "The value is 286.75\u0e41\u0e21\u0e48"}
{"id": "line_599", "truth_parsed_k": 287.47, "prediction_parsed_k_with_c_conv": 284.85, "prediction_parsed_k_strict": 284.85, "absolute_error_k_vs_c_conv_pred": 2.6200000000000045, "score_vs_c_conv_pred": 16.812638582158147, "absolute_error_k_vs_strict_pred": 2.6200000000000045, "score_vs_pred_strict": 16.812638582158147, "truth_raw_assistant_text": "The value is 287.47", "prediction_raw_text": "The value is 284.85."}
{"id": "line_600", "truth_parsed_k": 286.83, "prediction_parsed_k_with_c_conv": 284.5, "prediction_parsed_k_strict": 284.5, "absolute_error_k_vs_c_conv_pred": 2.329999999999984, "score_vs_c_conv_pred": 19.81929394549701, "absolute_error_k_vs_strict_pred": 2.329999999999984, "score_vs_pred_strict": 19.81929394549701, "truth_raw_assistant_text": "The value is 286.83", "prediction_raw_text": "The value is 284.50\u0e2d\u0e32\u0e0a\u0e35\u0e1e"}
{"id": "line_601", "truth_parsed_k": 286.22, "prediction_parsed_k_with_c_conv": 285.66, "prediction_parsed_k_strict": 285.66, "absolute_error_k_vs_c_conv_pred": 0.5600000000000023, "score_vs_c_conv_pred": 54.19221519323817, "absolute_error_k_vs_strict_pred": 0.5600000000000023, "score_vs_pred_strict": 54.19221519323817, "truth_raw_assistant_text": "The value is 286.22", "prediction_raw_text": "The value is 285.66 \u0623\u0646\u0651"}
{"id": "line_602", "truth_parsed_k": 286.48, "prediction_parsed_k_with_c_conv": 284.72, "prediction_parsed_k_strict": 284.72, "absolute_error_k_vs_c_conv_pred": 1.759999999999991, "score_vs_c_conv_pred": 26.93370927394322, "absolute_error_k_vs_strict_pred": 1.759999999999991, "score_vs_pred_strict": 26.93370927394322, "truth_raw_assistant_text": "The value is 286.48", "prediction_raw_text": "The value is 284.72\u0e23\u0e31\u0e1a\u0e1b\u0e23\u0e30"}
{"id": "line_603", "truth_parsed_k": 287.1, "prediction_parsed_k_with_c_conv": 286.67, "prediction_parsed_k_strict": 286.67, "absolute_error_k_vs_c_conv_pred": 0.4300000000000068, "score_vs_c_conv_pred": 59.85928508539595, "absolute_error_k_vs_strict_pred": 0.4300000000000068, "score_vs_pred_strict": 59.85928508539595, "truth_raw_assistant_text": "The value is 287.10", "prediction_raw_text": "The value is 286.67\u0e40\u0e22\u0e35\u0e48\u0e22\u0e21"}
{"id": "line_604", "truth_parsed_k": 288.07, "prediction_parsed_k_with_c_conv": 284.96, "prediction_parsed_k_strict": 284.96, "absolute_error_k_vs_c_conv_pred": 3.1100000000000136, "score_vs_c_conv_pred": 12.389731202387344, "absolute_error_k_vs_strict_pred": 3.1100000000000136, "score_vs_pred_strict": 12.389731202387344, "truth_raw_assistant_text": "The value is 288.07", "prediction_raw_text": "The value is 284.96."}
{"id": "line_605", "truth_parsed_k": 289.13, "prediction_parsed_k_with_c_conv": 284.91, "prediction_parsed_k_strict": 284.91, "absolute_error_k_vs_c_conv_pred": 4.21999999999997, "score_vs_c_conv_pred": 4.44598255645674, "absolute_error_k_vs_strict_pred": 4.21999999999997, "score_vs_pred_strict": 4.44598255645674, "truth_raw_assistant_text": "The value is 289.13", "prediction_raw_text": "The value is 284.91\u0e04\u0e27\u0e49\u0e32"}
{"id": "line_606", "truth_parsed_k": 289.8, "prediction_parsed_k_with_c_conv": 285.5, "prediction_parsed_k_strict": 285.5, "absolute_error_k_vs_c_conv_pred": 4.300000000000011, "score_vs_c_conv_pred": 3.9546895529864012, "absolute_error_k_vs_strict_pred": 4.300000000000011, "score_vs_pred_strict": 3.9546895529864012, "truth_raw_assistant_text": "The value is 289.80", "prediction_raw_text": "The value is 285.50\u0e41\u0e1e\u0e17\u0e22\u0e4c"}
{"id": "line_607", "truth_parsed_k": 290.0, "prediction_parsed_k_with_c_conv": 284.98, "prediction_parsed_k_strict": 284.98, "absolute_error_k_vs_c_conv_pred": 5.019999999999982, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.019999999999982, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.00", "prediction_raw_text": "The value is 284.98\u0e19\u0e36\u0e01"}
{"id": "line_608", "truth_parsed_k": 289.76, "prediction_parsed_k_with_c_conv": 285.08, "prediction_parsed_k_strict": 285.08, "absolute_error_k_vs_c_conv_pred": 4.680000000000007, "score_vs_c_conv_pred": 1.7361581098770285, "absolute_error_k_vs_strict_pred": 4.680000000000007, "score_vs_pred_strict": 1.7361581098770285, "truth_raw_assistant_text": "The value is 289.76", "prediction_raw_text": "The value is 285.08."}
{"id": "line_609", "truth_parsed_k": 289.35, "prediction_parsed_k_with_c_conv": 284.63, "prediction_parsed_k_strict": 284.63, "absolute_error_k_vs_c_conv_pred": 4.720000000000027, "score_vs_c_conv_pred": 1.512917446398121, "absolute_error_k_vs_strict_pred": 4.720000000000027, "score_vs_pred_strict": 1.512917446398121, "truth_raw_assistant_text": "The value is 289.35", "prediction_raw_text": "The value is 284.63."}
{"id": "line_610", "truth_parsed_k": 288.36, "prediction_parsed_k_with_c_conv": 285.76, "prediction_parsed_k_strict": 285.76, "absolute_error_k_vs_c_conv_pred": 2.6000000000000227, "score_vs_c_conv_pred": 17.00955537049369, "absolute_error_k_vs_strict_pred": 2.6000000000000227, "score_vs_pred_strict": 17.00955537049369, "truth_raw_assistant_text": "The value is 288.36", "prediction_raw_text": "The value is 285.76\u0e17\u0e31\u0e1a"}
{"id": "line_611", "truth_parsed_k": 287.83, "prediction_parsed_k_with_c_conv": 285.65, "prediction_parsed_k_strict": 285.65, "absolute_error_k_vs_c_conv_pred": 2.180000000000007, "score_vs_c_conv_pred": 21.517014729297866, "absolute_error_k_vs_strict_pred": 2.180000000000007, "score_vs_pred_strict": 21.517014729297866, "truth_raw_assistant_text": "The value is 287.83", "prediction_raw_text": "The value is 285.65."}
{"id": "line_612", "truth_parsed_k": 286.91, "prediction_parsed_k_with_c_conv": 284.84, "prediction_parsed_k_strict": 284.84, "absolute_error_k_vs_c_conv_pred": 2.07000000000005, "score_vs_c_conv_pred": 22.83376929991412, "absolute_error_k_vs_strict_pred": 2.07000000000005, "score_vs_pred_strict": 22.83376929991412, "truth_raw_assistant_text": "The value is 286.91", "prediction_raw_text": "The value is 284.84\u0374"}
{"id": "line_613", "truth_parsed_k": 286.48, "prediction_parsed_k_with_c_conv": 285.07, "prediction_parsed_k_strict": 285.07, "absolute_error_k_vs_c_conv_pred": 1.410000000000025, "score_vs_c_conv_pred": 32.46470304951512, "absolute_error_k_vs_strict_pred": 1.410000000000025, "score_vs_pred_strict": 32.46470304951512, "truth_raw_assistant_text": "The value is 286.48", "prediction_raw_text": "The value is 285.07\uf9df"}
{"id": "line_614", "truth_parsed_k": 286.67, "prediction_parsed_k_with_c_conv": 285.06, "prediction_parsed_k_strict": 285.06, "absolute_error_k_vs_c_conv_pred": 1.6100000000000136, "score_vs_c_conv_pred": 29.166610804106018, "absolute_error_k_vs_strict_pred": 1.6100000000000136, "score_vs_pred_strict": 29.166610804106018, "truth_raw_assistant_text": "The value is 286.67", "prediction_raw_text": "The value is 285.06\u0e2d\u0e19\u0e38\u0e0d\u0e32"}
{"id": "line_615", "truth_parsed_k": 287.24, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 2.269999999999982, "score_vs_c_conv_pred": 20.4855939375035, "absolute_error_k_vs_strict_pred": 2.269999999999982, "score_vs_pred_strict": 20.4855939375035, "truth_raw_assistant_text": "The value is 287.24", "prediction_raw_text": "The value is 284.97"}
{"id": "line_616", "truth_parsed_k": 288.23, "prediction_parsed_k_with_c_conv": 284.83, "prediction_parsed_k_strict": 284.83, "absolute_error_k_vs_c_conv_pred": 3.400000000000034, "score_vs_c_conv_pred": 10.077900254467576, "absolute_error_k_vs_strict_pred": 3.400000000000034, "score_vs_pred_strict": 10.077900254467576, "truth_raw_assistant_text": "The value is 288.23", "prediction_raw_text": "The value is 284.83"}
{"id": "line_617", "truth_parsed_k": 289.1, "prediction_parsed_k_with_c_conv": 284.9, "prediction_parsed_k_strict": 284.9, "absolute_error_k_vs_c_conv_pred": 4.2000000000000455, "score_vs_c_conv_pred": 4.570219228066286, "absolute_error_k_vs_strict_pred": 4.2000000000000455, "score_vs_pred_strict": 4.570219228066286, "truth_raw_assistant_text": "The value is 289.10", "prediction_raw_text": "The value is 284.90\u0e08\u0e31\u0e19\u0e17\u0e23\u0e4c"}
{"id": "line_618", "truth_parsed_k": 289.89, "prediction_parsed_k_with_c_conv": 286.7, "prediction_parsed_k_strict": 286.7, "absolute_error_k_vs_c_conv_pred": 3.1899999999999977, "score_vs_c_conv_pred": 11.731909158108634, "absolute_error_k_vs_strict_pred": 3.1899999999999977, "score_vs_pred_strict": 11.731909158108634, "truth_raw_assistant_text": "The value is 289.89", "prediction_raw_text": "The value is 286.70\u0e2a\u0e49\u0e21"}
{"id": "line_619", "truth_parsed_k": 290.09, "prediction_parsed_k_with_c_conv": 285.3, "prediction_parsed_k_strict": 285.3, "absolute_error_k_vs_c_conv_pred": 4.789999999999964, "score_vs_c_conv_pred": 1.126644930375098, "absolute_error_k_vs_strict_pred": 4.789999999999964, "score_vs_pred_strict": 1.126644930375098, "truth_raw_assistant_text": "The value is 290.09", "prediction_raw_text": "The value is 285.30"}
{"id": "line_620", "truth_parsed_k": 289.9, "prediction_parsed_k_with_c_conv": 285.71, "prediction_parsed_k_strict": 285.71, "absolute_error_k_vs_c_conv_pred": 4.189999999999998, "score_vs_c_conv_pred": 4.632553169804399, "absolute_error_k_vs_strict_pred": 4.189999999999998, "score_vs_pred_strict": 4.632553169804399, "truth_raw_assistant_text": "The value is 289.90", "prediction_raw_text": "The value is 285.71 \u0e21\u0e35\u0e19\u0e32\u0e04\u0e21"}
{"id": "line_621", "truth_parsed_k": 289.36, "prediction_parsed_k_with_c_conv": 284.67, "prediction_parsed_k_strict": 284.67, "absolute_error_k_vs_c_conv_pred": 4.689999999999998, "score_vs_c_conv_pred": 1.6801742003018982, "absolute_error_k_vs_strict_pred": 4.689999999999998, "score_vs_pred_strict": 1.6801742003018982, "truth_raw_assistant_text": "The value is 289.36", "prediction_raw_text": "The value is 284.67."}
{"id": "line_622", "truth_parsed_k": 288.5, "prediction_parsed_k_with_c_conv": 286.47, "prediction_parsed_k_strict": 286.47, "absolute_error_k_vs_c_conv_pred": 2.0299999999999727, "score_vs_c_conv_pred": 23.329015457551794, "absolute_error_k_vs_strict_pred": 2.0299999999999727, "score_vs_pred_strict": 23.329015457551794, "truth_raw_assistant_text": "The value is 288.50", "prediction_raw_text": "The value is 286.47\u0e41\u0e19\u0e48\u0e19"}
{"id": "line_623", "truth_parsed_k": 287.71, "prediction_parsed_k_with_c_conv": 284.59, "prediction_parsed_k_strict": 284.59, "absolute_error_k_vs_c_conv_pred": 3.1200000000000045, "score_vs_c_conv_pred": 12.306619251205364, "absolute_error_k_vs_strict_pred": 3.1200000000000045, "score_vs_pred_strict": 12.306619251205364, "truth_raw_assistant_text": "The value is 287.71", "prediction_raw_text": "The value is 284.59\u0e1a\u0e32\u0e23\u0e4c"}
{"id": "line_624", "truth_parsed_k": 287.05, "prediction_parsed_k_with_c_conv": 284.89, "prediction_parsed_k_strict": 284.89, "absolute_error_k_vs_c_conv_pred": 2.160000000000025, "score_vs_c_conv_pred": 21.751685066803127, "absolute_error_k_vs_strict_pred": 2.160000000000025, "score_vs_pred_strict": 21.751685066803127, "truth_raw_assistant_text": "The value is 287.05", "prediction_raw_text": "The value is 284.89\u0e22\u0e31\u0e07\u0e04\u0e07"}
{"id": "line_625", "truth_parsed_k": 286.31, "prediction_parsed_k_with_c_conv": 284.64, "prediction_parsed_k_strict": 284.64, "absolute_error_k_vs_c_conv_pred": 1.670000000000016, "score_vs_c_conv_pred": 28.251114674483503, "absolute_error_k_vs_strict_pred": 1.670000000000016, "score_vs_pred_strict": 28.251114674483503, "truth_raw_assistant_text": "The value is 286.31", "prediction_raw_text": "The value is 284.64"}
{"id": "line_626", "truth_parsed_k": 286.5, "prediction_parsed_k_with_c_conv": 284.95, "prediction_parsed_k_strict": 284.95, "absolute_error_k_vs_c_conv_pred": 1.5500000000000114, "score_vs_c_conv_pred": 30.11433034447373, "absolute_error_k_vs_strict_pred": 1.5500000000000114, "score_vs_pred_strict": 30.11433034447373, "truth_raw_assistant_text": "The value is 286.50", "prediction_raw_text": "The value is 284.95"}
{"id": "line_627", "truth_parsed_k": 287.07, "prediction_parsed_k_with_c_conv": 286.81, "prediction_parsed_k_strict": 286.81, "absolute_error_k_vs_c_conv_pred": 0.2599999999999909, "score_vs_c_conv_pred": 69.70761367278908, "absolute_error_k_vs_strict_pred": 0.2599999999999909, "score_vs_pred_strict": 69.70761367278908, "truth_raw_assistant_text": "The value is 287.07", "prediction_raw_text": "The value is 286.81\u0e40\u0e15\u0e34\u0e1a\u0e42\u0e15"}
{"id": "line_628", "truth_parsed_k": 288.07, "prediction_parsed_k_with_c_conv": 285.99, "prediction_parsed_k_strict": 285.99, "absolute_error_k_vs_c_conv_pred": 2.079999999999984, "score_vs_c_conv_pred": 22.711367949139248, "absolute_error_k_vs_strict_pred": 2.079999999999984, "score_vs_pred_strict": 22.711367949139248, "truth_raw_assistant_text": "The value is 288.07", "prediction_raw_text": "The value is 285.99\u0e40\u0e1b\u0e47\u0e19\u0e04\u0e19"}
{"id": "line_629", "truth_parsed_k": 289.13, "prediction_parsed_k_with_c_conv": 285.81, "prediction_parsed_k_strict": 285.81, "absolute_error_k_vs_c_conv_pred": 3.319999999999993, "score_vs_c_conv_pred": 10.696079500386857, "absolute_error_k_vs_strict_pred": 3.319999999999993, "score_vs_pred_strict": 10.696079500386857, "truth_raw_assistant_text": "The value is 289.13", "prediction_raw_text": "The value is 285.81\u0e15\u0e01\u0e41\u0e15\u0e48\u0e07"}
{"id": "line_630", "truth_parsed_k": 289.73, "prediction_parsed_k_with_c_conv": 284.5, "prediction_parsed_k_strict": 284.5, "absolute_error_k_vs_c_conv_pred": 5.230000000000018, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.230000000000018, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 289.73", "prediction_raw_text": "The value is 284.50\u0e14\u0e48\u0e27\u0e19"}
{"id": "line_631", "truth_parsed_k": 289.94, "prediction_parsed_k_with_c_conv": 284.88, "prediction_parsed_k_strict": 284.88, "absolute_error_k_vs_c_conv_pred": 5.060000000000002, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.060000000000002, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 289.94", "prediction_raw_text": "The value is 284.88 \u0e25\u0e49\u0e32\u0e19"}
{"id": "line_632", "truth_parsed_k": 289.96, "prediction_parsed_k_with_c_conv": 285.62, "prediction_parsed_k_strict": 285.62, "absolute_error_k_vs_c_conv_pred": 4.339999999999975, "score_vs_c_conv_pred": 3.7123639509267226, "absolute_error_k_vs_strict_pred": 4.339999999999975, "score_vs_pred_strict": 3.7123639509267226, "truth_raw_assistant_text": "The value is 289.96", "prediction_raw_text": "The value is 285.62\u0e20\u0e32\u0e1e\u0e22\u0e19\u0e15\u0e23\u0e4c"}
{"id": "line_633", "truth_parsed_k": 289.31, "prediction_parsed_k_with_c_conv": 285.95, "prediction_parsed_k_strict": 285.95, "absolute_error_k_vs_c_conv_pred": 3.3600000000000136, "score_vs_c_conv_pred": 10.385216009938524, "absolute_error_k_vs_strict_pred": 3.3600000000000136, "score_vs_pred_strict": 10.385216009938524, "truth_raw_assistant_text": "The value is 289.31", "prediction_raw_text": "The value is 285.95\u0e40\u0e15\u0e34\u0e1a\u0e42\u0e15"}
{"id": "line_634", "truth_parsed_k": 288.39, "prediction_parsed_k_with_c_conv": 285.49, "prediction_parsed_k_strict": 285.49, "absolute_error_k_vs_c_conv_pred": 2.8999999999999773, "score_vs_c_conv_pred": 14.197097136438963, "absolute_error_k_vs_strict_pred": 2.8999999999999773, "score_vs_pred_strict": 14.197097136438963, "truth_raw_assistant_text": "The value is 288.39", "prediction_raw_text": "The value is 285.49 \u0e02\u0e13\u0e30\u0e17\u0e35\u0e48"}
{"id": "line_635", "truth_parsed_k": 287.64, "prediction_parsed_k_with_c_conv": 285.82, "prediction_parsed_k_strict": 285.82, "absolute_error_k_vs_c_conv_pred": 1.8199999999999932, "score_vs_c_conv_pred": 26.08993525777755, "absolute_error_k_vs_strict_pred": 1.8199999999999932, "score_vs_pred_strict": 26.08993525777755, "truth_raw_assistant_text": "The value is 287.64", "prediction_raw_text": "The value is 285.82\u0e2a\u0e21\u0e31"}
{"id": "line_636", "truth_parsed_k": 286.94, "prediction_parsed_k_with_c_conv": 284.92, "prediction_parsed_k_strict": 284.92, "absolute_error_k_vs_c_conv_pred": 2.019999999999982, "score_vs_c_conv_pred": 23.454263361515448, "absolute_error_k_vs_strict_pred": 2.019999999999982, "score_vs_pred_strict": 23.454263361515448, "truth_raw_assistant_text": "The value is 286.94", "prediction_raw_text": "The value is 284.92\u0e1e\u0e31"}
{"id": "line_637", "truth_parsed_k": 286.71, "prediction_parsed_k_with_c_conv": 285.29, "prediction_parsed_k_strict": 285.29, "absolute_error_k_vs_c_conv_pred": 1.419999999999959, "score_vs_c_conv_pred": 32.28984366362286, "absolute_error_k_vs_strict_pred": 1.419999999999959, "score_vs_pred_strict": 32.28984366362286, "truth_raw_assistant_text": "The value is 286.71", "prediction_raw_text": "The value is 285.29."}
{"id": "line_638", "truth_parsed_k": 286.54, "prediction_parsed_k_with_c_conv": 284.84, "prediction_parsed_k_strict": 284.84, "absolute_error_k_vs_c_conv_pred": 1.7000000000000455, "score_vs_c_conv_pred": 27.80477985731814, "absolute_error_k_vs_strict_pred": 1.7000000000000455, "score_vs_pred_strict": 27.80477985731814, "truth_raw_assistant_text": "The value is 286.54", "prediction_raw_text": "The value is 284.84."}
{"id": "line_639", "truth_parsed_k": 287.15, "prediction_parsed_k_with_c_conv": 285.91, "prediction_parsed_k_strict": 285.91, "absolute_error_k_vs_c_conv_pred": 1.2399999999999523, "score_vs_c_conv_pred": 35.62543213444861, "absolute_error_k_vs_strict_pred": 1.2399999999999523, "score_vs_pred_strict": 35.62543213444861, "truth_raw_assistant_text": "The value is 287.15", "prediction_raw_text": "The value is 285.91."}
{"id": "line_640", "truth_parsed_k": 288.37, "prediction_parsed_k_with_c_conv": 285.9, "prediction_parsed_k_strict": 285.9, "absolute_error_k_vs_c_conv_pred": 2.4700000000000273, "score_vs_c_conv_pred": 18.325859743192574, "absolute_error_k_vs_strict_pred": 2.4700000000000273, "score_vs_pred_strict": 18.325859743192574, "truth_raw_assistant_text": "The value is 288.37", "prediction_raw_text": "The value is 285.90\u0e19\u0e49\u0e33\u0e15\u0e32"}
{"id": "line_641", "truth_parsed_k": 289.19, "prediction_parsed_k_with_c_conv": 287.5, "prediction_parsed_k_strict": 287.5, "absolute_error_k_vs_c_conv_pred": 1.6899999999999977, "score_vs_c_conv_pred": 27.95273765156511, "absolute_error_k_vs_strict_pred": 1.6899999999999977, "score_vs_pred_strict": 27.95273765156511, "truth_raw_assistant_text": "The value is 289.19", "prediction_raw_text": "The value is 287.50\u0e01\u0e32\u0e23\u0e08\u0e31\u0e14"}
{"id": "line_642", "truth_parsed_k": 289.75, "prediction_parsed_k_with_c_conv": 285.84, "prediction_parsed_k_strict": 285.84, "absolute_error_k_vs_c_conv_pred": 3.910000000000025, "score_vs_c_conv_pred": 6.439197058986112, "absolute_error_k_vs_strict_pred": 3.910000000000025, "score_vs_pred_strict": 6.439197058986112, "truth_raw_assistant_text": "The value is 289.75", "prediction_raw_text": "The value is 285.84\u0e2a\u0e33\u0e19\u0e31\u0e01"}
{"id": "line_643", "truth_parsed_k": 290.11, "prediction_parsed_k_with_c_conv": 284.87, "prediction_parsed_k_strict": 284.87, "absolute_error_k_vs_c_conv_pred": 5.240000000000009, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.240000000000009, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.11", "prediction_raw_text": "The value is 284.87\u0e43\u0e2b\u0e49\u0e1a\u0e23\u0e34\u0e01\u0e32\u0e23"}
{"id": "line_644", "truth_parsed_k": 290.0, "prediction_parsed_k_with_c_conv": 286.95, "prediction_parsed_k_strict": 286.95, "absolute_error_k_vs_c_conv_pred": 3.0500000000000114, "score_vs_c_conv_pred": 12.893862945946688, "absolute_error_k_vs_strict_pred": 3.0500000000000114, "score_vs_pred_strict": 12.893862945946688, "truth_raw_assistant_text": "The value is 290.00", "prediction_raw_text": "The value is 286.95."}
{"id": "line_645", "truth_parsed_k": 289.29, "prediction_parsed_k_with_c_conv": 284.68, "prediction_parsed_k_strict": 284.68, "absolute_error_k_vs_c_conv_pred": 4.610000000000014, "score_vs_c_conv_pred": 2.131338693978002, "absolute_error_k_vs_strict_pred": 4.610000000000014, "score_vs_pred_strict": 2.131338693978002, "truth_raw_assistant_text": "The value is 289.29", "prediction_raw_text": "The value is 284.68\u0e2d\u0e35\u0e40\u0e21\u0e25"}
{"id": "line_646", "truth_parsed_k": 288.61, "prediction_parsed_k_with_c_conv": 286.85, "prediction_parsed_k_strict": 286.85, "absolute_error_k_vs_c_conv_pred": 1.759999999999991, "score_vs_c_conv_pred": 26.93370927394322, "absolute_error_k_vs_strict_pred": 1.759999999999991, "score_vs_pred_strict": 26.93370927394322, "truth_raw_assistant_text": "The value is 288.61", "prediction_raw_text": "The value is 286.85."}
{"id": "line_647", "truth_parsed_k": 287.78, "prediction_parsed_k_with_c_conv": 285.72, "prediction_parsed_k_strict": 285.72, "absolute_error_k_vs_c_conv_pred": 2.0599999999999454, "score_vs_c_conv_pred": 22.956729562906443, "absolute_error_k_vs_strict_pred": 2.0599999999999454, "score_vs_pred_strict": 22.956729562906443, "truth_raw_assistant_text": "The value is 287.78", "prediction_raw_text": "The value is 285.72."}
{"id": "line_648", "truth_parsed_k": 287.01, "prediction_parsed_k_with_c_conv": 285.97, "prediction_parsed_k_strict": 285.97, "absolute_error_k_vs_c_conv_pred": 1.0399999999999636, "score_vs_c_conv_pred": 39.891764893674164, "absolute_error_k_vs_strict_pred": 1.0399999999999636, "score_vs_pred_strict": 39.891764893674164, "truth_raw_assistant_text": "The value is 287.01", "prediction_raw_text": "The value is 285.97\uf981"}
{"id": "line_649", "truth_parsed_k": 286.69, "prediction_parsed_k_with_c_conv": 285.67, "prediction_parsed_k_strict": 285.67, "absolute_error_k_vs_c_conv_pred": 1.0199999999999818, "score_vs_c_conv_pred": 40.358066602659534, "absolute_error_k_vs_strict_pred": 1.0199999999999818, "score_vs_pred_strict": 40.358066602659534, "truth_raw_assistant_text": "The value is 286.69", "prediction_raw_text": "The value is 285.67 \u0e2a\u0e34\u0e07\u0e2b\u0e32"}
{"id": "line_650", "truth_parsed_k": 286.75, "prediction_parsed_k_with_c_conv": 284.72, "prediction_parsed_k_strict": 284.72, "absolute_error_k_vs_c_conv_pred": 2.0299999999999727, "score_vs_c_conv_pred": 23.329015457551794, "absolute_error_k_vs_strict_pred": 2.0299999999999727, "score_vs_pred_strict": 23.329015457551794, "truth_raw_assistant_text": "The value is 286.75", "prediction_raw_text": "The value is 284.72."}
{"id": "line_651", "truth_parsed_k": 287.38, "prediction_parsed_k_with_c_conv": 286.77, "prediction_parsed_k_strict": 286.77, "absolute_error_k_vs_c_conv_pred": 0.6100000000000136, "score_vs_c_conv_pred": 52.29507519742948, "absolute_error_k_vs_strict_pred": 0.6100000000000136, "score_vs_pred_strict": 52.29507519742948, "truth_raw_assistant_text": "The value is 287.38", "prediction_raw_text": "The value is 286.77."}
{"id": "line_652", "truth_parsed_k": 288.36, "prediction_parsed_k_with_c_conv": 285.88, "prediction_parsed_k_strict": 285.88, "absolute_error_k_vs_c_conv_pred": 2.480000000000018, "score_vs_c_conv_pred": 18.222289420107728, "absolute_error_k_vs_strict_pred": 2.480000000000018, "score_vs_pred_strict": 18.222289420107728, "truth_raw_assistant_text": "The value is 288.36", "prediction_raw_text": "The value is 285.88"}
{"id": "line_653", "truth_parsed_k": 289.32, "prediction_parsed_k_with_c_conv": 285.56, "prediction_parsed_k_strict": 285.56, "absolute_error_k_vs_c_conv_pred": 3.759999999999991, "score_vs_c_conv_pred": 7.4593285448395825, "absolute_error_k_vs_strict_pred": 3.759999999999991, "score_vs_pred_strict": 7.4593285448395825, "truth_raw_assistant_text": "The value is 289.32", "prediction_raw_text": "The value is 285.56\u0e17\u0e35\u0e48\u0e21\u0e35"}
{"id": "line_654", "truth_parsed_k": 289.85, "prediction_parsed_k_with_c_conv": 286.57, "prediction_parsed_k_strict": 286.57, "absolute_error_k_vs_c_conv_pred": 3.2800000000000296, "score_vs_c_conv_pred": 11.010573595407891, "absolute_error_k_vs_strict_pred": 3.2800000000000296, "score_vs_pred_strict": 11.010573595407891, "truth_raw_assistant_text": "The value is 289.85", "prediction_raw_text": "The value is 286.57."}
{"id": "line_655", "truth_parsed_k": 290.12, "prediction_parsed_k_with_c_conv": 285.66, "prediction_parsed_k_strict": 285.66, "absolute_error_k_vs_c_conv_pred": 4.4599999999999795, "score_vs_c_conv_pred": 2.9982027362366437, "absolute_error_k_vs_strict_pred": 4.4599999999999795, "score_vs_pred_strict": 2.9982027362366437, "truth_raw_assistant_text": "The value is 290.12", "prediction_raw_text": "The value is 285.66"}
{"id": "line_656", "truth_parsed_k": 289.95, "prediction_parsed_k_with_c_conv": 288.75, "prediction_parsed_k_strict": 288.75, "absolute_error_k_vs_c_conv_pred": 1.1999999999999886, "score_vs_c_conv_pred": 36.42633134050889, "absolute_error_k_vs_strict_pred": 1.1999999999999886, "score_vs_pred_strict": 36.42633134050889, "truth_raw_assistant_text": "The value is 289.95", "prediction_raw_text": "The value is 288.75\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21"}
{"id": "line_657", "truth_parsed_k": 289.28, "prediction_parsed_k_with_c_conv": 286.85, "prediction_parsed_k_strict": 286.85, "absolute_error_k_vs_c_conv_pred": 2.42999999999995, "score_vs_c_conv_pred": 18.744171080179783, "absolute_error_k_vs_strict_pred": 2.42999999999995, "score_vs_pred_strict": 18.744171080179783, "truth_raw_assistant_text": "The value is 289.28", "prediction_raw_text": "The value is 286.85 \u0e01\u0e38\u0e21\u0e20\u0e32\u0e1e"}
{"id": "line_658", "truth_parsed_k": 288.51, "prediction_parsed_k_with_c_conv": 285.8, "prediction_parsed_k_strict": 285.8, "absolute_error_k_vs_c_conv_pred": 2.7099999999999795, "score_vs_c_conv_pred": 15.943909993114158, "absolute_error_k_vs_strict_pred": 2.7099999999999795, "score_vs_pred_strict": 15.943909993114158, "truth_raw_assistant_text": "The value is 288.51", "prediction_raw_text": "The value is 285.80\uf9c7"}
{"id": "line_659", "truth_parsed_k": 287.77, "prediction_parsed_k_with_c_conv": 284.98, "prediction_parsed_k_strict": 284.98, "absolute_error_k_vs_c_conv_pred": 2.7899999999999636, "score_vs_c_conv_pred": 15.194553790436993, "absolute_error_k_vs_strict_pred": 2.7899999999999636, "score_vs_pred_strict": 15.194553790436993, "truth_raw_assistant_text": "The value is 287.77", "prediction_raw_text": "The value is 284.98\u0e01\u0e25\u0e49\u0e2d\u0e07"}
{"id": "line_660", "truth_parsed_k": 287.03, "prediction_parsed_k_with_c_conv": 285.69, "prediction_parsed_k_strict": 285.69, "absolute_error_k_vs_c_conv_pred": 1.339999999999975, "score_vs_c_conv_pred": 33.72158288735778, "absolute_error_k_vs_strict_pred": 1.339999999999975, "score_vs_pred_strict": 33.72158288735778, "truth_raw_assistant_text": "The value is 287.03", "prediction_raw_text": "The value is 285.69."}
{"id": "line_661", "truth_parsed_k": 286.64, "prediction_parsed_k_with_c_conv": 285.98, "prediction_parsed_k_strict": 285.98, "absolute_error_k_vs_c_conv_pred": 0.6599999999999682, "score_vs_c_conv_pred": 50.52284034141065, "absolute_error_k_vs_strict_pred": 0.6599999999999682, "score_vs_pred_strict": 50.52284034141065, "truth_raw_assistant_text": "The value is 286.64", "prediction_raw_text": "The value is 285.98\u0e2a\u0e31\u0e48\u0e07\u0e0b\u0e37\u0e49\u0e2d"}
{"id": "line_662", "truth_parsed_k": 286.74, "prediction_parsed_k_with_c_conv": 285.64, "prediction_parsed_k_strict": 285.64, "absolute_error_k_vs_c_conv_pred": 1.1000000000000227, "score_vs_c_conv_pred": 38.53943468230319, "absolute_error_k_vs_strict_pred": 1.1000000000000227, "score_vs_pred_strict": 38.53943468230319, "truth_raw_assistant_text": "The value is 286.74", "prediction_raw_text": "The value is 285.64"}
{"id": "line_663", "truth_parsed_k": 287.43, "prediction_parsed_k_with_c_conv": 285.96, "prediction_parsed_k_strict": 285.96, "absolute_error_k_vs_c_conv_pred": 1.4700000000000273, "score_vs_c_conv_pred": 31.432183563992467, "absolute_error_k_vs_strict_pred": 1.4700000000000273, "score_vs_pred_strict": 31.432183563992467, "truth_raw_assistant_text": "The value is 287.43", "prediction_raw_text": "The value is 285.96."}
{"id": "line_664", "truth_parsed_k": 288.37, "prediction_parsed_k_with_c_conv": 285.65, "prediction_parsed_k_strict": 285.65, "absolute_error_k_vs_c_conv_pred": 2.7200000000000273, "score_vs_c_conv_pred": 15.84909211061859, "absolute_error_k_vs_strict_pred": 2.7200000000000273, "score_vs_pred_strict": 15.84909211061859, "truth_raw_assistant_text": "The value is 288.37", "prediction_raw_text": "The value is 285.65."}
{"id": "line_665", "truth_parsed_k": 289.4, "prediction_parsed_k_with_c_conv": 284.65, "prediction_parsed_k_strict": 284.65, "absolute_error_k_vs_c_conv_pred": 4.75, "score_vs_c_conv_pred": 1.3466931481699862, "absolute_error_k_vs_strict_pred": 4.75, "score_vs_pred_strict": 1.3466931481699862, "truth_raw_assistant_text": "The value is 289.40", "prediction_raw_text": "The value is 284.65"}
{"id": "line_666", "truth_parsed_k": 289.98, "prediction_parsed_k_with_c_conv": 284.85, "prediction_parsed_k_strict": 284.85, "absolute_error_k_vs_c_conv_pred": 5.1299999999999955, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.1299999999999955, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 289.98", "prediction_raw_text": "The value is 284.85"}
{"id": "line_667", "truth_parsed_k": 290.23, "prediction_parsed_k_with_c_conv": 285.91, "prediction_parsed_k_strict": 285.91, "absolute_error_k_vs_c_conv_pred": 4.319999999999993, "score_vs_c_conv_pred": 3.8332541688072874, "absolute_error_k_vs_strict_pred": 4.319999999999993, "score_vs_pred_strict": 3.8332541688072874, "truth_raw_assistant_text": "The value is 290.23", "prediction_raw_text": "The value is 285.91 \u0e15\u0e38"}
{"id": "line_668", "truth_parsed_k": 290.0, "prediction_parsed_k_with_c_conv": 285.72, "prediction_parsed_k_strict": 285.72, "absolute_error_k_vs_c_conv_pred": 4.279999999999973, "score_vs_c_conv_pred": 4.076675042708677, "absolute_error_k_vs_strict_pred": 4.279999999999973, "score_vs_pred_strict": 4.076675042708677, "truth_raw_assistant_text": "The value is 290.00", "prediction_raw_text": "The value is 285.72"}
{"id": "line_669", "truth_parsed_k": 289.51, "prediction_parsed_k_with_c_conv": 285.79, "prediction_parsed_k_strict": 285.79, "absolute_error_k_vs_c_conv_pred": 3.7199999999999704, "score_vs_c_conv_pred": 7.738019268383123, "absolute_error_k_vs_strict_pred": 3.7199999999999704, "score_vs_pred_strict": 7.738019268383123, "truth_raw_assistant_text": "The value is 289.51", "prediction_raw_text": "The value is 285.79\u0e19\u0e30\u0e04\u0e23\u0e31\u0e1a"}
{"id": "line_670", "truth_parsed_k": 288.73, "prediction_parsed_k_with_c_conv": 284.78, "prediction_parsed_k_strict": 284.78, "absolute_error_k_vs_c_conv_pred": 3.9500000000000455, "score_vs_c_conv_pred": 6.173564810742905, "absolute_error_k_vs_strict_pred": 3.9500000000000455, "score_vs_pred_strict": 6.173564810742905, "truth_raw_assistant_text": "The value is 288.73", "prediction_raw_text": "The value is 284.78\u0e40\u0e01\u0e2d\u0e23\u0e4c"}
{"id": "line_671", "truth_parsed_k": 287.78, "prediction_parsed_k_with_c_conv": 286.91, "prediction_parsed_k_strict": 286.91, "absolute_error_k_vs_c_conv_pred": 0.8699999999999477, "score_vs_c_conv_pred": 44.139255611720564, "absolute_error_k_vs_strict_pred": 0.8699999999999477, "score_vs_pred_strict": 44.139255611720564, "truth_raw_assistant_text": "The value is 287.78", "prediction_raw_text": "The value is 286.91\uf94f"}
{"id": "line_672", "truth_parsed_k": 287.27, "prediction_parsed_k_with_c_conv": 284.78, "prediction_parsed_k_strict": 284.78, "absolute_error_k_vs_c_conv_pred": 2.490000000000009, "score_vs_c_conv_pred": 18.119115919156847, "absolute_error_k_vs_strict_pred": 2.490000000000009, "score_vs_pred_strict": 18.119115919156847, "truth_raw_assistant_text": "The value is 287.27", "prediction_raw_text": "The value is 284.78\u0e1d\u0e31\u0e19"}
{"id": "line_673", "truth_parsed_k": 287.12, "prediction_parsed_k_with_c_conv": 285.84, "prediction_parsed_k_strict": 285.84, "absolute_error_k_vs_c_conv_pred": 1.2800000000000296, "score_vs_c_conv_pred": 34.84766685535777, "absolute_error_k_vs_strict_pred": 1.2800000000000296, "score_vs_pred_strict": 34.84766685535777, "truth_raw_assistant_text": "The value is 287.12", "prediction_raw_text": "The value is 285.84."}
{"id": "line_674", "truth_parsed_k": 287.24, "prediction_parsed_k_with_c_conv": 287.66, "prediction_parsed_k_strict": 287.66, "absolute_error_k_vs_c_conv_pred": 0.4200000000000159, "score_vs_c_conv_pred": 60.34890313391036, "absolute_error_k_vs_strict_pred": 0.4200000000000159, "score_vs_pred_strict": 60.34890313391036, "truth_raw_assistant_text": "The value is 287.24", "prediction_raw_text": "The value is 287.66\u0e17\u0e35\u0e48\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23"}
{"id": "line_675", "truth_parsed_k": 287.67, "prediction_parsed_k_with_c_conv": 284.81, "prediction_parsed_k_strict": 284.81, "absolute_error_k_vs_c_conv_pred": 2.8600000000000136, "score_vs_c_conv_pred": 14.555548363428105, "absolute_error_k_vs_strict_pred": 2.8600000000000136, "score_vs_pred_strict": 14.555548363428105, "truth_raw_assistant_text": "The value is 287.67", "prediction_raw_text": "The value is 284.81\u0e2d\u0e35\u0e01\u0e04\u0e23\u0e31\u0e49\u0e07"}
{"id": "line_676", "truth_parsed_k": 288.49, "prediction_parsed_k_with_c_conv": 285.54, "prediction_parsed_k_strict": 285.54, "absolute_error_k_vs_c_conv_pred": 2.9499999999999886, "score_vs_c_conv_pred": 13.7556405151264, "absolute_error_k_vs_strict_pred": 2.9499999999999886, "score_vs_pred_strict": 13.7556405151264, "truth_raw_assistant_text": "The value is 288.49", "prediction_raw_text": "The value is 285.54."}
{"id": "line_677", "truth_parsed_k": 289.4, "prediction_parsed_k_with_c_conv": 286.32, "prediction_parsed_k_strict": 286.32, "absolute_error_k_vs_c_conv_pred": 3.079999999999984, "score_vs_c_conv_pred": 12.640617341470882, "absolute_error_k_vs_strict_pred": 3.079999999999984, "score_vs_pred_strict": 12.640617341470882, "truth_raw_assistant_text": "The value is 289.40", "prediction_raw_text": "The value is 286.32."}
{"id": "line_678", "truth_parsed_k": 290.29, "prediction_parsed_k_with_c_conv": 284.89, "prediction_parsed_k_strict": 284.89, "absolute_error_k_vs_c_conv_pred": 5.400000000000034, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.400000000000034, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.29", "prediction_raw_text": "The value is 284.89."}
{"id": "line_679", "truth_parsed_k": 290.43, "prediction_parsed_k_with_c_conv": 285.34, "prediction_parsed_k_strict": 285.34, "absolute_error_k_vs_c_conv_pred": 5.090000000000032, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.090000000000032, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.43", "prediction_raw_text": "The value is 285.34."}
{"id": "line_680", "truth_parsed_k": 290.32, "prediction_parsed_k_with_c_conv": 287.65, "prediction_parsed_k_strict": 287.65, "absolute_error_k_vs_c_conv_pred": 2.670000000000016, "score_vs_c_conv_pred": 16.326555874691216, "absolute_error_k_vs_strict_pred": 2.670000000000016, "score_vs_pred_strict": 16.326555874691216, "truth_raw_assistant_text": "The value is 290.32", "prediction_raw_text": "The value is 287.65\u0e40\u0e19\u0e35\u0e22"}
{"id": "line_681", "truth_parsed_k": 289.62, "prediction_parsed_k_with_c_conv": 284.93, "prediction_parsed_k_strict": 284.93, "absolute_error_k_vs_c_conv_pred": 4.689999999999998, "score_vs_c_conv_pred": 1.6801742003018982, "absolute_error_k_vs_strict_pred": 4.689999999999998, "score_vs_pred_strict": 1.6801742003018982, "truth_raw_assistant_text": "The value is 289.62", "prediction_raw_text": "The value is 284.93\u0e2a\u0e39\u0e15\u0e23"}
{"id": "line_682", "truth_parsed_k": 288.97, "prediction_parsed_k_with_c_conv": 285.40175708008, "prediction_parsed_k_strict": 285.40175708008, "absolute_error_k_vs_c_conv_pred": 3.56824291992001, "score_vs_c_conv_pred": 8.822384673145955, "absolute_error_k_vs_strict_pred": 3.56824291992001, "score_vs_pred_strict": 8.822384673145955, "truth_raw_assistant_text": "The value is 288.97", "prediction_raw_text": "The value is 285.40175708008."}
{"id": "line_683", "truth_parsed_k": 288.34, "prediction_parsed_k_with_c_conv": 284.64, "prediction_parsed_k_strict": 284.64, "absolute_error_k_vs_c_conv_pred": 3.6999999999999886, "score_vs_c_conv_pred": 7.878453749939618, "absolute_error_k_vs_strict_pred": 3.6999999999999886, "score_vs_pred_strict": 7.878453749939618, "truth_raw_assistant_text": "The value is 288.34", "prediction_raw_text": "The value is 284.64."}
{"id": "line_684", "truth_parsed_k": 287.37, "prediction_parsed_k_with_c_conv": 285.69, "prediction_parsed_k_strict": 285.69, "absolute_error_k_vs_c_conv_pred": 1.6800000000000068, "score_vs_c_conv_pred": 28.1015128963355, "absolute_error_k_vs_strict_pred": 1.6800000000000068, "score_vs_pred_strict": 28.1015128963355, "truth_raw_assistant_text": "The value is 287.37", "prediction_raw_text": "The value is 285.69\u0e08\u0e30\u0e15\u0e49\u0e2d\u0e07"}
{"id": "line_685", "truth_parsed_k": 287.03, "prediction_parsed_k_with_c_conv": 285.68, "prediction_parsed_k_strict": 285.68, "absolute_error_k_vs_c_conv_pred": 1.349999999999966, "score_vs_c_conv_pred": 33.538396801278026, "absolute_error_k_vs_strict_pred": 1.349999999999966, "score_vs_pred_strict": 33.538396801278026, "truth_raw_assistant_text": "The value is 287.03", "prediction_raw_text": "The value is 285.68\u0e14\u0e37\u0e48"}
{"id": "line_686", "truth_parsed_k": 287.15, "prediction_parsed_k_with_c_conv": 284.93, "prediction_parsed_k_strict": 284.93, "absolute_error_k_vs_c_conv_pred": 2.2199999999999704, "score_vs_c_conv_pred": 21.053721567334694, "absolute_error_k_vs_strict_pred": 2.2199999999999704, "score_vs_pred_strict": 21.053721567334694, "truth_raw_assistant_text": "The value is 287.15", "prediction_raw_text": "The value is 284.93\uf9d1"}
{"id": "line_687", "truth_parsed_k": 287.67, "prediction_parsed_k_with_c_conv": 286.34, "prediction_parsed_k_strict": 286.34, "absolute_error_k_vs_c_conv_pred": 1.330000000000041, "score_vs_c_conv_pred": 33.906023682433236, "absolute_error_k_vs_strict_pred": 1.330000000000041, "score_vs_pred_strict": 33.906023682433236, "truth_raw_assistant_text": "The value is 287.67", "prediction_raw_text": "The value is 286.34\u0e25\u0e38\u0e01"}
{"id": "line_688", "truth_parsed_k": 288.57, "prediction_parsed_k_with_c_conv": 285.85, "prediction_parsed_k_strict": 285.85, "absolute_error_k_vs_c_conv_pred": 2.7199999999999704, "score_vs_c_conv_pred": 15.849092110619134, "absolute_error_k_vs_strict_pred": 2.7199999999999704, "score_vs_pred_strict": 15.849092110619134, "truth_raw_assistant_text": "The value is 288.57", "prediction_raw_text": "The value is 285.85."}
{"id": "line_689", "truth_parsed_k": 289.58, "prediction_parsed_k_with_c_conv": 288.35, "prediction_parsed_k_strict": 288.35, "absolute_error_k_vs_c_conv_pred": 1.2299999999999613, "score_vs_c_conv_pred": 35.823434870713754, "absolute_error_k_vs_strict_pred": 1.2299999999999613, "score_vs_pred_strict": 35.823434870713754, "truth_raw_assistant_text": "The value is 289.58", "prediction_raw_text": "The value is 288.35."}
{"id": "line_690", "truth_parsed_k": 290.23, "prediction_parsed_k_with_c_conv": 285.6, "prediction_parsed_k_strict": 285.6, "absolute_error_k_vs_c_conv_pred": 4.6299999999999955, "score_vs_c_conv_pred": 2.0178369419290965, "absolute_error_k_vs_strict_pred": 4.6299999999999955, "score_vs_pred_strict": 2.0178369419290965, "truth_raw_assistant_text": "The value is 290.23", "prediction_raw_text": "The value is 285.60\u0e1b\u0e39"}
{"id": "line_691", "truth_parsed_k": 290.51, "prediction_parsed_k_with_c_conv": 287.61, "prediction_parsed_k_strict": 287.61, "absolute_error_k_vs_c_conv_pred": 2.8999999999999773, "score_vs_c_conv_pred": 14.197097136438963, "absolute_error_k_vs_strict_pred": 2.8999999999999773, "score_vs_pred_strict": 14.197097136438963, "truth_raw_assistant_text": "The value is 290.51", "prediction_raw_text": "The value is 287.61\u0e2d\u0e07\u0e04\u0e4c\u0e01\u0e23"}
{"id": "line_692", "truth_parsed_k": 290.41, "prediction_parsed_k_with_c_conv": 285.9, "prediction_parsed_k_strict": 285.9, "absolute_error_k_vs_c_conv_pred": 4.510000000000048, "score_vs_c_conv_pred": 2.706136415330951, "absolute_error_k_vs_strict_pred": 4.510000000000048, "score_vs_pred_strict": 2.706136415330951, "truth_raw_assistant_text": "The value is 290.41", "prediction_raw_text": "The value is 285.90\u0e43\u0e15\u0e49"}
{"id": "line_693", "truth_parsed_k": 289.7, "prediction_parsed_k_with_c_conv": 285.79, "prediction_parsed_k_strict": 285.79, "absolute_error_k_vs_c_conv_pred": 3.909999999999968, "score_vs_c_conv_pred": 6.439197058986501, "absolute_error_k_vs_strict_pred": 3.909999999999968, "score_vs_pred_strict": 6.439197058986501, "truth_raw_assistant_text": "The value is 289.70", "prediction_raw_text": "The value is 285.79"}
{"id": "line_694", "truth_parsed_k": 288.9, "prediction_parsed_k_with_c_conv": 284.8, "prediction_parsed_k_strict": 284.8, "absolute_error_k_vs_c_conv_pred": 4.099999999999966, "score_vs_c_conv_pred": 5.200148556339279, "absolute_error_k_vs_strict_pred": 4.099999999999966, "score_vs_pred_strict": 5.200148556339279, "truth_raw_assistant_text": "The value is 288.90", "prediction_raw_text": "The value is 284.80\u0e25\u0e49\u0e32\u0e07"}
{"id": "line_695", "truth_parsed_k": 288.07, "prediction_parsed_k_with_c_conv": 287.96, "prediction_parsed_k_strict": 287.96, "absolute_error_k_vs_c_conv_pred": 0.11000000000001364, "score_vs_c_conv_pred": 83.00095526617443, "absolute_error_k_vs_strict_pred": 0.11000000000001364, "score_vs_pred_strict": 83.00095526617443, "truth_raw_assistant_text": "The value is 288.07", "prediction_raw_text": "The value is 287.96\u0e2d\u0e35\u0e40\u0e21"}
{"id": "line_696", "truth_parsed_k": 287.47, "prediction_parsed_k_with_c_conv": 284.81, "prediction_parsed_k_strict": 284.81, "absolute_error_k_vs_c_conv_pred": 2.660000000000025, "score_vs_c_conv_pred": 16.4230730059985, "absolute_error_k_vs_strict_pred": 2.660000000000025, "score_vs_pred_strict": 16.4230730059985, "truth_raw_assistant_text": "The value is 287.47", "prediction_raw_text": "The value is 284.81\u0e23\u0e39\u0e49\u0e2a\u0e36\u0e01"}
{"id": "line_697", "truth_parsed_k": 286.9, "prediction_parsed_k_with_c_conv": 284.81, "prediction_parsed_k_strict": 284.81, "absolute_error_k_vs_c_conv_pred": 2.089999999999975, "score_vs_c_conv_pred": 22.589520452523903, "absolute_error_k_vs_strict_pred": 2.089999999999975, "score_vs_pred_strict": 22.589520452523903, "truth_raw_assistant_text": "The value is 286.90", "prediction_raw_text": "The value is 284.81\u0e04\u0e23\u0e31\u0e49\u0e07\u0e41\u0e23\u0e01"}
{"id": "line_698", "truth_parsed_k": 286.8, "prediction_parsed_k_with_c_conv": 287.64, "prediction_parsed_k_strict": 287.64, "absolute_error_k_vs_c_conv_pred": 0.839999999999975, "score_vs_c_conv_pred": 44.963654203418024, "absolute_error_k_vs_strict_pred": 0.839999999999975, "score_vs_pred_strict": 44.963654203418024, "truth_raw_assistant_text": "The value is 286.80", "prediction_raw_text": "The value is 287.64."}
{"id": "line_699", "truth_parsed_k": 287.54, "prediction_parsed_k_with_c_conv": 285.97, "prediction_parsed_k_strict": 285.97, "absolute_error_k_vs_c_conv_pred": 1.5699999999999932, "score_vs_c_conv_pred": 29.794703436952663, "absolute_error_k_vs_strict_pred": 1.5699999999999932, "score_vs_pred_strict": 29.794703436952663, "truth_raw_assistant_text": "The value is 287.54", "prediction_raw_text": "The value is 285.97\u0e40\u0e15\u0e47\u0e21"}
{"id": "line_700", "truth_parsed_k": 288.49, "prediction_parsed_k_with_c_conv": 285.78, "prediction_parsed_k_strict": 285.78, "absolute_error_k_vs_c_conv_pred": 2.7100000000000364, "score_vs_c_conv_pred": 15.943909993113614, "absolute_error_k_vs_strict_pred": 2.7100000000000364, "score_vs_pred_strict": 15.943909993113614, "truth_raw_assistant_text": "The value is 288.49", "prediction_raw_text": "The value is 285.78\u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01"}
{"id": "line_701", "truth_parsed_k": 289.4, "prediction_parsed_k_with_c_conv": 285.68, "prediction_parsed_k_strict": 285.68, "absolute_error_k_vs_c_conv_pred": 3.7199999999999704, "score_vs_c_conv_pred": 7.738019268383123, "absolute_error_k_vs_strict_pred": 3.7199999999999704, "score_vs_pred_strict": 7.738019268383123, "truth_raw_assistant_text": "The value is 289.40", "prediction_raw_text": "The value is 285.68\u0e1b\u0e38\u0e48\u0e21"}
{"id": "line_702", "truth_parsed_k": 290.1, "prediction_parsed_k_with_c_conv": 285.95, "prediction_parsed_k_strict": 285.95, "absolute_error_k_vs_c_conv_pred": 4.150000000000034, "score_vs_c_conv_pred": 4.883341951585729, "absolute_error_k_vs_strict_pred": 4.150000000000034, "score_vs_pred_strict": 4.883341951585729, "truth_raw_assistant_text": "The value is 290.10", "prediction_raw_text": "The value is 285.95"}
{"id": "line_703", "truth_parsed_k": 290.29, "prediction_parsed_k_with_c_conv": 285.85, "prediction_parsed_k_strict": 285.85, "absolute_error_k_vs_c_conv_pred": 4.439999999999998, "score_vs_c_conv_pred": 3.115922052920661, "absolute_error_k_vs_strict_pred": 4.439999999999998, "score_vs_pred_strict": 3.115922052920661, "truth_raw_assistant_text": "The value is 290.29", "prediction_raw_text": "The value is 285.85."}
{"id": "line_704", "truth_parsed_k": 290.02, "prediction_parsed_k_with_c_conv": 285.74, "prediction_parsed_k_strict": 285.74, "absolute_error_k_vs_c_conv_pred": 4.279999999999973, "score_vs_c_conv_pred": 4.076675042708677, "absolute_error_k_vs_strict_pred": 4.279999999999973, "score_vs_pred_strict": 4.076675042708677, "truth_raw_assistant_text": "The value is 290.02", "prediction_raw_text": "The value is 285.74\u0e1a\u0e34"}
{"id": "line_705", "truth_parsed_k": 289.49, "prediction_parsed_k_with_c_conv": 285.65, "prediction_parsed_k_strict": 285.65, "absolute_error_k_vs_c_conv_pred": 3.840000000000032, "score_vs_c_conv_pred": 6.910453475200773, "absolute_error_k_vs_strict_pred": 3.840000000000032, "score_vs_pred_strict": 6.910453475200773, "truth_raw_assistant_text": "The value is 289.49", "prediction_raw_text": "The value is 285.65."}
{"id": "line_706", "truth_parsed_k": 288.7, "prediction_parsed_k_with_c_conv": 286.72, "prediction_parsed_k_strict": 286.72, "absolute_error_k_vs_c_conv_pred": 1.9799999999999613, "score_vs_c_conv_pred": 23.961163050211965, "absolute_error_k_vs_strict_pred": 1.9799999999999613, "score_vs_pred_strict": 23.961163050211965, "truth_raw_assistant_text": "The value is 288.70", "prediction_raw_text": "The value is 286.72\u0e0b\u0e48"}
{"id": "line_707", "truth_parsed_k": 287.75, "prediction_parsed_k_with_c_conv": 285.81, "prediction_parsed_k_strict": 285.81, "absolute_error_k_vs_c_conv_pred": 1.9399999999999977, "score_vs_c_conv_pred": 24.477788024137894, "absolute_error_k_vs_strict_pred": 1.9399999999999977, "score_vs_pred_strict": 24.477788024137894, "truth_raw_assistant_text": "The value is 287.75", "prediction_raw_text": "The value is 285.81\ufa06"}
{"id": "line_708", "truth_parsed_k": 287.13, "prediction_parsed_k_with_c_conv": 285.68, "prediction_parsed_k_strict": 285.68, "absolute_error_k_vs_c_conv_pred": 1.4499999999999886, "score_vs_c_conv_pred": 31.771976726278794, "absolute_error_k_vs_strict_pred": 1.4499999999999886, "score_vs_pred_strict": 31.771976726278794, "truth_raw_assistant_text": "The value is 287.13", "prediction_raw_text": "The value is 285.68\u0e04\u0e32\u0e2a\u0e34"}
{"id": "line_709", "truth_parsed_k": 286.68, "prediction_parsed_k_with_c_conv": 284.57, "prediction_parsed_k_strict": 284.57, "absolute_error_k_vs_c_conv_pred": 2.1100000000000136, "score_vs_c_conv_pred": 22.347467130089584, "absolute_error_k_vs_strict_pred": 2.1100000000000136, "score_vs_pred_strict": 22.347467130089584, "truth_raw_assistant_text": "The value is 286.68", "prediction_raw_text": "The value is 284.57\u0e21\u0e2b\u0e32\u0e27\u0e34\u0e17\u0e22\u0e32\u0e25\u0e31\u0e22"}
{"id": "line_710", "truth_parsed_k": 286.71, "prediction_parsed_k_with_c_conv": 285.92, "prediction_parsed_k_strict": 285.92, "absolute_error_k_vs_c_conv_pred": 0.7899999999999636, "score_vs_c_conv_pred": 46.3963458304502, "absolute_error_k_vs_strict_pred": 0.7899999999999636, "score_vs_pred_strict": 46.3963458304502, "truth_raw_assistant_text": "The value is 286.71", "prediction_raw_text": "The value is 285.92."}
{"id": "line_711", "truth_parsed_k": 287.21, "prediction_parsed_k_with_c_conv": 285.8, "prediction_parsed_k_strict": 285.8, "absolute_error_k_vs_c_conv_pred": 1.4099999999999682, "score_vs_c_conv_pred": 32.46470304951612, "absolute_error_k_vs_strict_pred": 1.4099999999999682, "score_vs_pred_strict": 32.46470304951612, "truth_raw_assistant_text": "The value is 287.21", "prediction_raw_text": "The value is 285.80\u0e04\u0e38\u0e13\u0e20\u0e32\u0e1e"}
{"id": "line_712", "truth_parsed_k": 288.4, "prediction_parsed_k_with_c_conv": 286.4, "prediction_parsed_k_strict": 286.4, "absolute_error_k_vs_c_conv_pred": 2.0, "score_vs_c_conv_pred": 23.70652048310418, "absolute_error_k_vs_strict_pred": 2.0, "score_vs_pred_strict": 23.70652048310418, "truth_raw_assistant_text": "The value is 288.40", "prediction_raw_text": "The value is 286.40\u0e25\u0e35\u0e01"}
{"id": "line_713", "truth_parsed_k": 289.32, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 4.5, "score_vs_c_conv_pred": 2.764296806852773, "absolute_error_k_vs_strict_pred": 4.5, "score_vs_pred_strict": 2.764296806852773, "truth_raw_assistant_text": "The value is 289.32", "prediction_raw_text": "The value is 284.82 \u0e2a\u0e34\u0e07"}
{"id": "line_714", "truth_parsed_k": 290.02, "prediction_parsed_k_with_c_conv": 284.88, "prediction_parsed_k_strict": 284.88, "absolute_error_k_vs_c_conv_pred": 5.139999999999986, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.139999999999986, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.02", "prediction_raw_text": "The value is 284.88 \u0e15\u0e38\u0e25\u0e32"}
{"id": "line_715", "truth_parsed_k": 290.29, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 5.470000000000027, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.470000000000027, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.29", "prediction_raw_text": "The value is 284.82\u0e1b\u0e31\u0e08\u0e08\u0e38\u0e1a\u0e31\u0e19"}
{"id": "line_716", "truth_parsed_k": 289.98, "prediction_parsed_k_with_c_conv": 284.84, "prediction_parsed_k_strict": 284.84, "absolute_error_k_vs_c_conv_pred": 5.140000000000043, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.140000000000043, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 289.98", "prediction_raw_text": "The value is 284.84."}
{"id": "line_717", "truth_parsed_k": 289.48, "prediction_parsed_k_with_c_conv": 284.67, "prediction_parsed_k_strict": 284.67, "absolute_error_k_vs_c_conv_pred": 4.810000000000002, "score_vs_c_conv_pred": 1.017291477328286, "absolute_error_k_vs_strict_pred": 4.810000000000002, "score_vs_pred_strict": 1.017291477328286, "truth_raw_assistant_text": "The value is 289.48", "prediction_raw_text": "The value is 284.67\uf906"}
{"id": "line_718", "truth_parsed_k": 288.69, "prediction_parsed_k_with_c_conv": 284.88, "prediction_parsed_k_strict": 284.88, "absolute_error_k_vs_c_conv_pred": 3.8100000000000023, "score_vs_c_conv_pred": 7.1149728188685035, "absolute_error_k_vs_strict_pred": 3.8100000000000023, "score_vs_pred_strict": 7.1149728188685035, "truth_raw_assistant_text": "The value is 288.69", "prediction_raw_text": "The value is 284.88\u0e14\u0e35\u0e46"}
{"id": "line_719", "truth_parsed_k": 287.71, "prediction_parsed_k_with_c_conv": 287.37, "prediction_parsed_k_strict": 287.37, "absolute_error_k_vs_c_conv_pred": 0.339999999999975, "score_vs_c_conv_pred": 64.62371957006627, "absolute_error_k_vs_strict_pred": 0.339999999999975, "score_vs_pred_strict": 64.62371957006627, "truth_raw_assistant_text": "The value is 287.71", "prediction_raw_text": "The value is 287.37"}
{"id": "line_720", "truth_parsed_k": 287.14, "prediction_parsed_k_with_c_conv": 286.78, "prediction_parsed_k_strict": 286.78, "absolute_error_k_vs_c_conv_pred": 0.36000000000001364, "score_vs_c_conv_pred": 63.489730930724164, "absolute_error_k_vs_strict_pred": 0.36000000000001364, "score_vs_pred_strict": 63.489730930724164, "truth_raw_assistant_text": "The value is 287.14", "prediction_raw_text": "The value is 286.78\u0e02\u0e31\u0e49\u0e19\u0e15\u0e2d\u0e19"}
{"id": "line_721", "truth_parsed_k": 286.73, "prediction_parsed_k_with_c_conv": 284.91, "prediction_parsed_k_strict": 284.91, "absolute_error_k_vs_c_conv_pred": 1.8199999999999932, "score_vs_c_conv_pred": 26.08993525777755, "absolute_error_k_vs_strict_pred": 1.8199999999999932, "score_vs_pred_strict": 26.08993525777755, "truth_raw_assistant_text": "The value is 286.73", "prediction_raw_text": "The value is 284.91\u0e1b\u0e0f\u0e34\u0e1a\u0e31\u0e15\u0e34"}
{"id": "line_722", "truth_parsed_k": 287.05, "prediction_parsed_k_with_c_conv": 284.81, "prediction_parsed_k_strict": 284.81, "absolute_error_k_vs_c_conv_pred": 2.240000000000009, "score_vs_c_conv_pred": 20.825030164741108, "absolute_error_k_vs_strict_pred": 2.240000000000009, "score_vs_pred_strict": 20.825030164741108, "truth_raw_assistant_text": "The value is 287.05", "prediction_raw_text": "The value is 284.81."}
{"id": "line_723", "truth_parsed_k": 287.5, "prediction_parsed_k_with_c_conv": 285.59, "prediction_parsed_k_strict": 285.59, "absolute_error_k_vs_c_conv_pred": 1.910000000000025, "score_vs_c_conv_pred": 24.87186702019766, "absolute_error_k_vs_strict_pred": 1.910000000000025, "score_vs_pred_strict": 24.87186702019766, "truth_raw_assistant_text": "The value is 287.50", "prediction_raw_text": "The value is 285.59."}
{"id": "line_724", "truth_parsed_k": 288.56, "prediction_parsed_k_with_c_conv": 285.84, "prediction_parsed_k_strict": 285.84, "absolute_error_k_vs_c_conv_pred": 2.7200000000000273, "score_vs_c_conv_pred": 15.84909211061859, "absolute_error_k_vs_strict_pred": 2.7200000000000273, "score_vs_pred_strict": 15.84909211061859, "truth_raw_assistant_text": "The value is 288.56", "prediction_raw_text": "The value is 285.84\u05d5\u05bc"}
{"id": "line_725", "truth_parsed_k": 289.54, "prediction_parsed_k_with_c_conv": 284.86, "prediction_parsed_k_strict": 284.86, "absolute_error_k_vs_c_conv_pred": 4.680000000000007, "score_vs_c_conv_pred": 1.7361581098770285, "absolute_error_k_vs_strict_pred": 4.680000000000007, "score_vs_pred_strict": 1.7361581098770285, "truth_raw_assistant_text": "The value is 289.54", "prediction_raw_text": "The value is 284.86"}
{"id": "line_726", "truth_parsed_k": 290.2, "prediction_parsed_k_with_c_conv": 285.77, "prediction_parsed_k_strict": 285.77, "absolute_error_k_vs_c_conv_pred": 4.430000000000007, "score_vs_c_conv_pred": 3.1749752581800905, "absolute_error_k_vs_strict_pred": 4.430000000000007, "score_vs_pred_strict": 3.1749752581800905, "truth_raw_assistant_text": "The value is 290.20", "prediction_raw_text": "The value is 285.77\u0e1a\u0e2d\u0e23\u0e4c\u0e14"}
{"id": "line_727", "truth_parsed_k": 290.53, "prediction_parsed_k_with_c_conv": 285.96, "prediction_parsed_k_strict": 285.96, "absolute_error_k_vs_c_conv_pred": 4.569999999999993, "score_vs_c_conv_pred": 2.359787578917749, "absolute_error_k_vs_strict_pred": 4.569999999999993, "score_vs_pred_strict": 2.359787578917749, "truth_raw_assistant_text": "The value is 290.53", "prediction_raw_text": "The value is 285.96."}
{"id": "line_728", "truth_parsed_k": 290.33, "prediction_parsed_k_with_c_conv": 285.76, "prediction_parsed_k_strict": 285.76, "absolute_error_k_vs_c_conv_pred": 4.569999999999993, "score_vs_c_conv_pred": 2.359787578917749, "absolute_error_k_vs_strict_pred": 4.569999999999993, "score_vs_pred_strict": 2.359787578917749, "truth_raw_assistant_text": "The value is 290.33", "prediction_raw_text": "The value is 285.76\u0e41\u0e1a\u0e48\u0e07"}
{"id": "line_729", "truth_parsed_k": 289.64, "prediction_parsed_k_with_c_conv": 284.96, "prediction_parsed_k_strict": 284.96, "absolute_error_k_vs_c_conv_pred": 4.680000000000007, "score_vs_c_conv_pred": 1.7361581098770285, "absolute_error_k_vs_strict_pred": 4.680000000000007, "score_vs_pred_strict": 1.7361581098770285, "truth_raw_assistant_text": "The value is 289.64", "prediction_raw_text": "The value is 284.96"}
{"id": "line_730", "truth_parsed_k": 288.92, "prediction_parsed_k_with_c_conv": 284.76, "prediction_parsed_k_strict": 284.76, "absolute_error_k_vs_c_conv_pred": 4.160000000000025, "score_vs_c_conv_pred": 4.820425449117227, "absolute_error_k_vs_strict_pred": 4.160000000000025, "score_vs_pred_strict": 4.820425449117227, "truth_raw_assistant_text": "The value is 288.92", "prediction_raw_text": "The value is 284.76\uf98f"}
{"id": "line_731", "truth_parsed_k": 288.11, "prediction_parsed_k_with_c_conv": 284.95, "prediction_parsed_k_strict": 284.95, "absolute_error_k_vs_c_conv_pred": 3.160000000000025, "score_vs_c_conv_pred": 11.97671312415396, "absolute_error_k_vs_strict_pred": 3.160000000000025, "score_vs_pred_strict": 11.97671312415396, "truth_raw_assistant_text": "The value is 288.11", "prediction_raw_text": "The value is 284.95"}
{"id": "line_732", "truth_parsed_k": 287.41, "prediction_parsed_k_with_c_conv": 284.89, "prediction_parsed_k_strict": 284.89, "absolute_error_k_vs_c_conv_pred": 2.5200000000000387, "score_vs_c_conv_pred": 17.811946229475527, "absolute_error_k_vs_strict_pred": 2.5200000000000387, "score_vs_pred_strict": 17.811946229475527, "truth_raw_assistant_text": "The value is 287.41", "prediction_raw_text": "The value is 284.89\u0e04\u0e49\u0e19\u0e2b\u0e32"}
{"id": "line_733", "truth_parsed_k": 286.98, "prediction_parsed_k_with_c_conv": 285.85, "prediction_parsed_k_strict": 285.85, "absolute_error_k_vs_c_conv_pred": 1.1299999999999955, "score_vs_c_conv_pred": 37.8879129627178, "absolute_error_k_vs_strict_pred": 1.1299999999999955, "score_vs_pred_strict": 37.8879129627178, "truth_raw_assistant_text": "The value is 286.98", "prediction_raw_text": "The value is 285.85."}
{"id": "line_734", "truth_parsed_k": 286.95, "prediction_parsed_k_with_c_conv": 285.07, "prediction_parsed_k_strict": 285.07, "absolute_error_k_vs_c_conv_pred": 1.8799999999999955, "score_vs_c_conv_pred": 25.271798888201523, "absolute_error_k_vs_strict_pred": 1.8799999999999955, "score_vs_pred_strict": 25.271798888201523, "truth_raw_assistant_text": "The value is 286.95", "prediction_raw_text": "The value is 285.07"}
{"id": "line_735", "truth_parsed_k": 287.55, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 2.730000000000018, "score_vs_c_conv_pred": 15.754606923131975, "absolute_error_k_vs_strict_pred": 2.730000000000018, "score_vs_pred_strict": 15.754606923131975, "truth_raw_assistant_text": "The value is 287.55", "prediction_raw_text": "The value is 284.82\u0e27\u0e34\u0e14\u0e35\u0e42\u0e2d"}
{"id": "line_736", "truth_parsed_k": 288.7, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 3.759999999999991, "score_vs_c_conv_pred": 7.4593285448395825, "absolute_error_k_vs_strict_pred": 3.759999999999991, "score_vs_pred_strict": 7.4593285448395825, "truth_raw_assistant_text": "The value is 288.70", "prediction_raw_text": "The value is 284.94\u0e23\u0e30\u0e14\u0e31\u0e1a"}
{"id": "line_737", "truth_parsed_k": 289.68, "prediction_parsed_k_with_c_conv": 286.94, "prediction_parsed_k_strict": 286.94, "absolute_error_k_vs_c_conv_pred": 2.740000000000009, "score_vs_c_conv_pred": 15.660452104108114, "absolute_error_k_vs_strict_pred": 2.740000000000009, "score_vs_pred_strict": 15.660452104108114, "truth_raw_assistant_text": "The value is 289.68", "prediction_raw_text": "The value is 286.94\u0e22\u0e2d\u0e21\u0e23\u0e31\u0e1a"}
{"id": "line_738", "truth_parsed_k": 290.37, "prediction_parsed_k_with_c_conv": 285.8, "prediction_parsed_k_strict": 285.8, "absolute_error_k_vs_c_conv_pred": 4.569999999999993, "score_vs_c_conv_pred": 2.359787578917749, "absolute_error_k_vs_strict_pred": 4.569999999999993, "score_vs_pred_strict": 2.359787578917749, "truth_raw_assistant_text": "The value is 290.37", "prediction_raw_text": "The value is 285.80\uf9ca"}
{"id": "line_739", "truth_parsed_k": 290.62, "prediction_parsed_k_with_c_conv": 285.99, "prediction_parsed_k_strict": 285.99, "absolute_error_k_vs_c_conv_pred": 4.6299999999999955, "score_vs_c_conv_pred": 2.0178369419290965, "absolute_error_k_vs_strict_pred": 4.6299999999999955, "score_vs_pred_strict": 2.0178369419290965, "truth_raw_assistant_text": "The value is 290.62", "prediction_raw_text": "The value is 285.99\u0e23\u0e30\u0e2b\u0e27\u0e48\u0e32\u0e07"}
{"id": "line_740", "truth_parsed_k": 290.22, "prediction_parsed_k_with_c_conv": 285.89, "prediction_parsed_k_strict": 285.89, "absolute_error_k_vs_c_conv_pred": 4.330000000000041, "score_vs_c_conv_pred": 3.7727412201812527, "absolute_error_k_vs_strict_pred": 4.330000000000041, "score_vs_pred_strict": 3.7727412201812527, "truth_raw_assistant_text": "The value is 290.22", "prediction_raw_text": "The value is 285.89."}
{"id": "line_741", "truth_parsed_k": 289.52, "prediction_parsed_k_with_c_conv": 284.83, "prediction_parsed_k_strict": 284.83, "absolute_error_k_vs_c_conv_pred": 4.689999999999998, "score_vs_c_conv_pred": 1.6801742003018982, "absolute_error_k_vs_strict_pred": 4.689999999999998, "score_vs_pred_strict": 1.6801742003018982, "truth_raw_assistant_text": "The value is 289.52", "prediction_raw_text": "The value is 284.83\u0e2b\u0e19\u0e49\u0e32\u0e17\u0e35\u0e48"}
{"id": "line_742", "truth_parsed_k": 288.83, "prediction_parsed_k_with_c_conv": 284.85, "prediction_parsed_k_strict": 284.85, "absolute_error_k_vs_c_conv_pred": 3.9799999999999613, "score_vs_c_conv_pred": 5.976046193094131, "absolute_error_k_vs_strict_pred": 3.9799999999999613, "score_vs_pred_strict": 5.976046193094131, "truth_raw_assistant_text": "The value is 288.83", "prediction_raw_text": "The value is 284.85\u062c\u064e"}
{"id": "line_743", "truth_parsed_k": 288.09, "prediction_parsed_k_with_c_conv": 285.47, "prediction_parsed_k_strict": 285.47, "absolute_error_k_vs_c_conv_pred": 2.6199999999999477, "score_vs_c_conv_pred": 16.812638582158712, "absolute_error_k_vs_strict_pred": 2.6199999999999477, "score_vs_pred_strict": 16.812638582158712, "truth_raw_assistant_text": "The value is 288.09", "prediction_raw_text": "The value is 285.47."}
{"id": "line_744", "truth_parsed_k": 287.43, "prediction_parsed_k_with_c_conv": 284.5, "prediction_parsed_k_strict": 284.5, "absolute_error_k_vs_c_conv_pred": 2.930000000000007, "score_vs_c_conv_pred": 13.931355662636857, "absolute_error_k_vs_strict_pred": 2.930000000000007, "score_vs_pred_strict": 13.931355662636857, "truth_raw_assistant_text": "The value is 287.43", "prediction_raw_text": "The value is 284.50\u0e17\u0e35\u0e48\u0e1e\u0e31\u0e01"}
{"id": "line_745", "truth_parsed_k": 286.97, "prediction_parsed_k_with_c_conv": 284.98, "prediction_parsed_k_strict": 284.98, "absolute_error_k_vs_c_conv_pred": 1.990000000000009, "score_vs_c_conv_pred": 23.83354076959402, "absolute_error_k_vs_strict_pred": 1.990000000000009, "score_vs_pred_strict": 23.83354076959402, "truth_raw_assistant_text": "The value is 286.97", "prediction_raw_text": "The value is 284.98."}
{"id": "line_746", "truth_parsed_k": 287.14, "prediction_parsed_k_with_c_conv": 286.78, "prediction_parsed_k_strict": 286.78, "absolute_error_k_vs_c_conv_pred": 0.36000000000001364, "score_vs_c_conv_pred": 63.489730930724164, "absolute_error_k_vs_strict_pred": 0.36000000000001364, "score_vs_pred_strict": 63.489730930724164, "truth_raw_assistant_text": "The value is 287.14", "prediction_raw_text": "The value is 286.78\u0e23\u0e39\u0e49\u0e08"}
{"id": "line_747", "truth_parsed_k": 287.49, "prediction_parsed_k_with_c_conv": 286.62, "prediction_parsed_k_strict": 286.62, "absolute_error_k_vs_c_conv_pred": 0.8700000000000045, "score_vs_c_conv_pred": 44.13925561171903, "absolute_error_k_vs_strict_pred": 0.8700000000000045, "score_vs_pred_strict": 44.13925561171903, "truth_raw_assistant_text": "The value is 287.49", "prediction_raw_text": "The value is 286.62\u0e1e\u0e19\u0e31\u0e01\u0e07\u0e32\u0e19"}
{"id": "line_748", "truth_parsed_k": 288.7, "prediction_parsed_k_with_c_conv": 284.6, "prediction_parsed_k_strict": 284.6, "absolute_error_k_vs_c_conv_pred": 4.099999999999966, "score_vs_c_conv_pred": 5.200148556339279, "absolute_error_k_vs_strict_pred": 4.099999999999966, "score_vs_pred_strict": 5.200148556339279, "truth_raw_assistant_text": "The value is 288.70", "prediction_raw_text": "The value is 284.60\ufa56"}
{"id": "line_749", "truth_parsed_k": 289.61, "prediction_parsed_k_with_c_conv": 285.62, "prediction_parsed_k_strict": 285.62, "absolute_error_k_vs_c_conv_pred": 3.990000000000009, "score_vs_c_conv_pred": 5.910527302361723, "absolute_error_k_vs_strict_pred": 3.990000000000009, "score_vs_pred_strict": 5.910527302361723, "truth_raw_assistant_text": "The value is 289.61", "prediction_raw_text": "The value is 285.62\uf98c"}
{"id": "line_750", "truth_parsed_k": 290.21, "prediction_parsed_k_with_c_conv": 285.74, "prediction_parsed_k_strict": 285.74, "absolute_error_k_vs_c_conv_pred": 4.46999999999997, "score_vs_c_conv_pred": 2.939535499945378, "absolute_error_k_vs_strict_pred": 4.46999999999997, "score_vs_pred_strict": 2.939535499945378, "truth_raw_assistant_text": "The value is 290.21", "prediction_raw_text": "The value is 285.74."}
{"id": "line_751", "truth_parsed_k": 290.52, "prediction_parsed_k_with_c_conv": 285.81, "prediction_parsed_k_strict": 285.81, "absolute_error_k_vs_c_conv_pred": 4.7099999999999795, "score_vs_c_conv_pred": 1.5685543480219444, "absolute_error_k_vs_strict_pred": 4.7099999999999795, "score_vs_pred_strict": 1.5685543480219444, "truth_raw_assistant_text": "The value is 290.52", "prediction_raw_text": "The value is 285.81 \u0e01\u0e38\u0e21\u0e20\u0e32\u0e1e\u0e31\u0e19\u0e18"}
{"id": "line_752", "truth_parsed_k": 290.36, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 5.389999999999986, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.389999999999986, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.36", "prediction_raw_text": "The value is 284.97\u0e01\u0e25\u0e34\u0e48\u0e19"}
{"id": "line_753", "truth_parsed_k": 289.62, "prediction_parsed_k_with_c_conv": 285.04, "prediction_parsed_k_strict": 285.04, "absolute_error_k_vs_c_conv_pred": 4.579999999999984, "score_vs_c_conv_pred": 2.302493406651873, "absolute_error_k_vs_strict_pred": 4.579999999999984, "score_vs_pred_strict": 2.302493406651873, "truth_raw_assistant_text": "The value is 289.62", "prediction_raw_text": "The value is 285.04\u0e44\u0e1b\u0e16\u0e36\u0e07"}
{"id": "line_754", "truth_parsed_k": 288.83, "prediction_parsed_k_with_c_conv": 285.97, "prediction_parsed_k_strict": 285.97, "absolute_error_k_vs_c_conv_pred": 2.859999999999957, "score_vs_c_conv_pred": 14.555548363428606, "absolute_error_k_vs_strict_pred": 2.859999999999957, "score_vs_pred_strict": 14.555548363428606, "truth_raw_assistant_text": "The value is 288.83", "prediction_raw_text": "The value is 285.97\u0e22\u0e34\u0e49\u0e21"}
{"id": "line_755", "truth_parsed_k": 287.79, "prediction_parsed_k_with_c_conv": 285.77, "prediction_parsed_k_strict": 285.77, "absolute_error_k_vs_c_conv_pred": 2.0200000000000387, "score_vs_c_conv_pred": 23.454263361514727, "absolute_error_k_vs_strict_pred": 2.0200000000000387, "score_vs_pred_strict": 23.454263361514727, "truth_raw_assistant_text": "The value is 287.79", "prediction_raw_text": "The value is 285.77\u0e2a\u0e31\u0e1b\u0e14\u0e32"}
{"id": "line_756", "truth_parsed_k": 287.26, "prediction_parsed_k_with_c_conv": 284.75, "prediction_parsed_k_strict": 284.75, "absolute_error_k_vs_c_conv_pred": 2.509999999999991, "score_vs_c_conv_pred": 17.91394730146002, "absolute_error_k_vs_strict_pred": 2.509999999999991, "score_vs_pred_strict": 17.91394730146002, "truth_raw_assistant_text": "The value is 287.26", "prediction_raw_text": "The value is 284.75."}
{"id": "line_757", "truth_parsed_k": 286.91, "prediction_parsed_k_with_c_conv": 284.87, "prediction_parsed_k_strict": 284.87, "absolute_error_k_vs_c_conv_pred": 2.0400000000000205, "score_vs_c_conv_pred": 23.20434740707088, "absolute_error_k_vs_strict_pred": 2.0400000000000205, "score_vs_pred_strict": 23.20434740707088, "truth_raw_assistant_text": "The value is 286.91", "prediction_raw_text": "The value is 284.87\u0e27\u0e34\u0e18\u0e35\u0e01\u0e32\u0e23"}
{"id": "line_758", "truth_parsed_k": 286.9, "prediction_parsed_k_with_c_conv": 284.58, "prediction_parsed_k_strict": 284.58, "absolute_error_k_vs_c_conv_pred": 2.319999999999993, "score_vs_c_conv_pred": 19.92920532586956, "absolute_error_k_vs_strict_pred": 2.319999999999993, "score_vs_pred_strict": 19.92920532586956, "truth_raw_assistant_text": "The value is 286.90", "prediction_raw_text": "The value is 284.58"}
{"id": "line_759", "truth_parsed_k": 287.36, "prediction_parsed_k_with_c_conv": 284.85, "prediction_parsed_k_strict": 284.85, "absolute_error_k_vs_c_conv_pred": 2.509999999999991, "score_vs_c_conv_pred": 17.91394730146002, "absolute_error_k_vs_strict_pred": 2.509999999999991, "score_vs_pred_strict": 17.91394730146002, "truth_raw_assistant_text": "The value is 287.36", "prediction_raw_text": "The value is 284.85\u0e40\u0e2b\u0e15\u0e38\u0e01\u0e32\u0e23\u0e13\u0e4c"}
{"id": "line_760", "truth_parsed_k": 288.52, "prediction_parsed_k_with_c_conv": 285.54, "prediction_parsed_k_strict": 285.54, "absolute_error_k_vs_c_conv_pred": 2.9799999999999613, "score_vs_c_conv_pred": 13.494199133282736, "absolute_error_k_vs_strict_pred": 2.9799999999999613, "score_vs_pred_strict": 13.494199133282736, "truth_raw_assistant_text": "The value is 288.52", "prediction_raw_text": "The value is 285.54."}
{"id": "line_761", "truth_parsed_k": 289.46, "prediction_parsed_k_with_c_conv": 285.97, "prediction_parsed_k_strict": 285.97, "absolute_error_k_vs_c_conv_pred": 3.4899999999999523, "score_vs_c_conv_pred": 9.39900083111943, "absolute_error_k_vs_strict_pred": 3.4899999999999523, "score_vs_pred_strict": 9.39900083111943, "truth_raw_assistant_text": "The value is 289.46", "prediction_raw_text": "The value is 285.97."}
{"id": "line_762", "truth_parsed_k": 290.14, "prediction_parsed_k_with_c_conv": 285.6, "prediction_parsed_k_strict": 285.6, "absolute_error_k_vs_c_conv_pred": 4.539999999999964, "score_vs_c_conv_pred": 2.53240516235983, "absolute_error_k_vs_strict_pred": 4.539999999999964, "score_vs_pred_strict": 2.53240516235983, "truth_raw_assistant_text": "The value is 290.14", "prediction_raw_text": "The value is 285.60\uf9e4"}
{"id": "line_763", "truth_parsed_k": 290.37, "prediction_parsed_k_with_c_conv": 284.56, "prediction_parsed_k_strict": 284.56, "absolute_error_k_vs_c_conv_pred": 5.810000000000002, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.810000000000002, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.37", "prediction_raw_text": "The value is 284.56."}
{"id": "line_764", "truth_parsed_k": 290.21, "prediction_parsed_k_with_c_conv": 284.86, "prediction_parsed_k_strict": 284.86, "absolute_error_k_vs_c_conv_pred": 5.349999999999966, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.349999999999966, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.21", "prediction_raw_text": "The value is 284.86\u0e41\u0e2b\u0e25\u0e48\u0e07"}
{"id": "line_765", "truth_parsed_k": 289.56, "prediction_parsed_k_with_c_conv": 285.63, "prediction_parsed_k_strict": 285.63, "absolute_error_k_vs_c_conv_pred": 3.930000000000007, "score_vs_c_conv_pred": 6.306053396855771, "absolute_error_k_vs_strict_pred": 3.930000000000007, "score_vs_pred_strict": 6.306053396855771, "truth_raw_assistant_text": "The value is 289.56", "prediction_raw_text": "The value is 285.63\u7c21\ufffd"}
{"id": "line_766", "truth_parsed_k": 288.75, "prediction_parsed_k_with_c_conv": 285.56, "prediction_parsed_k_strict": 285.56, "absolute_error_k_vs_c_conv_pred": 3.1899999999999977, "score_vs_c_conv_pred": 11.731909158108634, "absolute_error_k_vs_strict_pred": 3.1899999999999977, "score_vs_pred_strict": 11.731909158108634, "truth_raw_assistant_text": "The value is 288.75", "prediction_raw_text": "The value is 285.56."}
{"id": "line_767", "truth_parsed_k": 288.01, "prediction_parsed_k_with_c_conv": 285.91, "prediction_parsed_k_strict": 285.91, "absolute_error_k_vs_c_conv_pred": 2.099999999999966, "score_vs_c_conv_pred": 22.468221820366264, "absolute_error_k_vs_strict_pred": 2.099999999999966, "score_vs_pred_strict": 22.468221820366264, "truth_raw_assistant_text": "The value is 288.01", "prediction_raw_text": "The value is 285.91."}
{"id": "line_768", "truth_parsed_k": 287.39, "prediction_parsed_k_with_c_conv": 284.68, "prediction_parsed_k_strict": 284.68, "absolute_error_k_vs_c_conv_pred": 2.7099999999999795, "score_vs_c_conv_pred": 15.943909993114158, "absolute_error_k_vs_strict_pred": 2.7099999999999795, "score_vs_pred_strict": 15.943909993114158, "truth_raw_assistant_text": "The value is 287.39", "prediction_raw_text": "The value is 284.68\ufa06"}
{"id": "line_769", "truth_parsed_k": 286.93, "prediction_parsed_k_with_c_conv": 284.95, "prediction_parsed_k_strict": 284.95, "absolute_error_k_vs_c_conv_pred": 1.9800000000000182, "score_vs_c_conv_pred": 23.961163050211244, "absolute_error_k_vs_strict_pred": 1.9800000000000182, "score_vs_pred_strict": 23.961163050211244, "truth_raw_assistant_text": "The value is 286.93", "prediction_raw_text": "The value is 284.95\u0e22\u0e48\u0e32\u0e19"}
{"id": "line_770", "truth_parsed_k": 287.0, "prediction_parsed_k_with_c_conv": 285.74, "prediction_parsed_k_strict": 285.74, "absolute_error_k_vs_c_conv_pred": 1.259999999999991, "score_vs_c_conv_pred": 35.233741580417814, "absolute_error_k_vs_strict_pred": 1.259999999999991, "score_vs_pred_strict": 35.233741580417814, "truth_raw_assistant_text": "The value is 287.00", "prediction_raw_text": "The value is 285.74\u0e40\u0e14\u0e35"}
{"id": "line_771", "truth_parsed_k": 287.53, "prediction_parsed_k_with_c_conv": 285.76, "prediction_parsed_k_strict": 285.76, "absolute_error_k_vs_c_conv_pred": 1.7699999999999818, "score_vs_c_conv_pred": 26.791231424062502, "absolute_error_k_vs_strict_pred": 1.7699999999999818, "score_vs_pred_strict": 26.791231424062502, "truth_raw_assistant_text": "The value is 287.53", "prediction_raw_text": "The value is 285.76."}
{"id": "line_772", "truth_parsed_k": 288.82, "prediction_parsed_k_with_c_conv": 284.7, "prediction_parsed_k_strict": 284.7, "absolute_error_k_vs_c_conv_pred": 4.1200000000000045, "score_vs_c_conv_pred": 5.072978304372855, "absolute_error_k_vs_strict_pred": 4.1200000000000045, "score_vs_pred_strict": 5.072978304372855, "truth_raw_assistant_text": "The value is 288.82", "prediction_raw_text": "The value is 284.70\u0e04\u0e39\u0e48"}
{"id": "line_773", "truth_parsed_k": 289.61, "prediction_parsed_k_with_c_conv": 284.93, "prediction_parsed_k_strict": 284.93, "absolute_error_k_vs_c_conv_pred": 4.680000000000007, "score_vs_c_conv_pred": 1.7361581098770285, "absolute_error_k_vs_strict_pred": 4.680000000000007, "score_vs_pred_strict": 1.7361581098770285, "truth_raw_assistant_text": "The value is 289.61", "prediction_raw_text": "The value is 284.93\u0e16\u0e37\u0e2d\u0e27\u0e48\u0e32"}
{"id": "line_774", "truth_parsed_k": 290.33, "prediction_parsed_k_with_c_conv": 284.92, "prediction_parsed_k_strict": 284.92, "absolute_error_k_vs_c_conv_pred": 5.409999999999968, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.409999999999968, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.33", "prediction_raw_text": "The value is 284.92"}
{"id": "line_775", "truth_parsed_k": 290.59, "prediction_parsed_k_with_c_conv": 287.88, "prediction_parsed_k_strict": 287.88, "absolute_error_k_vs_c_conv_pred": 2.7099999999999795, "score_vs_c_conv_pred": 15.943909993114158, "absolute_error_k_vs_strict_pred": 2.7099999999999795, "score_vs_pred_strict": 15.943909993114158, "truth_raw_assistant_text": "The value is 290.59", "prediction_raw_text": "The value is 287.88\u0e15\u0e33\u0e41\u0e2b\u0e19\u0e48\u0e07"}
{"id": "line_776", "truth_parsed_k": 290.44, "prediction_parsed_k_with_c_conv": 285.76, "prediction_parsed_k_strict": 285.76, "absolute_error_k_vs_c_conv_pred": 4.680000000000007, "score_vs_c_conv_pred": 1.7361581098770285, "absolute_error_k_vs_strict_pred": 4.680000000000007, "score_vs_pred_strict": 1.7361581098770285, "truth_raw_assistant_text": "The value is 290.44", "prediction_raw_text": "The value is 285.76\u0e27\u0e34\u0e40\u0e04\u0e23\u0e32\u0e30"}
{"id": "line_777", "truth_parsed_k": 289.78, "prediction_parsed_k_with_c_conv": 284.88, "prediction_parsed_k_strict": 284.88, "absolute_error_k_vs_c_conv_pred": 4.899999999999977, "score_vs_c_conv_pred": 0.5306230961831027, "absolute_error_k_vs_strict_pred": 4.899999999999977, "score_vs_pred_strict": 0.5306230961831027, "truth_raw_assistant_text": "The value is 289.78", "prediction_raw_text": "The value is 284.88\u0e41\u0e15\u0e01\u0e15\u0e48\u0e32\u0e07\u0e01\u0e31\u0e19"}
{"id": "line_778", "truth_parsed_k": 289.02, "prediction_parsed_k_with_c_conv": 285.81, "prediction_parsed_k_strict": 285.81, "absolute_error_k_vs_c_conv_pred": 3.2099999999999795, "score_vs_c_conv_pred": 11.569934228803058, "absolute_error_k_vs_strict_pred": 3.2099999999999795, "score_vs_pred_strict": 11.569934228803058, "truth_raw_assistant_text": "The value is 289.02", "prediction_raw_text": "The value is 285.81."}
{"id": "line_779", "truth_parsed_k": 288.26, "prediction_parsed_k_with_c_conv": 286.47, "prediction_parsed_k_strict": 286.47, "absolute_error_k_vs_c_conv_pred": 1.7899999999999636, "score_vs_c_conv_pred": 26.508517532703234, "absolute_error_k_vs_strict_pred": 1.7899999999999636, "score_vs_pred_strict": 26.508517532703234, "truth_raw_assistant_text": "The value is 288.26", "prediction_raw_text": "The value is 286.47\u0e2a\u0e31\u0e1b"}
{"id": "line_780", "truth_parsed_k": 287.46, "prediction_parsed_k_with_c_conv": 285.48, "prediction_parsed_k_strict": 285.48, "absolute_error_k_vs_c_conv_pred": 1.9799999999999613, "score_vs_c_conv_pred": 23.961163050211965, "absolute_error_k_vs_strict_pred": 1.9799999999999613, "score_vs_pred_strict": 23.961163050211965, "truth_raw_assistant_text": "The value is 287.46", "prediction_raw_text": "The value is 285.48"}
{"id": "line_781", "truth_parsed_k": 286.81, "prediction_parsed_k_with_c_conv": 288.35, "prediction_parsed_k_strict": 288.35, "absolute_error_k_vs_c_conv_pred": 1.5400000000000205, "score_vs_c_conv_pred": 30.27557783710503, "absolute_error_k_vs_strict_pred": 1.5400000000000205, "score_vs_pred_strict": 30.27557783710503, "truth_raw_assistant_text": "The value is 286.81", "prediction_raw_text": "The value is 288.35."}
{"id": "line_782", "truth_parsed_k": 287.03, "prediction_parsed_k_with_c_conv": 285.69, "prediction_parsed_k_strict": 285.69, "absolute_error_k_vs_c_conv_pred": 1.339999999999975, "score_vs_c_conv_pred": 33.72158288735778, "absolute_error_k_vs_strict_pred": 1.339999999999975, "score_vs_pred_strict": 33.72158288735778, "truth_raw_assistant_text": "The value is 287.03", "prediction_raw_text": "The value is 285.69."}
{"id": "line_783", "truth_parsed_k": 287.77, "prediction_parsed_k_with_c_conv": 285.82, "prediction_parsed_k_strict": 285.82, "absolute_error_k_vs_c_conv_pred": 1.9499999999999886, "score_vs_c_conv_pred": 24.34769960299279, "absolute_error_k_vs_strict_pred": 1.9499999999999886, "score_vs_pred_strict": 24.34769960299279, "truth_raw_assistant_text": "The value is 287.77", "prediction_raw_text": "The value is 285.82."}
{"id": "line_784", "truth_parsed_k": 288.74, "prediction_parsed_k_with_c_conv": 285.92, "prediction_parsed_k_strict": 285.92, "absolute_error_k_vs_c_conv_pred": 2.819999999999993, "score_vs_c_conv_pred": 14.918835503106397, "absolute_error_k_vs_strict_pred": 2.819999999999993, "score_vs_pred_strict": 14.918835503106397, "truth_raw_assistant_text": "The value is 288.74", "prediction_raw_text": "The value is 285.92"}
{"id": "line_785", "truth_parsed_k": 289.73, "prediction_parsed_k_with_c_conv": 284.92, "prediction_parsed_k_strict": 284.92, "absolute_error_k_vs_c_conv_pred": 4.810000000000002, "score_vs_c_conv_pred": 1.017291477328286, "absolute_error_k_vs_strict_pred": 4.810000000000002, "score_vs_pred_strict": 1.017291477328286, "truth_raw_assistant_text": "The value is 289.73", "prediction_raw_text": "The value is 284.92\u0e01\u0e23\u0e35"}
{"id": "line_786", "truth_parsed_k": 290.32, "prediction_parsed_k_with_c_conv": 285.4, "prediction_parsed_k_strict": 285.4, "absolute_error_k_vs_c_conv_pred": 4.920000000000016, "score_vs_c_conv_pred": 0.42365870208784795, "absolute_error_k_vs_strict_pred": 4.920000000000016, "score_vs_pred_strict": 0.42365870208784795, "truth_raw_assistant_text": "The value is 290.32", "prediction_raw_text": "The value is 285.40\u0e21\u0e31\u0e49\u0e22"}
{"id": "line_787", "truth_parsed_k": 290.64, "prediction_parsed_k_with_c_conv": 285.94, "prediction_parsed_k_strict": 285.94, "absolute_error_k_vs_c_conv_pred": 4.699999999999989, "score_vs_c_conv_pred": 1.6243064400001561, "absolute_error_k_vs_strict_pred": 4.699999999999989, "score_vs_pred_strict": 1.6243064400001561, "truth_raw_assistant_text": "The value is 290.64", "prediction_raw_text": "The value is 285.94\u0e44\u0e21\u0e48\u0e27\u0e48\u0e32\u0e08\u0e30"}
{"id": "line_788", "truth_parsed_k": 290.4, "prediction_parsed_k_with_c_conv": 288.29, "prediction_parsed_k_strict": 288.29, "absolute_error_k_vs_c_conv_pred": 2.109999999999957, "score_vs_c_conv_pred": 22.34746713009027, "absolute_error_k_vs_strict_pred": 2.109999999999957, "score_vs_pred_strict": 22.34746713009027, "truth_raw_assistant_text": "The value is 290.40", "prediction_raw_text": "The value is 288.29\u0e0d\u0e35\u0e48"}
{"id": "line_789", "truth_parsed_k": 289.88, "prediction_parsed_k_with_c_conv": 284.55, "prediction_parsed_k_strict": 284.55, "absolute_error_k_vs_c_conv_pred": 5.329999999999984, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.329999999999984, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 289.88", "prediction_raw_text": "The value is 284.55."}
{"id": "line_790", "truth_parsed_k": 289.03, "prediction_parsed_k_with_c_conv": 284.52, "prediction_parsed_k_strict": 284.52, "absolute_error_k_vs_c_conv_pred": 4.509999999999991, "score_vs_c_conv_pred": 2.706136415331273, "absolute_error_k_vs_strict_pred": 4.509999999999991, "score_vs_pred_strict": 2.706136415331273, "truth_raw_assistant_text": "The value is 289.03", "prediction_raw_text": "The value is 284.52\u0e1d\u0e36\u0e01"}
{"id": "line_791", "truth_parsed_k": 288.3, "prediction_parsed_k_with_c_conv": 284.89, "prediction_parsed_k_strict": 284.89, "absolute_error_k_vs_c_conv_pred": 3.410000000000025, "score_vs_c_conv_pred": 10.001616206833198, "absolute_error_k_vs_strict_pred": 3.410000000000025, "score_vs_pred_strict": 10.001616206833198, "truth_raw_assistant_text": "The value is 288.30", "prediction_raw_text": "The value is 284.89"}
{"id": "line_792", "truth_parsed_k": 287.48, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 2.5400000000000205, "score_vs_c_conv_pred": 17.609095923295158, "absolute_error_k_vs_strict_pred": 2.5400000000000205, "score_vs_pred_strict": 17.609095923295158, "truth_raw_assistant_text": "The value is 287.48", "prediction_raw_text": "The value is 284.94."}
{"id": "line_793", "truth_parsed_k": 287.1, "prediction_parsed_k_with_c_conv": 285.82, "prediction_parsed_k_strict": 285.82, "absolute_error_k_vs_c_conv_pred": 1.2800000000000296, "score_vs_c_conv_pred": 34.84766685535777, "absolute_error_k_vs_strict_pred": 1.2800000000000296, "score_vs_pred_strict": 34.84766685535777, "truth_raw_assistant_text": "The value is 287.10", "prediction_raw_text": "The value is 285.82\u0e2d\u0e32\u0e23\u0e4c"}
{"id": "line_794", "truth_parsed_k": 287.42, "prediction_parsed_k_with_c_conv": 286.86, "prediction_parsed_k_strict": 286.86, "absolute_error_k_vs_c_conv_pred": 0.5600000000000023, "score_vs_c_conv_pred": 54.19221519323817, "absolute_error_k_vs_strict_pred": 0.5600000000000023, "score_vs_pred_strict": 54.19221519323817, "truth_raw_assistant_text": "The value is 287.42", "prediction_raw_text": "The value is 286.86\u0e27\u0e31\u0e19\u0e19\u0e35\u0e49"}
{"id": "line_795", "truth_parsed_k": 288.01, "prediction_parsed_k_with_c_conv": 285.99, "prediction_parsed_k_strict": 285.99, "absolute_error_k_vs_c_conv_pred": 2.019999999999982, "score_vs_c_conv_pred": 23.454263361515448, "absolute_error_k_vs_strict_pred": 2.019999999999982, "score_vs_pred_strict": 23.454263361515448, "truth_raw_assistant_text": "The value is 288.01", "prediction_raw_text": "The value is 285.99\u0e40\u0e23\u0e35\u0e22\u0e01\u0e27\u0e48\u0e32"}
{"id": "line_796", "truth_parsed_k": 288.93, "prediction_parsed_k_with_c_conv": 285.94, "prediction_parsed_k_strict": 285.94, "absolute_error_k_vs_c_conv_pred": 2.990000000000009, "score_vs_c_conv_pred": 13.40761304322623, "absolute_error_k_vs_strict_pred": 2.990000000000009, "score_vs_pred_strict": 13.40761304322623, "truth_raw_assistant_text": "The value is 288.93", "prediction_raw_text": "The value is 285.94\u0e23\u0e38\u0e48\u0e19"}
{"id": "line_797", "truth_parsed_k": 289.75, "prediction_parsed_k_with_c_conv": 284.92, "prediction_parsed_k_strict": 284.92, "absolute_error_k_vs_c_conv_pred": 4.829999999999984, "score_vs_c_conv_pred": 0.9083803043698935, "absolute_error_k_vs_strict_pred": 4.829999999999984, "score_vs_pred_strict": 0.9083803043698935, "truth_raw_assistant_text": "The value is 289.75", "prediction_raw_text": "The value is 284.92\u0e17\u0e35\u0e48\u0e14\u0e35"}
{"id": "line_798", "truth_parsed_k": 290.47, "prediction_parsed_k_with_c_conv": 288.09, "prediction_parsed_k_strict": 288.09, "absolute_error_k_vs_c_conv_pred": 2.3800000000000523, "score_vs_c_conv_pred": 19.276367271981144, "absolute_error_k_vs_strict_pred": 2.3800000000000523, "score_vs_pred_strict": 19.276367271981144, "truth_raw_assistant_text": "The value is 290.47", "prediction_raw_text": "The value is 288.09."}
{"id": "line_799", "truth_parsed_k": 290.8, "prediction_parsed_k_with_c_conv": 285.5, "prediction_parsed_k_strict": 285.5, "absolute_error_k_vs_c_conv_pred": 5.300000000000011, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.300000000000011, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.80", "prediction_raw_text": "The value is 285.50\u0e43\u0e0a\u0e49\u0e40\u0e27\u0e25\u0e32"}
{"id": "line_800", "truth_parsed_k": 290.51, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 5.539999999999964, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.539999999999964, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.51", "prediction_raw_text": "The value is 284.97"}
{"id": "line_801", "truth_parsed_k": 290.01, "prediction_parsed_k_with_c_conv": 286.6, "prediction_parsed_k_strict": 286.6, "absolute_error_k_vs_c_conv_pred": 3.409999999999968, "score_vs_c_conv_pred": 10.001616206833642, "absolute_error_k_vs_strict_pred": 3.409999999999968, "score_vs_pred_strict": 10.001616206833642, "truth_raw_assistant_text": "The value is 290.01", "prediction_raw_text": "The value is 286.60."}
{"id": "line_802", "truth_parsed_k": 289.26, "prediction_parsed_k_with_c_conv": 284.99, "prediction_parsed_k_strict": 284.99, "absolute_error_k_vs_c_conv_pred": 4.269999999999982, "score_vs_c_conv_pred": 4.137875639066491, "absolute_error_k_vs_strict_pred": 4.269999999999982, "score_vs_pred_strict": 4.137875639066491, "truth_raw_assistant_text": "The value is 289.26", "prediction_raw_text": "The value is 284.99\u0e43\u0e2b\u0e49\u0e44\u0e14\u0e49"}
{"id": "line_803", "truth_parsed_k": 288.51, "prediction_parsed_k_with_c_conv": 285.91, "prediction_parsed_k_strict": 285.91, "absolute_error_k_vs_c_conv_pred": 2.599999999999966, "score_vs_c_conv_pred": 17.009555370494255, "absolute_error_k_vs_strict_pred": 2.599999999999966, "score_vs_pred_strict": 17.009555370494255, "truth_raw_assistant_text": "The value is 288.51", "prediction_raw_text": "The value is 285.91."}
{"id": "line_804", "truth_parsed_k": 288.04, "prediction_parsed_k_with_c_conv": 286.75, "prediction_parsed_k_strict": 286.75, "absolute_error_k_vs_c_conv_pred": 1.2900000000000205, "score_vs_c_conv_pred": 34.656685552663426, "absolute_error_k_vs_strict_pred": 1.2900000000000205, "score_vs_pred_strict": 34.656685552663426, "truth_raw_assistant_text": "The value is 288.04", "prediction_raw_text": "The value is 286.75\u0e01\u0e25\u0e31\u0e27"}
{"id": "line_805", "truth_parsed_k": 287.56, "prediction_parsed_k_with_c_conv": 285.75, "prediction_parsed_k_strict": 285.75, "absolute_error_k_vs_c_conv_pred": 1.8100000000000023, "score_vs_c_conv_pred": 26.228740981648546, "absolute_error_k_vs_strict_pred": 1.8100000000000023, "score_vs_pred_strict": 26.228740981648546, "truth_raw_assistant_text": "The value is 287.56", "prediction_raw_text": "The value is 285.75."}
{"id": "line_806", "truth_parsed_k": 287.87, "prediction_parsed_k_with_c_conv": 284.62, "prediction_parsed_k_strict": 284.62, "absolute_error_k_vs_c_conv_pred": 3.25, "score_vs_c_conv_pred": 11.248878255127204, "absolute_error_k_vs_strict_pred": 3.25, "score_vs_pred_strict": 11.248878255127204, "truth_raw_assistant_text": "The value is 287.87", "prediction_raw_text": "The value is 284.62"}
{"id": "line_807", "truth_parsed_k": 288.22, "prediction_parsed_k_with_c_conv": 285.94, "prediction_parsed_k_strict": 285.94, "absolute_error_k_vs_c_conv_pred": 2.2800000000000296, "score_vs_c_conv_pred": 20.37339273014781, "absolute_error_k_vs_strict_pred": 2.2800000000000296, "score_vs_pred_strict": 20.37339273014781, "truth_raw_assistant_text": "The value is 288.22", "prediction_raw_text": "The value is 285.94"}
{"id": "line_808", "truth_parsed_k": 289.09, "prediction_parsed_k_with_c_conv": 285.35, "prediction_parsed_k_strict": 285.35, "absolute_error_k_vs_c_conv_pred": 3.7399999999999523, "score_vs_c_conv_pred": 7.598313373665477, "absolute_error_k_vs_strict_pred": 3.7399999999999523, "score_vs_pred_strict": 7.598313373665477, "truth_raw_assistant_text": "The value is 289.09", "prediction_raw_text": "The value is 285.35."}
{"id": "line_809", "truth_parsed_k": 289.84, "prediction_parsed_k_with_c_conv": 287.82, "prediction_parsed_k_strict": 287.82, "absolute_error_k_vs_c_conv_pred": 2.019999999999982, "score_vs_c_conv_pred": 23.454263361515448, "absolute_error_k_vs_strict_pred": 2.019999999999982, "score_vs_pred_strict": 23.454263361515448, "truth_raw_assistant_text": "The value is 289.84", "prediction_raw_text": "The value is 287.82\u0e0b\u0e48\u0e2d\u0e21"}
{"id": "line_810", "truth_parsed_k": 290.59, "prediction_parsed_k_with_c_conv": 286.68, "prediction_parsed_k_strict": 286.68, "absolute_error_k_vs_c_conv_pred": 3.909999999999968, "score_vs_c_conv_pred": 6.439197058986501, "absolute_error_k_vs_strict_pred": 3.909999999999968, "score_vs_pred_strict": 6.439197058986501, "truth_raw_assistant_text": "The value is 290.59", "prediction_raw_text": "The value is 286.68\u0e0a\u0e31\u0e48\u0e27"}
{"id": "line_811", "truth_parsed_k": 290.86, "prediction_parsed_k_with_c_conv": 285.94, "prediction_parsed_k_strict": 285.94, "absolute_error_k_vs_c_conv_pred": 4.920000000000016, "score_vs_c_conv_pred": 0.42365870208784795, "absolute_error_k_vs_strict_pred": 4.920000000000016, "score_vs_pred_strict": 0.42365870208784795, "truth_raw_assistant_text": "The value is 290.86", "prediction_raw_text": "The value is 285.94."}
{"id": "line_812", "truth_parsed_k": 290.58, "prediction_parsed_k_with_c_conv": 284.95, "prediction_parsed_k_strict": 284.95, "absolute_error_k_vs_c_conv_pred": 5.6299999999999955, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.6299999999999955, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.58", "prediction_raw_text": "The value is 284.95"}
{"id": "line_813", "truth_parsed_k": 289.82, "prediction_parsed_k_with_c_conv": 286.05, "prediction_parsed_k_strict": 286.05, "absolute_error_k_vs_c_conv_pred": 3.769999999999982, "score_vs_c_conv_pred": 7.390104211585157, "absolute_error_k_vs_strict_pred": 3.769999999999982, "score_vs_pred_strict": 7.390104211585157, "truth_raw_assistant_text": "The value is 289.82", "prediction_raw_text": "The value is 286.05."}
{"id": "line_814", "truth_parsed_k": 288.93, "prediction_parsed_k_with_c_conv": 284.73, "prediction_parsed_k_strict": 284.73, "absolute_error_k_vs_c_conv_pred": 4.199999999999989, "score_vs_c_conv_pred": 4.570219228066641, "absolute_error_k_vs_strict_pred": 4.199999999999989, "score_vs_pred_strict": 4.570219228066641, "truth_raw_assistant_text": "The value is 288.93", "prediction_raw_text": "The value is 284.73"}
{"id": "line_815", "truth_parsed_k": 288.34, "prediction_parsed_k_with_c_conv": 285.79, "prediction_parsed_k_strict": 285.79, "absolute_error_k_vs_c_conv_pred": 2.5499999999999545, "score_vs_c_conv_pred": 17.50824093346698, "absolute_error_k_vs_strict_pred": 2.5499999999999545, "score_vs_pred_strict": 17.50824093346698, "truth_raw_assistant_text": "The value is 288.34", "prediction_raw_text": "The value is 285.79>tagger"}
{"id": "line_816", "truth_parsed_k": 287.5, "prediction_parsed_k_with_c_conv": 285.36, "prediction_parsed_k_strict": 285.36, "absolute_error_k_vs_c_conv_pred": 2.1399999999999864, "score_vs_c_conv_pred": 21.988418466701564, "absolute_error_k_vs_strict_pred": 2.1399999999999864, "score_vs_pred_strict": 21.988418466701564, "truth_raw_assistant_text": "The value is 287.50", "prediction_raw_text": "The value is 285.36\u0e41\u0e1e\u0e49"}
{"id": "line_817", "truth_parsed_k": 287.11, "prediction_parsed_k_with_c_conv": 285.92, "prediction_parsed_k_strict": 285.92, "absolute_error_k_vs_c_conv_pred": 1.1899999999999977, "score_vs_c_conv_pred": 36.630334216240726, "absolute_error_k_vs_strict_pred": 1.1899999999999977, "score_vs_pred_strict": 36.630334216240726, "truth_raw_assistant_text": "The value is 287.11", "prediction_raw_text": "The value is 285.92"}
{"id": "line_818", "truth_parsed_k": 287.12, "prediction_parsed_k_with_c_conv": 284.62, "prediction_parsed_k_strict": 284.62, "absolute_error_k_vs_c_conv_pred": 2.5, "score_vs_c_conv_pred": 18.016336211152296, "absolute_error_k_vs_strict_pred": 2.5, "score_vs_pred_strict": 18.016336211152296, "truth_raw_assistant_text": "The value is 287.12", "prediction_raw_text": "The value is 284.62"}
{"id": "line_819", "truth_parsed_k": 287.75, "prediction_parsed_k_with_c_conv": 285.53, "prediction_parsed_k_strict": 285.53, "absolute_error_k_vs_c_conv_pred": 2.2200000000000273, "score_vs_c_conv_pred": 21.05372156733405, "absolute_error_k_vs_strict_pred": 2.2200000000000273, "score_vs_pred_strict": 21.05372156733405, "truth_raw_assistant_text": "The value is 287.75", "prediction_raw_text": "The value is 285.53."}
{"id": "line_820", "truth_parsed_k": 288.83, "prediction_parsed_k_with_c_conv": 285.7, "prediction_parsed_k_strict": 285.7, "absolute_error_k_vs_c_conv_pred": 3.1299999999999955, "score_vs_c_conv_pred": 12.223763029508095, "absolute_error_k_vs_strict_pred": 3.1299999999999955, "score_vs_pred_strict": 12.223763029508095, "truth_raw_assistant_text": "The value is 288.83", "prediction_raw_text": "The value is 285.70\u0646\u064f"}
{"id": "line_821", "truth_parsed_k": 289.65, "prediction_parsed_k_with_c_conv": 286.7, "prediction_parsed_k_strict": 286.7, "absolute_error_k_vs_c_conv_pred": 2.9499999999999886, "score_vs_c_conv_pred": 13.7556405151264, "absolute_error_k_vs_strict_pred": 2.9499999999999886, "score_vs_pred_strict": 13.7556405151264, "truth_raw_assistant_text": "The value is 289.65", "prediction_raw_text": "The value is 286.70\u0e15\u0e49\u0e2d\u0e19"}
{"id": "line_822", "truth_parsed_k": 290.09, "prediction_parsed_k_with_c_conv": 284.54, "prediction_parsed_k_strict": 284.54, "absolute_error_k_vs_c_conv_pred": 5.5499999999999545, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.5499999999999545, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.09", "prediction_raw_text": "The value is 284.54\u0e1c\u0e39\u0e49\u0e40\u0e25\u0e48\u0e19"}
{"id": "line_823", "truth_parsed_k": 290.45, "prediction_parsed_k_with_c_conv": 285.87, "prediction_parsed_k_strict": 285.87, "absolute_error_k_vs_c_conv_pred": 4.579999999999984, "score_vs_c_conv_pred": 2.302493406651873, "absolute_error_k_vs_strict_pred": 4.579999999999984, "score_vs_pred_strict": 2.302493406651873, "truth_raw_assistant_text": "The value is 290.45", "prediction_raw_text": "The value is 285.87"}
{"id": "line_824", "truth_parsed_k": 290.22, "prediction_parsed_k_with_c_conv": 285.84, "prediction_parsed_k_strict": 285.84, "absolute_error_k_vs_c_conv_pred": 4.380000000000052, "score_vs_c_conv_pred": 3.472199587029756, "absolute_error_k_vs_strict_pred": 4.380000000000052, "score_vs_pred_strict": 3.472199587029756, "truth_raw_assistant_text": "The value is 290.22", "prediction_raw_text": "The value is 285.84"}
{"id": "line_825", "truth_parsed_k": 289.59, "prediction_parsed_k_with_c_conv": 285.59, "prediction_parsed_k_strict": 285.59, "absolute_error_k_vs_c_conv_pred": 4.0, "score_vs_c_conv_pred": 5.845167438219834, "absolute_error_k_vs_strict_pred": 4.0, "score_vs_pred_strict": 5.845167438219834, "truth_raw_assistant_text": "The value is 289.59", "prediction_raw_text": "The value is 285.59."}
{"id": "line_826", "truth_parsed_k": 288.8, "prediction_parsed_k_with_c_conv": 287.6, "prediction_parsed_k_strict": 287.6, "absolute_error_k_vs_c_conv_pred": 1.1999999999999886, "score_vs_c_conv_pred": 36.42633134050889, "absolute_error_k_vs_strict_pred": 1.1999999999999886, "score_vs_pred_strict": 36.42633134050889, "truth_raw_assistant_text": "The value is 288.80", "prediction_raw_text": "The value is 287.60"}
{"id": "line_827", "truth_parsed_k": 287.97, "prediction_parsed_k_with_c_conv": 286.9, "prediction_parsed_k_strict": 286.9, "absolute_error_k_vs_c_conv_pred": 1.07000000000005, "score_vs_c_conv_pred": 39.20711146809928, "absolute_error_k_vs_strict_pred": 1.07000000000005, "score_vs_pred_strict": 39.20711146809928, "truth_raw_assistant_text": "The value is 287.97", "prediction_raw_text": "The value is 286.90"}
{"id": "line_828", "truth_parsed_k": 287.47, "prediction_parsed_k_with_c_conv": 285.28, "prediction_parsed_k_strict": 285.28, "absolute_error_k_vs_c_conv_pred": 2.1900000000000546, "score_vs_c_conv_pred": 21.4004420329331, "absolute_error_k_vs_strict_pred": 2.1900000000000546, "score_vs_pred_strict": 21.4004420329331, "truth_raw_assistant_text": "The value is 287.47", "prediction_raw_text": "The value is 285.28."}
{"id": "line_829", "truth_parsed_k": 286.85, "prediction_parsed_k_with_c_conv": 285.09, "prediction_parsed_k_strict": 285.09, "absolute_error_k_vs_c_conv_pred": 1.7600000000000477, "score_vs_c_conv_pred": 26.93370927394241, "absolute_error_k_vs_strict_pred": 1.7600000000000477, "score_vs_pred_strict": 26.93370927394241, "truth_raw_assistant_text": "The value is 286.85", "prediction_raw_text": "The value is 285.09\uf993"}
{"id": "line_830", "truth_parsed_k": 287.13, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 2.159999999999968, "score_vs_c_conv_pred": 21.751685066803795, "absolute_error_k_vs_strict_pred": 2.159999999999968, "score_vs_pred_strict": 21.751685066803795, "truth_raw_assistant_text": "The value is 287.13", "prediction_raw_text": "The value is 284.97\ufa22"}
{"id": "line_831", "truth_parsed_k": 287.47, "prediction_parsed_k_with_c_conv": 285.82, "prediction_parsed_k_strict": 285.82, "absolute_error_k_vs_c_conv_pred": 1.650000000000034, "score_vs_c_conv_pred": 28.552834926887183, "absolute_error_k_vs_strict_pred": 1.650000000000034, "score_vs_pred_strict": 28.552834926887183, "truth_raw_assistant_text": "The value is 287.47", "prediction_raw_text": "The value is 285.82\u0e01\u0e32\u0e23\u0e40\u0e21\u0e37\u0e2d\u0e07"}
{"id": "line_832", "truth_parsed_k": 288.6, "prediction_parsed_k_with_c_conv": 285.3, "prediction_parsed_k_strict": 285.3, "absolute_error_k_vs_c_conv_pred": 3.3000000000000114, "score_vs_c_conv_pred": 10.852867429820945, "absolute_error_k_vs_strict_pred": 3.3000000000000114, "score_vs_pred_strict": 10.852867429820945, "truth_raw_assistant_text": "The value is 288.60", "prediction_raw_text": "The value is 285.30\uf911"}
{"id": "line_833", "truth_parsed_k": 289.47, "prediction_parsed_k_with_c_conv": 285.59, "prediction_parsed_k_strict": 285.59, "absolute_error_k_vs_c_conv_pred": 3.8800000000000523, "score_vs_c_conv_pred": 6.640155087200861, "absolute_error_k_vs_strict_pred": 3.8800000000000523, "score_vs_pred_strict": 6.640155087200861, "truth_raw_assistant_text": "The value is 289.47", "prediction_raw_text": "The value is 285.59."}
{"id": "line_834", "truth_parsed_k": 290.11, "prediction_parsed_k_with_c_conv": 286.67, "prediction_parsed_k_strict": 286.67, "absolute_error_k_vs_c_conv_pred": 3.4399999999999977, "score_vs_c_conv_pred": 9.774052169676517, "absolute_error_k_vs_strict_pred": 3.4399999999999977, "score_vs_pred_strict": 9.774052169676517, "truth_raw_assistant_text": "The value is 290.11", "prediction_raw_text": "The value is 286.67\u0e04\u0e23\u0e31\u0e49\u0e07\u0e41\u0e23\u0e01"}
{"id": "line_835", "truth_parsed_k": 290.36, "prediction_parsed_k_with_c_conv": 284.83, "prediction_parsed_k_strict": 284.83, "absolute_error_k_vs_c_conv_pred": 5.53000000000003, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.53000000000003, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.36", "prediction_raw_text": "The value is 284.83\u0e43\u0e2b\u0e49\u0e40\u0e1b\u0e47\u0e19"}
{"id": "line_836", "truth_parsed_k": 290.23, "prediction_parsed_k_with_c_conv": 285.67, "prediction_parsed_k_strict": 285.67, "absolute_error_k_vs_c_conv_pred": 4.560000000000002, "score_vs_c_conv_pred": 2.417203913690291, "absolute_error_k_vs_strict_pred": 4.560000000000002, "score_vs_pred_strict": 2.417203913690291, "truth_raw_assistant_text": "The value is 290.23", "prediction_raw_text": "The value is 285.67."}
{"id": "line_837", "truth_parsed_k": 289.72, "prediction_parsed_k_with_c_conv": 286.89, "prediction_parsed_k_strict": 286.89, "absolute_error_k_vs_c_conv_pred": 2.830000000000041, "score_vs_c_conv_pred": 14.827553209572342, "absolute_error_k_vs_strict_pred": 2.830000000000041, "score_vs_pred_strict": 14.827553209572342, "truth_raw_assistant_text": "The value is 289.72", "prediction_raw_text": "The value is 286.89."}
{"id": "line_838", "truth_parsed_k": 288.85, "prediction_parsed_k_with_c_conv": 285.67, "prediction_parsed_k_strict": 285.67, "absolute_error_k_vs_c_conv_pred": 3.180000000000007, "score_vs_c_conv_pred": 11.813263452031219, "absolute_error_k_vs_strict_pred": 3.180000000000007, "score_vs_pred_strict": 11.813263452031219, "truth_raw_assistant_text": "The value is 288.85", "prediction_raw_text": "The value is 285.67."}
{"id": "line_839", "truth_parsed_k": 287.92, "prediction_parsed_k_with_c_conv": 285.89, "prediction_parsed_k_strict": 285.89, "absolute_error_k_vs_c_conv_pred": 2.0300000000000296, "score_vs_c_conv_pred": 23.329015457551083, "absolute_error_k_vs_strict_pred": 2.0300000000000296, "score_vs_pred_strict": 23.329015457551083, "truth_raw_assistant_text": "The value is 287.92", "prediction_raw_text": "The value is 285.89."}
{"id": "line_840", "truth_parsed_k": 287.27, "prediction_parsed_k_with_c_conv": 286.94, "prediction_parsed_k_strict": 286.94, "absolute_error_k_vs_c_conv_pred": 0.3299999999999841, "score_vs_c_conv_pred": 65.20913938273955, "absolute_error_k_vs_strict_pred": 0.3299999999999841, "score_vs_pred_strict": 65.20913938273955, "truth_raw_assistant_text": "The value is 287.27", "prediction_raw_text": "The value is 286.94\u0e2b\u0e49\u0e32"}
{"id": "line_841", "truth_parsed_k": 286.81, "prediction_parsed_k_with_c_conv": 286.92, "prediction_parsed_k_strict": 286.92, "absolute_error_k_vs_c_conv_pred": 0.11000000000001364, "score_vs_c_conv_pred": 83.00095526617443, "absolute_error_k_vs_strict_pred": 0.11000000000001364, "score_vs_pred_strict": 83.00095526617443, "truth_raw_assistant_text": "The value is 286.81", "prediction_raw_text": "The value is 286.92\u0e40\u0e01\u0e37\u0e2d\u0e1a"}
{"id": "line_842", "truth_parsed_k": 286.83, "prediction_parsed_k_with_c_conv": 284.98, "prediction_parsed_k_strict": 284.98, "absolute_error_k_vs_c_conv_pred": 1.849999999999966, "score_vs_c_conv_pred": 25.677760106529025, "absolute_error_k_vs_strict_pred": 1.849999999999966, "score_vs_pred_strict": 25.677760106529025, "truth_raw_assistant_text": "The value is 286.83", "prediction_raw_text": "The value is 284.98\u0643\u064f"}
{"id": "line_843", "truth_parsed_k": 287.46, "prediction_parsed_k_with_c_conv": 286.41, "prediction_parsed_k_strict": 286.41, "absolute_error_k_vs_c_conv_pred": 1.0499999999999545, "score_vs_c_conv_pred": 39.66160750275982, "absolute_error_k_vs_strict_pred": 1.0499999999999545, "score_vs_pred_strict": 39.66160750275982, "truth_raw_assistant_text": "The value is 287.46", "prediction_raw_text": "The value is 286.41\u0e17\u0e31\u0e48\u0e27\u0e44\u0e1b"}
{"id": "line_844", "truth_parsed_k": 288.51, "prediction_parsed_k_with_c_conv": 285.52, "prediction_parsed_k_strict": 285.52, "absolute_error_k_vs_c_conv_pred": 2.990000000000009, "score_vs_c_conv_pred": 13.40761304322623, "absolute_error_k_vs_strict_pred": 2.990000000000009, "score_vs_pred_strict": 13.40761304322623, "truth_raw_assistant_text": "The value is 288.51", "prediction_raw_text": "The value is 285.52\u0e2d\u0e31\u0e19\u0e15\u0e23"}
{"id": "line_845", "truth_parsed_k": 289.48, "prediction_parsed_k_with_c_conv": 286.96, "prediction_parsed_k_strict": 286.96, "absolute_error_k_vs_c_conv_pred": 2.5200000000000387, "score_vs_c_conv_pred": 17.811946229475527, "absolute_error_k_vs_strict_pred": 2.5200000000000387, "score_vs_pred_strict": 17.811946229475527, "truth_raw_assistant_text": "The value is 289.48", "prediction_raw_text": "The value is 286.96 \u0e18\u0e31\u0e19"}
{"id": "line_846", "truth_parsed_k": 290.16, "prediction_parsed_k_with_c_conv": 284.86, "prediction_parsed_k_strict": 284.86, "absolute_error_k_vs_c_conv_pred": 5.300000000000011, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.300000000000011, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.16", "prediction_raw_text": "The value is 284.86"}
{"id": "line_847", "truth_parsed_k": 290.54, "prediction_parsed_k_with_c_conv": 285.3, "prediction_parsed_k_strict": 285.3, "absolute_error_k_vs_c_conv_pred": 5.240000000000009, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.240000000000009, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.54", "prediction_raw_text": "The value is 285.30"}
{"id": "line_848", "truth_parsed_k": 290.47, "prediction_parsed_k_with_c_conv": 284.93, "prediction_parsed_k_strict": 284.93, "absolute_error_k_vs_c_conv_pred": 5.5400000000000205, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.5400000000000205, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.47", "prediction_raw_text": "The value is 284.93\u0e22\u0e31\u0e07\u0e04\u0e07"}
{"id": "line_849", "truth_parsed_k": 289.77, "prediction_parsed_k_with_c_conv": 285.34, "prediction_parsed_k_strict": 285.34, "absolute_error_k_vs_c_conv_pred": 4.430000000000007, "score_vs_c_conv_pred": 3.1749752581800905, "absolute_error_k_vs_strict_pred": 4.430000000000007, "score_vs_pred_strict": 3.1749752581800905, "truth_raw_assistant_text": "The value is 289.77", "prediction_raw_text": "The value is 285.34."}
{"id": "line_850", "truth_parsed_k": 288.88, "prediction_parsed_k_with_c_conv": 285.82, "prediction_parsed_k_strict": 285.82, "absolute_error_k_vs_c_conv_pred": 3.0600000000000023, "score_vs_c_conv_pred": 12.809182842181798, "absolute_error_k_vs_strict_pred": 3.0600000000000023, "score_vs_pred_strict": 12.809182842181798, "truth_raw_assistant_text": "The value is 288.88", "prediction_raw_text": "The value is 285.82\u0e40\u0e01\u0e35\u0e22\u0e23\u0e15\u0e34"}
{"id": "line_851", "truth_parsed_k": 288.22, "prediction_parsed_k_with_c_conv": 284.83, "prediction_parsed_k_strict": 284.83, "absolute_error_k_vs_c_conv_pred": 3.390000000000043, "score_vs_c_conv_pred": 10.154401018437465, "absolute_error_k_vs_strict_pred": 3.390000000000043, "score_vs_pred_strict": 10.154401018437465, "truth_raw_assistant_text": "The value is 288.22", "prediction_raw_text": "The value is 284.83\u0e25\u0e38\u0e22"}
{"id": "line_852", "truth_parsed_k": 287.63, "prediction_parsed_k_with_c_conv": 284.93, "prediction_parsed_k_strict": 284.93, "absolute_error_k_vs_c_conv_pred": 2.6999999999999886, "score_vs_c_conv_pred": 16.039062921826364, "absolute_error_k_vs_strict_pred": 2.6999999999999886, "score_vs_pred_strict": 16.039062921826364, "truth_raw_assistant_text": "The value is 287.63", "prediction_raw_text": "The value is 284.93."}
{"id": "line_853", "truth_parsed_k": 287.32, "prediction_parsed_k_with_c_conv": 285.44, "prediction_parsed_k_strict": 285.44, "absolute_error_k_vs_c_conv_pred": 1.8799999999999955, "score_vs_c_conv_pred": 25.271798888201523, "absolute_error_k_vs_strict_pred": 1.8799999999999955, "score_vs_pred_strict": 25.271798888201523, "truth_raw_assistant_text": "The value is 287.32", "prediction_raw_text": "The value is 285.44RGBO"}
{"id": "line_854", "truth_parsed_k": 287.55, "prediction_parsed_k_with_c_conv": 285.64, "prediction_parsed_k_strict": 285.64, "absolute_error_k_vs_c_conv_pred": 1.910000000000025, "score_vs_c_conv_pred": 24.87186702019766, "absolute_error_k_vs_strict_pred": 1.910000000000025, "score_vs_pred_strict": 24.87186702019766, "truth_raw_assistant_text": "The value is 287.55", "prediction_raw_text": "The value is 285.64."}
{"id": "line_855", "truth_parsed_k": 287.93, "prediction_parsed_k_with_c_conv": 286.8, "prediction_parsed_k_strict": 286.8, "absolute_error_k_vs_c_conv_pred": 1.1299999999999955, "score_vs_c_conv_pred": 37.8879129627178, "absolute_error_k_vs_strict_pred": 1.1299999999999955, "score_vs_pred_strict": 37.8879129627178, "truth_raw_assistant_text": "The value is 287.93", "prediction_raw_text": "The value is 286.80\u0e01\u0e23\u0e30\u0e40\u0e1b\u0e4b\u0e32"}
{"id": "line_856", "truth_parsed_k": 289.02, "prediction_parsed_k_with_c_conv": 286.34, "prediction_parsed_k_strict": 286.34, "absolute_error_k_vs_c_conv_pred": 2.680000000000007, "score_vs_c_conv_pred": 16.230383448158403, "absolute_error_k_vs_strict_pred": 2.680000000000007, "score_vs_pred_strict": 16.230383448158403, "truth_raw_assistant_text": "The value is 289.02", "prediction_raw_text": "The value is 286.34."}
{"id": "line_857", "truth_parsed_k": 289.96, "prediction_parsed_k_with_c_conv": 285.52, "prediction_parsed_k_strict": 285.52, "absolute_error_k_vs_c_conv_pred": 4.439999999999998, "score_vs_c_conv_pred": 3.115922052920661, "absolute_error_k_vs_strict_pred": 4.439999999999998, "score_vs_pred_strict": 3.115922052920661, "truth_raw_assistant_text": "The value is 289.96", "prediction_raw_text": "The value is 285.52\u0e2a\u0e31\u0e15"}
{"id": "line_858", "truth_parsed_k": 290.74, "prediction_parsed_k_with_c_conv": 284.74, "prediction_parsed_k_strict": 284.74, "absolute_error_k_vs_c_conv_pred": 6.0, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.0, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.74", "prediction_raw_text": "The value is 284.74."}
{"id": "line_859", "truth_parsed_k": 290.99, "prediction_parsed_k_with_c_conv": 286.28, "prediction_parsed_k_strict": 286.28, "absolute_error_k_vs_c_conv_pred": 4.710000000000036, "score_vs_c_conv_pred": 1.5685543480216224, "absolute_error_k_vs_strict_pred": 4.710000000000036, "score_vs_pred_strict": 1.5685543480216224, "truth_raw_assistant_text": "The value is 290.99", "prediction_raw_text": "The value is 286.28\u0e40\u0e1a\u0e37"}
{"id": "line_860", "truth_parsed_k": 290.8, "prediction_parsed_k_with_c_conv": 284.93, "prediction_parsed_k_strict": 284.93, "absolute_error_k_vs_c_conv_pred": 5.8700000000000045, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.8700000000000045, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.80", "prediction_raw_text": "The value is 284.93"}
{"id": "line_861", "truth_parsed_k": 290.05, "prediction_parsed_k_with_c_conv": 286.94, "prediction_parsed_k_strict": 286.94, "absolute_error_k_vs_c_conv_pred": 3.1100000000000136, "score_vs_c_conv_pred": 12.389731202387344, "absolute_error_k_vs_strict_pred": 3.1100000000000136, "score_vs_pred_strict": 12.389731202387344, "truth_raw_assistant_text": "The value is 290.05", "prediction_raw_text": "The value is 286.94."}
{"id": "line_862", "truth_parsed_k": 289.25, "prediction_parsed_k_with_c_conv": 284.86, "prediction_parsed_k_strict": 284.86, "absolute_error_k_vs_c_conv_pred": 4.389999999999986, "score_vs_c_conv_pred": 3.4124916893828128, "absolute_error_k_vs_strict_pred": 4.389999999999986, "score_vs_pred_strict": 3.4124916893828128, "truth_raw_assistant_text": "The value is 289.25", "prediction_raw_text": "The value is 284.86"}
{"id": "line_863", "truth_parsed_k": 288.62, "prediction_parsed_k_with_c_conv": 284.76, "prediction_parsed_k_strict": 284.76, "absolute_error_k_vs_c_conv_pred": 3.8600000000000136, "score_vs_c_conv_pred": 6.774965134999311, "absolute_error_k_vs_strict_pred": 3.8600000000000136, "score_vs_pred_strict": 6.774965134999311, "truth_raw_assistant_text": "The value is 288.62", "prediction_raw_text": "The value is 284.76\u0e04\u0e23\u0e31\u0e49\u0e07\u0e17\u0e35\u0e48"}
{"id": "line_864", "truth_parsed_k": 287.77, "prediction_parsed_k_with_c_conv": 284.95, "prediction_parsed_k_strict": 284.95, "absolute_error_k_vs_c_conv_pred": 2.819999999999993, "score_vs_c_conv_pred": 14.918835503106397, "absolute_error_k_vs_strict_pred": 2.819999999999993, "score_vs_pred_strict": 14.918835503106397, "truth_raw_assistant_text": "The value is 287.77", "prediction_raw_text": "The value is 284.95\u0e17\u0e31\u0e28"}
{"id": "line_865", "truth_parsed_k": 287.73, "prediction_parsed_k_with_c_conv": 285.5, "prediction_parsed_k_strict": 285.5, "absolute_error_k_vs_c_conv_pred": 2.230000000000018, "score_vs_c_conv_pred": 20.93913309311841, "absolute_error_k_vs_strict_pred": 2.230000000000018, "score_vs_pred_strict": 20.93913309311841, "truth_raw_assistant_text": "The value is 287.73", "prediction_raw_text": "The value is 285.50"}
{"id": "line_866", "truth_parsed_k": 287.63, "prediction_parsed_k_with_c_conv": 287.79, "prediction_parsed_k_strict": 287.79, "absolute_error_k_vs_c_conv_pred": 0.160000000000025, "score_vs_c_conv_pred": 77.80639696329173, "absolute_error_k_vs_strict_pred": 0.160000000000025, "score_vs_pred_strict": 77.80639696329173, "truth_raw_assistant_text": "The value is 287.63", "prediction_raw_text": "The value is 287.79"}
{"id": "line_867", "truth_parsed_k": 288.2, "prediction_parsed_k_with_c_conv": 285.99, "prediction_parsed_k_strict": 285.99, "absolute_error_k_vs_c_conv_pred": 2.2099999999999795, "score_vs_c_conv_pred": 21.16879973737197, "absolute_error_k_vs_strict_pred": 2.2099999999999795, "score_vs_pred_strict": 21.16879973737197, "truth_raw_assistant_text": "The value is 288.20", "prediction_raw_text": "The value is 285.99\u0e15\u0e31\u0e14\u0e2a\u0e34\u0e19"}
{"id": "line_868", "truth_parsed_k": 289.04, "prediction_parsed_k_with_c_conv": 285.34, "prediction_parsed_k_strict": 285.34, "absolute_error_k_vs_c_conv_pred": 3.7000000000000455, "score_vs_c_conv_pred": 7.878453749939229, "absolute_error_k_vs_strict_pred": 3.7000000000000455, "score_vs_pred_strict": 7.878453749939229, "truth_raw_assistant_text": "The value is 289.04", "prediction_raw_text": "The value is 285.34\u0e1c\u0e39\u0e49\u0e43\u0e0a\u0e49"}
{"id": "line_869", "truth_parsed_k": 289.99, "prediction_parsed_k_with_c_conv": 285.3, "prediction_parsed_k_strict": 285.3, "absolute_error_k_vs_c_conv_pred": 4.689999999999998, "score_vs_c_conv_pred": 1.6801742003018982, "absolute_error_k_vs_strict_pred": 4.689999999999998, "score_vs_pred_strict": 1.6801742003018982, "truth_raw_assistant_text": "The value is 289.99", "prediction_raw_text": "The value is 285.30\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e2a\u0e36\u0e01"}
{"id": "line_870", "truth_parsed_k": 290.66, "prediction_parsed_k_with_c_conv": 284.84, "prediction_parsed_k_strict": 284.84, "absolute_error_k_vs_c_conv_pred": 5.82000000000005, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.82000000000005, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.66", "prediction_raw_text": "The value is 284.84\u0e41\u0e15\u0e48\u0e25\u0e30"}
{"id": "line_871", "truth_parsed_k": 290.86, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 6.0400000000000205, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.0400000000000205, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.86", "prediction_raw_text": "The value is 284.82."}
{"id": "line_872", "truth_parsed_k": 290.72, "prediction_parsed_k_with_c_conv": 284.66, "prediction_parsed_k_strict": 284.66, "absolute_error_k_vs_c_conv_pred": 6.060000000000002, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.060000000000002, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.72", "prediction_raw_text": "The value is 284.66\u0e44\u0e21\u0e48\u0e23\u0e39\u0e49"}
{"id": "line_873", "truth_parsed_k": 289.99, "prediction_parsed_k_with_c_conv": 285.32, "prediction_parsed_k_strict": 285.32, "absolute_error_k_vs_c_conv_pred": 4.670000000000016, "score_vs_c_conv_pred": 1.7922586526814, "absolute_error_k_vs_strict_pred": 4.670000000000016, "score_vs_pred_strict": 1.7922586526814, "truth_raw_assistant_text": "The value is 289.99", "prediction_raw_text": "The value is 285.32\u0e2a\u0e34\u0e48\u0e07\u0e41\u0e27\u0e14\u0e25"}
{"id": "line_874", "truth_parsed_k": 289.22, "prediction_parsed_k_with_c_conv": 285.37, "prediction_parsed_k_strict": 285.37, "absolute_error_k_vs_c_conv_pred": 3.8500000000000227, "score_vs_c_conv_pred": 6.842624092217575, "absolute_error_k_vs_strict_pred": 3.8500000000000227, "score_vs_pred_strict": 6.842624092217575, "truth_raw_assistant_text": "The value is 289.22", "prediction_raw_text": "The value is 285.37\u0e2a\u0e19\u0e31\u0e1a\u0e2a\u0e19\u0e38\u0e19"}
{"id": "line_875", "truth_parsed_k": 288.42, "prediction_parsed_k_with_c_conv": 284.99, "prediction_parsed_k_strict": 284.99, "absolute_error_k_vs_c_conv_pred": 3.430000000000007, "score_vs_c_conv_pred": 9.849693373363877, "absolute_error_k_vs_strict_pred": 3.430000000000007, "score_vs_pred_strict": 9.849693373363877, "truth_raw_assistant_text": "The value is 288.42", "prediction_raw_text": "The value is 284.99\u0e0a\u0e39"}
{"id": "line_876", "truth_parsed_k": 287.82, "prediction_parsed_k_with_c_conv": 285.81, "prediction_parsed_k_strict": 285.81, "absolute_error_k_vs_c_conv_pred": 2.009999999999991, "score_vs_c_conv_pred": 23.580096538183405, "absolute_error_k_vs_strict_pred": 2.009999999999991, "score_vs_pred_strict": 23.580096538183405, "truth_raw_assistant_text": "The value is 287.82", "prediction_raw_text": "The value is 285.81\u0e15\u0e31\u0e27\u0e41\u0e17\u0e19"}
{"id": "line_877", "truth_parsed_k": 287.45, "prediction_parsed_k_with_c_conv": 284.86, "prediction_parsed_k_strict": 284.86, "absolute_error_k_vs_c_conv_pred": 2.589999999999975, "score_vs_c_conv_pred": 17.108556404088525, "absolute_error_k_vs_strict_pred": 2.589999999999975, "score_vs_pred_strict": 17.108556404088525, "truth_raw_assistant_text": "The value is 287.45", "prediction_raw_text": "The value is 284.86\u0e1b\u0e23\u0e30\u0e0a\u0e38\u0e21"}
{"id": "line_878", "truth_parsed_k": 287.53, "prediction_parsed_k_with_c_conv": 285.53, "prediction_parsed_k_strict": 285.53, "absolute_error_k_vs_c_conv_pred": 2.0, "score_vs_c_conv_pred": 23.70652048310418, "absolute_error_k_vs_strict_pred": 2.0, "score_vs_pred_strict": 23.70652048310418, "truth_raw_assistant_text": "The value is 287.53", "prediction_raw_text": "The value is 285.53."}
{"id": "line_879", "truth_parsed_k": 288.04, "prediction_parsed_k_with_c_conv": 285.43, "prediction_parsed_k_strict": 285.43, "absolute_error_k_vs_c_conv_pred": 2.6100000000000136, "score_vs_c_conv_pred": 16.91091697886058, "absolute_error_k_vs_strict_pred": 2.6100000000000136, "score_vs_pred_strict": 16.91091697886058, "truth_raw_assistant_text": "The value is 288.04", "prediction_raw_text": "The value is 285.43\u0e40\u0e19\u0e49\u0e19"}
{"id": "line_880", "truth_parsed_k": 289.03, "prediction_parsed_k_with_c_conv": 286.46, "prediction_parsed_k_strict": 286.46, "absolute_error_k_vs_c_conv_pred": 2.569999999999993, "score_vs_c_conv_pred": 17.307657132231746, "absolute_error_k_vs_strict_pred": 2.569999999999993, "score_vs_pred_strict": 17.307657132231746, "truth_raw_assistant_text": "The value is 289.03", "prediction_raw_text": "The value is 286.46"}
{"id": "line_881", "truth_parsed_k": 289.89, "prediction_parsed_k_with_c_conv": 285.42, "prediction_parsed_k_strict": 285.42, "absolute_error_k_vs_c_conv_pred": 4.46999999999997, "score_vs_c_conv_pred": 2.939535499945378, "absolute_error_k_vs_strict_pred": 4.46999999999997, "score_vs_pred_strict": 2.939535499945378, "truth_raw_assistant_text": "The value is 289.89", "prediction_raw_text": "The value is 285.42."}
{"id": "line_882", "truth_parsed_k": 290.6, "prediction_parsed_k_with_c_conv": 286.78, "prediction_parsed_k_strict": 286.78, "absolute_error_k_vs_c_conv_pred": 3.82000000000005, "score_vs_c_conv_pred": 7.046626967949399, "absolute_error_k_vs_strict_pred": 3.82000000000005, "score_vs_pred_strict": 7.046626967949399, "truth_raw_assistant_text": "The value is 290.60", "prediction_raw_text": "The value is 286.78 \u0e1c\u0e39\u0e49"}
{"id": "line_883", "truth_parsed_k": 290.78, "prediction_parsed_k_with_c_conv": 285.2, "prediction_parsed_k_strict": 285.2, "absolute_error_k_vs_c_conv_pred": 5.579999999999984, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.579999999999984, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.78", "prediction_raw_text": "The value is 285.20\u0e01\u0e23\u0e38\u0e07\u0e40\u0e17\u0e1e\u0e2f"}
{"id": "line_884", "truth_parsed_k": 290.68, "prediction_parsed_k_with_c_conv": 285.49, "prediction_parsed_k_strict": 285.49, "absolute_error_k_vs_c_conv_pred": 5.189999999999998, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.189999999999998, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.68", "prediction_raw_text": "The value is 285.49"}
{"id": "line_885", "truth_parsed_k": 290.1, "prediction_parsed_k_with_c_conv": 285.38, "prediction_parsed_k_strict": 285.38, "absolute_error_k_vs_c_conv_pred": 4.720000000000027, "score_vs_c_conv_pred": 1.512917446398121, "absolute_error_k_vs_strict_pred": 4.720000000000027, "score_vs_pred_strict": 1.512917446398121, "truth_raw_assistant_text": "The value is 290.10", "prediction_raw_text": "The value is 285.38\u0e40\u0e1b\u0e47\u0e19\u0e04\u0e19"}
{"id": "line_886", "truth_parsed_k": 289.23, "prediction_parsed_k_with_c_conv": 285.0, "prediction_parsed_k_strict": 285.0, "absolute_error_k_vs_c_conv_pred": 4.230000000000018, "score_vs_c_conv_pred": 4.38407850435194, "absolute_error_k_vs_strict_pred": 4.230000000000018, "score_vs_pred_strict": 4.38407850435194, "truth_raw_assistant_text": "The value is 289.23", "prediction_raw_text": "The value is 285.00\u0e1b\u0e23\u0e30\u0e15\u0e39"}
{"id": "line_887", "truth_parsed_k": 288.32, "prediction_parsed_k_with_c_conv": 284.55, "prediction_parsed_k_strict": 284.55, "absolute_error_k_vs_c_conv_pred": 3.769999999999982, "score_vs_c_conv_pred": 7.390104211585157, "absolute_error_k_vs_strict_pred": 3.769999999999982, "score_vs_pred_strict": 7.390104211585157, "truth_raw_assistant_text": "The value is 288.32", "prediction_raw_text": "The value is 284.55\u0e17\u0e31\u0e48\u0e27\u0e44\u0e1b"}
{"id": "line_888", "truth_parsed_k": 287.81, "prediction_parsed_k_with_c_conv": 284.99, "prediction_parsed_k_strict": 284.99, "absolute_error_k_vs_c_conv_pred": 2.819999999999993, "score_vs_c_conv_pred": 14.918835503106397, "absolute_error_k_vs_strict_pred": 2.819999999999993, "score_vs_pred_strict": 14.918835503106397, "truth_raw_assistant_text": "The value is 287.81", "prediction_raw_text": "The value is 284.99\uf9ff"}
{"id": "line_889", "truth_parsed_k": 287.47, "prediction_parsed_k_with_c_conv": 285.68, "prediction_parsed_k_strict": 285.68, "absolute_error_k_vs_c_conv_pred": 1.7900000000000205, "score_vs_c_conv_pred": 26.50851753270245, "absolute_error_k_vs_strict_pred": 1.7900000000000205, "score_vs_pred_strict": 26.50851753270245, "truth_raw_assistant_text": "The value is 287.47", "prediction_raw_text": "The value is 285.68\u0e22\u0e48\u0e32\u0e19"}
{"id": "line_890", "truth_parsed_k": 287.68, "prediction_parsed_k_with_c_conv": 284.92, "prediction_parsed_k_strict": 284.92, "absolute_error_k_vs_c_conv_pred": 2.759999999999991, "score_vs_c_conv_pred": 15.473124386525205, "absolute_error_k_vs_strict_pred": 2.759999999999991, "score_vs_pred_strict": 15.473124386525205, "truth_raw_assistant_text": "The value is 287.68", "prediction_raw_text": "The value is 284.92\u00e2\ufffd"}
{"id": "line_891", "truth_parsed_k": 288.22, "prediction_parsed_k_with_c_conv": 285.24, "prediction_parsed_k_strict": 285.24, "absolute_error_k_vs_c_conv_pred": 2.980000000000018, "score_vs_c_conv_pred": 13.494199133282237, "absolute_error_k_vs_strict_pred": 2.980000000000018, "score_vs_pred_strict": 13.494199133282237, "truth_raw_assistant_text": "The value is 288.22", "prediction_raw_text": "The value is 285.24."}
{"id": "line_892", "truth_parsed_k": 288.99, "prediction_parsed_k_with_c_conv": 284.81, "prediction_parsed_k_strict": 284.81, "absolute_error_k_vs_c_conv_pred": 4.180000000000007, "score_vs_c_conv_pred": 4.69503173798671, "absolute_error_k_vs_strict_pred": 4.180000000000007, "score_vs_pred_strict": 4.69503173798671, "truth_raw_assistant_text": "The value is 288.99", "prediction_raw_text": "The value is 284.81\u0e1c\u0e25\u0e34\u0e15\u0e20\u0e31\u0e13\u0e11"}
{"id": "line_893", "truth_parsed_k": 290.03, "prediction_parsed_k_with_c_conv": 285.59, "prediction_parsed_k_strict": 285.59, "absolute_error_k_vs_c_conv_pred": 4.439999999999998, "score_vs_c_conv_pred": 3.115922052920661, "absolute_error_k_vs_strict_pred": 4.439999999999998, "score_vs_pred_strict": 3.115922052920661, "truth_raw_assistant_text": "The value is 290.03", "prediction_raw_text": "The value is 285.59"}
{"id": "line_894", "truth_parsed_k": 290.67, "prediction_parsed_k_with_c_conv": 285.34, "prediction_parsed_k_strict": 285.34, "absolute_error_k_vs_c_conv_pred": 5.330000000000041, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.330000000000041, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.67", "prediction_raw_text": "The value is 285.34."}
{"id": "line_895", "truth_parsed_k": 290.8, "prediction_parsed_k_with_c_conv": 285.71, "prediction_parsed_k_strict": 285.71, "absolute_error_k_vs_c_conv_pred": 5.090000000000032, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.090000000000032, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.80", "prediction_raw_text": "The value is 285.71\uf963"}
{"id": "line_896", "truth_parsed_k": 290.63, "prediction_parsed_k_with_c_conv": 284.77, "prediction_parsed_k_strict": 284.77, "absolute_error_k_vs_c_conv_pred": 5.860000000000014, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.860000000000014, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.63", "prediction_raw_text": "The value is 284.77"}
{"id": "line_897", "truth_parsed_k": 290.11, "prediction_parsed_k_with_c_conv": 285.9, "prediction_parsed_k_strict": 285.9, "absolute_error_k_vs_c_conv_pred": 4.210000000000036, "score_vs_c_conv_pred": 4.508029244751654, "absolute_error_k_vs_strict_pred": 4.210000000000036, "score_vs_pred_strict": 4.508029244751654, "truth_raw_assistant_text": "The value is 290.11", "prediction_raw_text": "The value is 285.90"}
{"id": "line_898", "truth_parsed_k": 289.29, "prediction_parsed_k_with_c_conv": 286.76, "prediction_parsed_k_strict": 286.76, "absolute_error_k_vs_c_conv_pred": 2.5300000000000296, "score_vs_c_conv_pred": 17.71033006811247, "absolute_error_k_vs_strict_pred": 2.5300000000000296, "score_vs_pred_strict": 17.71033006811247, "truth_raw_assistant_text": "The value is 289.29", "prediction_raw_text": "The value is 286.76\u0e15\u0e34\u0e14\u0e15\u0e31\u0e49\u0e07"}
{"id": "line_899", "truth_parsed_k": 288.43, "prediction_parsed_k_with_c_conv": 286.85, "prediction_parsed_k_strict": 286.85, "absolute_error_k_vs_c_conv_pred": 1.579999999999984, "score_vs_c_conv_pred": 29.63630150486678, "absolute_error_k_vs_strict_pred": 1.579999999999984, "score_vs_pred_strict": 29.63630150486678, "truth_raw_assistant_text": "The value is 288.43", "prediction_raw_text": "The value is 286.85"}
{"id": "line_900", "truth_parsed_k": 287.83, "prediction_parsed_k_with_c_conv": 287.97, "prediction_parsed_k_strict": 287.97, "absolute_error_k_vs_c_conv_pred": 0.1400000000000432, "score_vs_c_conv_pred": 79.7656791039215, "absolute_error_k_vs_strict_pred": 0.1400000000000432, "score_vs_pred_strict": 79.7656791039215, "truth_raw_assistant_text": "The value is 287.83", "prediction_raw_text": "The value is 287.97"}
{"id": "line_901", "truth_parsed_k": 287.36, "prediction_parsed_k_with_c_conv": 286.09, "prediction_parsed_k_strict": 286.09, "absolute_error_k_vs_c_conv_pred": 1.2700000000000387, "score_vs_c_conv_pred": 35.04001232177314, "absolute_error_k_vs_strict_pred": 1.2700000000000387, "score_vs_pred_strict": 35.04001232177314, "truth_raw_assistant_text": "The value is 287.36", "prediction_raw_text": "The value is 286.09"}
{"id": "line_902", "truth_parsed_k": 287.62, "prediction_parsed_k_with_c_conv": 286.82, "prediction_parsed_k_strict": 286.82, "absolute_error_k_vs_c_conv_pred": 0.8000000000000114, "score_vs_c_conv_pred": 46.103644570269665, "absolute_error_k_vs_strict_pred": 0.8000000000000114, "score_vs_pred_strict": 46.103644570269665, "truth_raw_assistant_text": "The value is 287.62", "prediction_raw_text": "The value is 286.82\u0e23\u0e48\u0e27\u0e21\u0e01\u0e31\u0e19"}
{"id": "line_903", "truth_parsed_k": 288.05, "prediction_parsed_k_with_c_conv": 285.82, "prediction_parsed_k_strict": 285.82, "absolute_error_k_vs_c_conv_pred": 2.230000000000018, "score_vs_c_conv_pred": 20.93913309311841, "absolute_error_k_vs_strict_pred": 2.230000000000018, "score_vs_pred_strict": 20.93913309311841, "truth_raw_assistant_text": "The value is 288.05", "prediction_raw_text": "The value is 285.82"}
{"id": "line_904", "truth_parsed_k": 289.08, "prediction_parsed_k_with_c_conv": 285.24, "prediction_parsed_k_strict": 285.24, "absolute_error_k_vs_c_conv_pred": 3.839999999999975, "score_vs_c_conv_pred": 6.91045347520115, "absolute_error_k_vs_strict_pred": 3.839999999999975, "score_vs_pred_strict": 6.91045347520115, "truth_raw_assistant_text": "The value is 289.08", "prediction_raw_text": "The value is 285.24\u0e01\u0e23\u0e31"}
{"id": "line_905", "truth_parsed_k": 289.82, "prediction_parsed_k_with_c_conv": 284.76, "prediction_parsed_k_strict": 284.76, "absolute_error_k_vs_c_conv_pred": 5.060000000000002, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.060000000000002, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 289.82", "prediction_raw_text": "The value is 284.76\uf9f7"}
{"id": "line_906", "truth_parsed_k": 290.57, "prediction_parsed_k_with_c_conv": 286.0, "prediction_parsed_k_strict": 286.0, "absolute_error_k_vs_c_conv_pred": 4.569999999999993, "score_vs_c_conv_pred": 2.359787578917749, "absolute_error_k_vs_strict_pred": 4.569999999999993, "score_vs_pred_strict": 2.359787578917749, "truth_raw_assistant_text": "The value is 290.57", "prediction_raw_text": "The value is 286.00\u0e40\u0e2a\u0e35\u0e48\u0e22\u0e07"}
{"id": "line_907", "truth_parsed_k": 290.86, "prediction_parsed_k_with_c_conv": 285.87, "prediction_parsed_k_strict": 285.87, "absolute_error_k_vs_c_conv_pred": 4.990000000000009, "score_vs_c_conv_pred": 0.052594256575544396, "absolute_error_k_vs_strict_pred": 4.990000000000009, "score_vs_pred_strict": 0.052594256575544396, "truth_raw_assistant_text": "The value is 290.86", "prediction_raw_text": "The value is 285.87\u0e1b\u0e23\u0e36\u0e01\u0e29\u0e32"}
{"id": "line_908", "truth_parsed_k": 290.59, "prediction_parsed_k_with_c_conv": 287.45, "prediction_parsed_k_strict": 287.45, "absolute_error_k_vs_c_conv_pred": 3.1399999999999864, "score_vs_c_conv_pred": 12.141160968399278, "absolute_error_k_vs_strict_pred": 3.1399999999999864, "score_vs_pred_strict": 12.141160968399278, "truth_raw_assistant_text": "The value is 290.59", "prediction_raw_text": "The value is 287.45."}
{"id": "line_909", "truth_parsed_k": 290.14, "prediction_parsed_k_with_c_conv": 284.88, "prediction_parsed_k_strict": 284.88, "absolute_error_k_vs_c_conv_pred": 5.259999999999991, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.259999999999991, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.14", "prediction_raw_text": "The value is 284.88\u0e25\u0e31\u0e1a"}
{"id": "line_910", "truth_parsed_k": 289.13, "prediction_parsed_k_with_c_conv": 286.35, "prediction_parsed_k_strict": 286.35, "absolute_error_k_vs_c_conv_pred": 2.7799999999999727, "score_vs_c_conv_pred": 15.287090825853355, "absolute_error_k_vs_strict_pred": 2.7799999999999727, "score_vs_pred_strict": 15.287090825853355, "truth_raw_assistant_text": "The value is 289.13", "prediction_raw_text": "The value is 286.35."}
{"id": "line_911", "truth_parsed_k": 288.48, "prediction_parsed_k_with_c_conv": 284.95, "prediction_parsed_k_strict": 284.95, "absolute_error_k_vs_c_conv_pred": 3.5300000000000296, "score_vs_c_conv_pred": 9.102675961334395, "absolute_error_k_vs_strict_pred": 3.5300000000000296, "score_vs_pred_strict": 9.102675961334395, "truth_raw_assistant_text": "The value is 288.48", "prediction_raw_text": "The value is 284.95"}
{"id": "line_912", "truth_parsed_k": 287.78, "prediction_parsed_k_with_c_conv": 286.38, "prediction_parsed_k_strict": 286.38, "absolute_error_k_vs_c_conv_pred": 1.3999999999999773, "score_vs_c_conv_pred": 32.64070531532324, "absolute_error_k_vs_strict_pred": 1.3999999999999773, "score_vs_pred_strict": 32.64070531532324, "truth_raw_assistant_text": "The value is 287.78", "prediction_raw_text": "The value is 286.38\u0e32\u0e18\u0e34"}
{"id": "line_913", "truth_parsed_k": 287.49, "prediction_parsed_k_with_c_conv": 286.5, "prediction_parsed_k_strict": 286.5, "absolute_error_k_vs_c_conv_pred": 0.9900000000000091, "score_vs_c_conv_pred": 41.07301791145854, "absolute_error_k_vs_strict_pred": 0.9900000000000091, "score_vs_pred_strict": 41.07301791145854, "truth_raw_assistant_text": "The value is 287.49", "prediction_raw_text": "The value is 286.50\u0e20\u0e31"}
{"id": "line_914", "truth_parsed_k": 287.66, "prediction_parsed_k_with_c_conv": 284.88, "prediction_parsed_k_strict": 284.88, "absolute_error_k_vs_c_conv_pred": 2.7800000000000296, "score_vs_c_conv_pred": 15.287090825852822, "absolute_error_k_vs_strict_pred": 2.7800000000000296, "score_vs_pred_strict": 15.287090825852822, "truth_raw_assistant_text": "The value is 287.66", "prediction_raw_text": "The value is 284.88\u0e40\u0e25\u0e48\u0e21"}
{"id": "line_915", "truth_parsed_k": 287.85, "prediction_parsed_k_with_c_conv": 285.91, "prediction_parsed_k_strict": 285.91, "absolute_error_k_vs_c_conv_pred": 1.9399999999999977, "score_vs_c_conv_pred": 24.477788024137894, "absolute_error_k_vs_strict_pred": 1.9399999999999977, "score_vs_pred_strict": 24.477788024137894, "truth_raw_assistant_text": "The value is 287.85", "prediction_raw_text": "The value is 285.91\u0e01\u0e47\u0e22\u0e31\u0e07"}
{"id": "line_916", "truth_parsed_k": 288.94, "prediction_parsed_k_with_c_conv": 285.3, "prediction_parsed_k_strict": 285.3, "absolute_error_k_vs_c_conv_pred": 3.6399999999999864, "score_vs_c_conv_pred": 8.30420571442686, "absolute_error_k_vs_strict_pred": 3.6399999999999864, "score_vs_pred_strict": 8.30420571442686, "truth_raw_assistant_text": "The value is 288.94", "prediction_raw_text": "The value is 285.30\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19"}
{"id": "line_917", "truth_parsed_k": 289.84, "prediction_parsed_k_with_c_conv": 286.86, "prediction_parsed_k_strict": 286.86, "absolute_error_k_vs_c_conv_pred": 2.9799999999999613, "score_vs_c_conv_pred": 13.494199133282736, "absolute_error_k_vs_strict_pred": 2.9799999999999613, "score_vs_pred_strict": 13.494199133282736, "truth_raw_assistant_text": "The value is 289.84", "prediction_raw_text": "The value is 286.86 cu\u0309a"}
{"id": "line_918", "truth_parsed_k": 290.6, "prediction_parsed_k_with_c_conv": 285.86, "prediction_parsed_k_strict": 285.86, "absolute_error_k_vs_c_conv_pred": 4.740000000000009, "score_vs_c_conv_pred": 1.4019873170983632, "absolute_error_k_vs_strict_pred": 4.740000000000009, "score_vs_pred_strict": 1.4019873170983632, "truth_raw_assistant_text": "The value is 290.60", "prediction_raw_text": "The value is 285.86"}
{"id": "line_919", "truth_parsed_k": 290.98, "prediction_parsed_k_with_c_conv": 284.81, "prediction_parsed_k_strict": 284.81, "absolute_error_k_vs_c_conv_pred": 6.170000000000016, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.170000000000016, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.98", "prediction_raw_text": "The value is 284.81."}
{"id": "line_920", "truth_parsed_k": 290.74, "prediction_parsed_k_with_c_conv": 286.89, "prediction_parsed_k_strict": 286.89, "absolute_error_k_vs_c_conv_pred": 3.8500000000000227, "score_vs_c_conv_pred": 6.842624092217575, "absolute_error_k_vs_strict_pred": 3.8500000000000227, "score_vs_pred_strict": 6.842624092217575, "truth_raw_assistant_text": "The value is 290.74", "prediction_raw_text": "The value is 286.89."}
{"id": "line_921", "truth_parsed_k": 290.05, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 5.110000000000014, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.110000000000014, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.05", "prediction_raw_text": "The value is 284.94."}
{"id": "line_922", "truth_parsed_k": 289.35, "prediction_parsed_k_with_c_conv": 286.52, "prediction_parsed_k_strict": 286.52, "absolute_error_k_vs_c_conv_pred": 2.830000000000041, "score_vs_c_conv_pred": 14.827553209572342, "absolute_error_k_vs_strict_pred": 2.830000000000041, "score_vs_pred_strict": 14.827553209572342, "truth_raw_assistant_text": "The value is 289.35", "prediction_raw_text": "The value is 286.52\u0e22\u0e34\u0e19"}
{"id": "line_923", "truth_parsed_k": 288.44, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 3.6200000000000045, "score_vs_c_conv_pred": 8.447632156326934, "absolute_error_k_vs_strict_pred": 3.6200000000000045, "score_vs_pred_strict": 8.447632156326934, "truth_raw_assistant_text": "The value is 288.44", "prediction_raw_text": "The value is 284.82"}
{"id": "line_924", "truth_parsed_k": 287.95, "prediction_parsed_k_with_c_conv": 286.7, "prediction_parsed_k_strict": 286.7, "absolute_error_k_vs_c_conv_pred": 1.25, "score_vs_c_conv_pred": 35.42887468651077, "absolute_error_k_vs_strict_pred": 1.25, "score_vs_pred_strict": 35.42887468651077, "truth_raw_assistant_text": "The value is 287.95", "prediction_raw_text": "The value is 286.70\u0e1e\u0e34\u0e21"}
{"id": "line_925", "truth_parsed_k": 287.72, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 2.75, "score_vs_c_conv_pred": 15.56662535132074, "absolute_error_k_vs_strict_pred": 2.75, "score_vs_pred_strict": 15.56662535132074, "truth_raw_assistant_text": "The value is 287.72", "prediction_raw_text": "The value is 284.97"}
{"id": "line_926", "truth_parsed_k": 287.62, "prediction_parsed_k_with_c_conv": 285.57, "prediction_parsed_k_strict": 285.57, "absolute_error_k_vs_c_conv_pred": 2.0500000000000114, "score_vs_c_conv_pred": 23.08025386577387, "absolute_error_k_vs_strict_pred": 2.0500000000000114, "score_vs_pred_strict": 23.08025386577387, "truth_raw_assistant_text": "The value is 287.62", "prediction_raw_text": "The value is 285.57"}
{"id": "line_927", "truth_parsed_k": 288.08, "prediction_parsed_k_with_c_conv": 286.58, "prediction_parsed_k_strict": 286.58, "absolute_error_k_vs_c_conv_pred": 1.5, "score_vs_c_conv_pred": 30.93040039646092, "absolute_error_k_vs_strict_pred": 1.5, "score_vs_pred_strict": 30.93040039646092, "truth_raw_assistant_text": "The value is 288.08", "prediction_raw_text": "The value is 286.58\u0e40\u0e23\u0e35\u0e22\u0e01\u0e27\u0e48\u0e32"}
{"id": "line_928", "truth_parsed_k": 289.03, "prediction_parsed_k_with_c_conv": 286.79, "prediction_parsed_k_strict": 286.79, "absolute_error_k_vs_c_conv_pred": 2.2399999999999523, "score_vs_c_conv_pred": 20.825030164741754, "absolute_error_k_vs_strict_pred": 2.2399999999999523, "score_vs_pred_strict": 20.825030164741754, "truth_raw_assistant_text": "The value is 289.03", "prediction_raw_text": "The value is 286.79\u0e25\u0e48\u0e32\u0e2a\u0e38\u0e14"}
{"id": "line_929", "truth_parsed_k": 290.05, "prediction_parsed_k_with_c_conv": 285.86, "prediction_parsed_k_strict": 285.86, "absolute_error_k_vs_c_conv_pred": 4.189999999999998, "score_vs_c_conv_pred": 4.632553169804399, "absolute_error_k_vs_strict_pred": 4.189999999999998, "score_vs_pred_strict": 4.632553169804399, "truth_raw_assistant_text": "The value is 290.05", "prediction_raw_text": "The value is 285.86\u0e41\u0e02\u0e47\u0e07"}
{"id": "line_930", "truth_parsed_k": 290.59, "prediction_parsed_k_with_c_conv": 284.67, "prediction_parsed_k_strict": 284.67, "absolute_error_k_vs_c_conv_pred": 5.919999999999959, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.919999999999959, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.59", "prediction_raw_text": "The value is 284.67"}
{"id": "line_931", "truth_parsed_k": 290.96, "prediction_parsed_k_with_c_conv": 284.64, "prediction_parsed_k_strict": 284.64, "absolute_error_k_vs_c_conv_pred": 6.319999999999993, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.319999999999993, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.96", "prediction_raw_text": "The value is 284.64\u0e21\u0e39\u0e25"}
{"id": "line_932", "truth_parsed_k": 290.83, "prediction_parsed_k_with_c_conv": 284.91, "prediction_parsed_k_strict": 284.91, "absolute_error_k_vs_c_conv_pred": 5.919999999999959, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.919999999999959, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.83", "prediction_raw_text": "The value is 284.91."}
{"id": "line_933", "truth_parsed_k": 290.25, "prediction_parsed_k_with_c_conv": 284.54, "prediction_parsed_k_strict": 284.54, "absolute_error_k_vs_c_conv_pred": 5.7099999999999795, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.7099999999999795, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.25", "prediction_raw_text": "The value is 284.54\u0e2b\u0e19\u0e48\u0e2d\u0e22"}
{"id": "line_934", "truth_parsed_k": 289.32, "prediction_parsed_k_with_c_conv": 284.93, "prediction_parsed_k_strict": 284.93, "absolute_error_k_vs_c_conv_pred": 4.389999999999986, "score_vs_c_conv_pred": 3.4124916893828128, "absolute_error_k_vs_strict_pred": 4.389999999999986, "score_vs_pred_strict": 3.4124916893828128, "truth_raw_assistant_text": "The value is 289.32", "prediction_raw_text": "The value is 284.93\u0e40\u0e1b\u0e47\u0e19\u0e04\u0e19"}
{"id": "line_935", "truth_parsed_k": 288.67, "prediction_parsed_k_with_c_conv": 285.64, "prediction_parsed_k_strict": 285.64, "absolute_error_k_vs_c_conv_pred": 3.0300000000000296, "score_vs_c_conv_pred": 13.064026234499227, "absolute_error_k_vs_strict_pred": 3.0300000000000296, "score_vs_pred_strict": 13.064026234499227, "truth_raw_assistant_text": "The value is 288.67", "prediction_raw_text": "The value is 285.64."}
{"id": "line_936", "truth_parsed_k": 287.99, "prediction_parsed_k_with_c_conv": 285.94, "prediction_parsed_k_strict": 285.94, "absolute_error_k_vs_c_conv_pred": 2.0500000000000114, "score_vs_c_conv_pred": 23.08025386577387, "absolute_error_k_vs_strict_pred": 2.0500000000000114, "score_vs_pred_strict": 23.08025386577387, "truth_raw_assistant_text": "The value is 287.99", "prediction_raw_text": "The value is 285.94\u0e44\u0e1f\u0e25\u0e4c"}
{"id": "line_937", "truth_parsed_k": 287.56, "prediction_parsed_k_with_c_conv": 285.8, "prediction_parsed_k_strict": 285.8, "absolute_error_k_vs_c_conv_pred": 1.759999999999991, "score_vs_c_conv_pred": 26.93370927394322, "absolute_error_k_vs_strict_pred": 1.759999999999991, "score_vs_pred_strict": 26.93370927394322, "truth_raw_assistant_text": "The value is 287.56", "prediction_raw_text": "The value is 285.80\u0e25\u0e48\u0e32\u0e2a\u0e38\u0e14"}
{"id": "line_938", "truth_parsed_k": 287.64, "prediction_parsed_k_with_c_conv": 284.75, "prediction_parsed_k_strict": 284.75, "absolute_error_k_vs_c_conv_pred": 2.8899999999999864, "score_vs_c_conv_pred": 14.28626361130939, "absolute_error_k_vs_strict_pred": 2.8899999999999864, "score_vs_pred_strict": 14.28626361130939, "truth_raw_assistant_text": "The value is 287.64", "prediction_raw_text": "The value is 284.75 \u064a\u064e"}
{"id": "line_939", "truth_parsed_k": 288.24, "prediction_parsed_k_with_c_conv": 285.81, "prediction_parsed_k_strict": 285.81, "absolute_error_k_vs_c_conv_pred": 2.430000000000007, "score_vs_c_conv_pred": 18.744171080179186, "absolute_error_k_vs_strict_pred": 2.430000000000007, "score_vs_pred_strict": 18.744171080179186, "truth_raw_assistant_text": "The value is 288.24", "prediction_raw_text": "The value is 285.81\uf9a3"}
{"id": "line_940", "truth_parsed_k": 289.09, "prediction_parsed_k_with_c_conv": 286.87, "prediction_parsed_k_strict": 286.87, "absolute_error_k_vs_c_conv_pred": 2.2199999999999704, "score_vs_c_conv_pred": 21.053721567334694, "absolute_error_k_vs_strict_pred": 2.2199999999999704, "score_vs_pred_strict": 21.053721567334694, "truth_raw_assistant_text": "The value is 289.09", "prediction_raw_text": "The value is 286.87\u0e40\u0e2a\u0e35\u0e22\u0e07"}
{"id": "line_941", "truth_parsed_k": 290.03, "prediction_parsed_k_with_c_conv": 284.93, "prediction_parsed_k_strict": 284.93, "absolute_error_k_vs_c_conv_pred": 5.099999999999966, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.099999999999966, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.03", "prediction_raw_text": "The value is 284.93"}
{"id": "line_942", "truth_parsed_k": 290.78, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 5.839999999999975, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.839999999999975, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.78", "prediction_raw_text": "The value is 284.94."}
{"id": "line_943", "truth_parsed_k": 290.96, "prediction_parsed_k_with_c_conv": 285.67, "prediction_parsed_k_strict": 285.67, "absolute_error_k_vs_c_conv_pred": 5.289999999999964, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.289999999999964, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.96", "prediction_raw_text": "The value is 285.67\u0e40\u0e2b\u0e19\u0e37\u0e2d"}
{"id": "line_944", "truth_parsed_k": 290.87, "prediction_parsed_k_with_c_conv": 285.62, "prediction_parsed_k_strict": 285.62, "absolute_error_k_vs_c_conv_pred": 5.25, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.25, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.87", "prediction_raw_text": "The value is 285.62 \u0e15\u0e38"}
{"id": "line_945", "truth_parsed_k": 290.29, "prediction_parsed_k_with_c_conv": 284.87, "prediction_parsed_k_strict": 284.87, "absolute_error_k_vs_c_conv_pred": 5.420000000000016, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.420000000000016, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.29", "prediction_raw_text": "The value is 284.87\u0e1e\u0e23\u0e35"}
{"id": "line_946", "truth_parsed_k": 289.64, "prediction_parsed_k_with_c_conv": 285.95, "prediction_parsed_k_strict": 285.95, "absolute_error_k_vs_c_conv_pred": 3.6899999999999977, "score_vs_c_conv_pred": 7.9489465933544, "absolute_error_k_vs_strict_pred": 3.6899999999999977, "score_vs_pred_strict": 7.9489465933544, "truth_raw_assistant_text": "The value is 289.64", "prediction_raw_text": "The value is 285.95."}
{"id": "line_947", "truth_parsed_k": 288.85, "prediction_parsed_k_with_c_conv": 285.82, "prediction_parsed_k_strict": 285.82, "absolute_error_k_vs_c_conv_pred": 3.0300000000000296, "score_vs_c_conv_pred": 13.064026234499227, "absolute_error_k_vs_strict_pred": 3.0300000000000296, "score_vs_pred_strict": 13.064026234499227, "truth_raw_assistant_text": "The value is 288.85", "prediction_raw_text": "The value is 285.82\u0e17\u0e31\u0e01\u0e29"}
{"id": "line_948", "truth_parsed_k": 288.23, "prediction_parsed_k_with_c_conv": 285.3, "prediction_parsed_k_strict": 285.3, "absolute_error_k_vs_c_conv_pred": 2.930000000000007, "score_vs_c_conv_pred": 13.931355662636857, "absolute_error_k_vs_strict_pred": 2.930000000000007, "score_vs_pred_strict": 13.931355662636857, "truth_raw_assistant_text": "The value is 288.23", "prediction_raw_text": "The value is 285.30\uf9e9"}
{"id": "line_949", "truth_parsed_k": 288.05, "prediction_parsed_k_with_c_conv": 285.0, "prediction_parsed_k_strict": 285.0, "absolute_error_k_vs_c_conv_pred": 3.0500000000000114, "score_vs_c_conv_pred": 12.893862945946688, "absolute_error_k_vs_strict_pred": 3.0500000000000114, "score_vs_pred_strict": 12.893862945946688, "truth_raw_assistant_text": "The value is 288.05", "prediction_raw_text": "The value is 285.00\u0e40\u0e01\u0e35\u0e22\u0e23\u0e15\u0e34"}
{"id": "line_950", "truth_parsed_k": 288.09, "prediction_parsed_k_with_c_conv": 284.9, "prediction_parsed_k_strict": 284.9, "absolute_error_k_vs_c_conv_pred": 3.1899999999999977, "score_vs_c_conv_pred": 11.731909158108634, "absolute_error_k_vs_strict_pred": 3.1899999999999977, "score_vs_pred_strict": 11.731909158108634, "truth_raw_assistant_text": "The value is 288.09", "prediction_raw_text": "The value is 284.90"}
{"id": "line_951", "truth_parsed_k": 288.5, "prediction_parsed_k_with_c_conv": 285.98, "prediction_parsed_k_strict": 285.98, "absolute_error_k_vs_c_conv_pred": 2.519999999999982, "score_vs_c_conv_pred": 17.811946229476117, "absolute_error_k_vs_strict_pred": 2.519999999999982, "score_vs_pred_strict": 17.811946229476117, "truth_raw_assistant_text": "The value is 288.50", "prediction_raw_text": "The value is 285.98\u0646\u0651"}
{"id": "line_952", "truth_parsed_k": 289.39, "prediction_parsed_k_with_c_conv": 284.46, "prediction_parsed_k_strict": 284.46, "absolute_error_k_vs_c_conv_pred": 4.930000000000007, "score_vs_c_conv_pred": 0.3703354157400529, "absolute_error_k_vs_strict_pred": 4.930000000000007, "score_vs_pred_strict": 0.3703354157400529, "truth_raw_assistant_text": "The value is 289.39", "prediction_raw_text": "The value is 284.46"}
{"id": "line_953", "truth_parsed_k": 290.37, "prediction_parsed_k_with_c_conv": 284.6, "prediction_parsed_k_strict": 284.6, "absolute_error_k_vs_c_conv_pred": 5.769999999999982, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.769999999999982, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.37", "prediction_raw_text": "The value is 284.60"}
{"id": "line_954", "truth_parsed_k": 291.07, "prediction_parsed_k_with_c_conv": 285.68, "prediction_parsed_k_strict": 285.68, "absolute_error_k_vs_c_conv_pred": 5.389999999999986, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.389999999999986, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.07", "prediction_raw_text": "The value is 285.68\ufb34"}
{"id": "line_955", "truth_parsed_k": 291.3, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 6.329999999999984, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.329999999999984, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.30", "prediction_raw_text": "The value is 284.97"}
{"id": "line_956", "truth_parsed_k": 291.14, "prediction_parsed_k_with_c_conv": 284.95, "prediction_parsed_k_strict": 284.95, "absolute_error_k_vs_c_conv_pred": 6.189999999999998, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.189999999999998, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.14", "prediction_raw_text": "The value is 284.95."}
{"id": "line_957", "truth_parsed_k": 290.46, "prediction_parsed_k_with_c_conv": 284.62, "prediction_parsed_k_strict": 284.62, "absolute_error_k_vs_c_conv_pred": 5.839999999999975, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.839999999999975, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.46", "prediction_raw_text": "The value is 284.62."}
{"id": "line_958", "truth_parsed_k": 289.67, "prediction_parsed_k_with_c_conv": 285.29, "prediction_parsed_k_strict": 285.29, "absolute_error_k_vs_c_conv_pred": 4.3799999999999955, "score_vs_c_conv_pred": 3.4721995870300892, "absolute_error_k_vs_strict_pred": 4.3799999999999955, "score_vs_pred_strict": 3.4721995870300892, "truth_raw_assistant_text": "The value is 289.67", "prediction_raw_text": "The value is 285.29."}
{"id": "line_959", "truth_parsed_k": 288.86, "prediction_parsed_k_with_c_conv": 284.53, "prediction_parsed_k_strict": 284.53, "absolute_error_k_vs_c_conv_pred": 4.330000000000041, "score_vs_c_conv_pred": 3.7727412201812527, "absolute_error_k_vs_strict_pred": 4.330000000000041, "score_vs_pred_strict": 3.7727412201812527, "truth_raw_assistant_text": "The value is 288.86", "prediction_raw_text": "The value is 284.53\u0e40\u0e0b\u0e25\u0e25\u0e4c"}
{"id": "line_960", "truth_parsed_k": 288.3, "prediction_parsed_k_with_c_conv": 284.84, "prediction_parsed_k_strict": 284.84, "absolute_error_k_vs_c_conv_pred": 3.4600000000000364, "score_vs_c_conv_pred": 9.623404219809206, "absolute_error_k_vs_strict_pred": 3.4600000000000364, "score_vs_pred_strict": 9.623404219809206, "truth_raw_assistant_text": "The value is 288.30", "prediction_raw_text": "The value is 284.84"}
{"id": "line_961", "truth_parsed_k": 287.93, "prediction_parsed_k_with_c_conv": 284.96, "prediction_parsed_k_strict": 284.96, "absolute_error_k_vs_c_conv_pred": 2.9700000000000273, "score_vs_c_conv_pred": 13.58106453378981, "absolute_error_k_vs_strict_pred": 2.9700000000000273, "score_vs_pred_strict": 13.58106453378981, "truth_raw_assistant_text": "The value is 287.93", "prediction_raw_text": "The value is 284.96"}
{"id": "line_962", "truth_parsed_k": 287.89, "prediction_parsed_k_with_c_conv": 284.72, "prediction_parsed_k_strict": 284.72, "absolute_error_k_vs_c_conv_pred": 3.169999999999959, "score_vs_c_conv_pred": 11.894864274494388, "absolute_error_k_vs_strict_pred": 3.169999999999959, "score_vs_pred_strict": 11.894864274494388, "truth_raw_assistant_text": "The value is 287.89", "prediction_raw_text": "The value is 284.72\u00ea\u0301"}
{"id": "line_963", "truth_parsed_k": 288.54, "prediction_parsed_k_with_c_conv": 284.84, "prediction_parsed_k_strict": 284.84, "absolute_error_k_vs_c_conv_pred": 3.7000000000000455, "score_vs_c_conv_pred": 7.878453749939229, "absolute_error_k_vs_strict_pred": 3.7000000000000455, "score_vs_pred_strict": 7.878453749939229, "truth_raw_assistant_text": "The value is 288.54", "prediction_raw_text": "The value is 284.84"}
{"id": "line_964", "truth_parsed_k": 289.57, "prediction_parsed_k_with_c_conv": 285.92, "prediction_parsed_k_strict": 285.92, "absolute_error_k_vs_c_conv_pred": 3.6499999999999773, "score_vs_c_conv_pred": 8.232777951575255, "absolute_error_k_vs_strict_pred": 3.6499999999999773, "score_vs_pred_strict": 8.232777951575255, "truth_raw_assistant_text": "The value is 289.57", "prediction_raw_text": "The value is 285.92\u0e2a\u0e34\u0e19\u0e04\u0e49\u0e32"}
{"id": "line_965", "truth_parsed_k": 290.29, "prediction_parsed_k_with_c_conv": 285.96, "prediction_parsed_k_strict": 285.96, "absolute_error_k_vs_c_conv_pred": 4.330000000000041, "score_vs_c_conv_pred": 3.7727412201812527, "absolute_error_k_vs_strict_pred": 4.330000000000041, "score_vs_pred_strict": 3.7727412201812527, "truth_raw_assistant_text": "The value is 290.29", "prediction_raw_text": "The value is 285.96\u0e01\u0e47\u0e44\u0e14\u0e49"}
{"id": "line_966", "truth_parsed_k": 290.89, "prediction_parsed_k_with_c_conv": 285.82, "prediction_parsed_k_strict": 285.82, "absolute_error_k_vs_c_conv_pred": 5.069999999999993, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.069999999999993, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.89", "prediction_raw_text": "The value is 285.82\u0e08\u0e33\u0e2b\u0e19\u0e48\u0e32\u0e22"}
{"id": "line_967", "truth_parsed_k": 291.22, "prediction_parsed_k_with_c_conv": 285.76, "prediction_parsed_k_strict": 285.76, "absolute_error_k_vs_c_conv_pred": 5.460000000000036, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.460000000000036, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.22", "prediction_raw_text": "The value is 285.76"}
{"id": "line_968", "truth_parsed_k": 291.08, "prediction_parsed_k_with_c_conv": 284.87, "prediction_parsed_k_strict": 284.87, "absolute_error_k_vs_c_conv_pred": 6.2099999999999795, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.2099999999999795, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.08", "prediction_raw_text": "The value is 284.87\u0e1b\u0e23\u0e31\u0e1a\u0e1b\u0e23"}
{"id": "line_969", "truth_parsed_k": 290.31, "prediction_parsed_k_with_c_conv": 285.32, "prediction_parsed_k_strict": 285.32, "absolute_error_k_vs_c_conv_pred": 4.990000000000009, "score_vs_c_conv_pred": 0.052594256575544396, "absolute_error_k_vs_strict_pred": 4.990000000000009, "score_vs_pred_strict": 0.052594256575544396, "truth_raw_assistant_text": "The value is 290.31", "prediction_raw_text": "The value is 285.32"}
{"id": "line_970", "truth_parsed_k": 289.43, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 4.490000000000009, "score_vs_c_conv_pred": 2.822583086765784, "absolute_error_k_vs_strict_pred": 4.490000000000009, "score_vs_pred_strict": 2.822583086765784, "truth_raw_assistant_text": "The value is 289.43", "prediction_raw_text": "The value is 284.94."}
{"id": "line_971", "truth_parsed_k": 288.65, "prediction_parsed_k_with_c_conv": 284.68, "prediction_parsed_k_strict": 284.68, "absolute_error_k_vs_c_conv_pred": 3.9699999999999704, "score_vs_c_conv_pred": 6.0417248861569295, "absolute_error_k_vs_strict_pred": 3.9699999999999704, "score_vs_pred_strict": 6.0417248861569295, "truth_raw_assistant_text": "The value is 288.65", "prediction_raw_text": "The value is 284.68\u0e40\u0e23\u0e35\u0e22\u0e19\u0e23\u0e39\u0e49"}
{"id": "line_972", "truth_parsed_k": 288.06, "prediction_parsed_k_with_c_conv": 284.77, "prediction_parsed_k_strict": 284.77, "absolute_error_k_vs_c_conv_pred": 3.2900000000000205, "score_vs_c_conv_pred": 10.931605061376516, "absolute_error_k_vs_strict_pred": 3.2900000000000205, "score_vs_pred_strict": 10.931605061376516, "truth_raw_assistant_text": "The value is 288.06", "prediction_raw_text": "The value is 284.77\u0e15\u0e48\u0e32\u0e07\u0e1b\u0e23\u0e30\u0e40\u0e17\u0e28"}
{"id": "line_973", "truth_parsed_k": 287.59, "prediction_parsed_k_with_c_conv": 284.95, "prediction_parsed_k_strict": 284.95, "absolute_error_k_vs_c_conv_pred": 2.6399999999999864, "score_vs_c_conv_pred": 16.617151329389145, "absolute_error_k_vs_strict_pred": 2.6399999999999864, "score_vs_pred_strict": 16.617151329389145, "truth_raw_assistant_text": "The value is 287.59", "prediction_raw_text": "The value is 284.95\u0e23\u0e31\u0e1a\u0e1c\u0e34\u0e14\u0e0a\u0e2d\u0e1a"}
{"id": "line_974", "truth_parsed_k": 287.92, "prediction_parsed_k_with_c_conv": 286.9, "prediction_parsed_k_strict": 286.9, "absolute_error_k_vs_c_conv_pred": 1.0200000000000387, "score_vs_c_conv_pred": 40.358066602658205, "absolute_error_k_vs_strict_pred": 1.0200000000000387, "score_vs_pred_strict": 40.358066602658205, "truth_raw_assistant_text": "The value is 287.92", "prediction_raw_text": "The value is 286.90"}
{"id": "line_975", "truth_parsed_k": 288.36, "prediction_parsed_k_with_c_conv": 285.85, "prediction_parsed_k_strict": 285.85, "absolute_error_k_vs_c_conv_pred": 2.509999999999991, "score_vs_c_conv_pred": 17.91394730146002, "absolute_error_k_vs_strict_pred": 2.509999999999991, "score_vs_pred_strict": 17.91394730146002, "truth_raw_assistant_text": "The value is 288.36", "prediction_raw_text": "The value is 285.85."}
{"id": "line_976", "truth_parsed_k": 289.34, "prediction_parsed_k_with_c_conv": 284.84, "prediction_parsed_k_strict": 284.84, "absolute_error_k_vs_c_conv_pred": 4.5, "score_vs_c_conv_pred": 2.764296806852773, "absolute_error_k_vs_strict_pred": 4.5, "score_vs_pred_strict": 2.764296806852773, "truth_raw_assistant_text": "The value is 289.34", "prediction_raw_text": "The value is 284.84\uf97c"}
{"id": "line_977", "truth_parsed_k": 290.08, "prediction_parsed_k_with_c_conv": 284.87, "prediction_parsed_k_strict": 284.87, "absolute_error_k_vs_c_conv_pred": 5.2099999999999795, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.2099999999999795, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.08", "prediction_raw_text": "The value is 284.87\u0e1b\u0e31\u0e08"}
{"id": "line_978", "truth_parsed_k": 290.81, "prediction_parsed_k_with_c_conv": 284.95, "prediction_parsed_k_strict": 284.95, "absolute_error_k_vs_c_conv_pred": 5.860000000000014, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.860000000000014, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.81", "prediction_raw_text": "The value is 284.95\u0e2a\u0e21\u0e1a\u0e39"}
{"id": "line_979", "truth_parsed_k": 291.17, "prediction_parsed_k_with_c_conv": 284.64, "prediction_parsed_k_strict": 284.64, "absolute_error_k_vs_c_conv_pred": 6.53000000000003, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.53000000000003, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.17", "prediction_raw_text": "The value is 284.64."}
{"id": "line_980", "truth_parsed_k": 291.04, "prediction_parsed_k_with_c_conv": 284.56, "prediction_parsed_k_strict": 284.56, "absolute_error_k_vs_c_conv_pred": 6.480000000000018, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.480000000000018, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.04", "prediction_raw_text": "The value is 284.56."}
{"id": "line_981", "truth_parsed_k": 290.39, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 5.449999999999989, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.449999999999989, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.39", "prediction_raw_text": "The value is 284.94\u0e17\u0e39"}
{"id": "line_982", "truth_parsed_k": 289.48, "prediction_parsed_k_with_c_conv": 284.55, "prediction_parsed_k_strict": 284.55, "absolute_error_k_vs_c_conv_pred": 4.930000000000007, "score_vs_c_conv_pred": 0.3703354157400529, "absolute_error_k_vs_strict_pred": 4.930000000000007, "score_vs_pred_strict": 0.3703354157400529, "truth_raw_assistant_text": "The value is 289.48", "prediction_raw_text": "The value is 284.55\u0e17\u0e31\u0e1e"}
{"id": "line_983", "truth_parsed_k": 288.7, "prediction_parsed_k_with_c_conv": 285.22, "prediction_parsed_k_strict": 285.22, "absolute_error_k_vs_c_conv_pred": 3.4799999999999613, "score_vs_c_conv_pred": 9.473594371356597, "absolute_error_k_vs_strict_pred": 3.4799999999999613, "score_vs_pred_strict": 9.473594371356597, "truth_raw_assistant_text": "The value is 288.70", "prediction_raw_text": "The value is 285.22\u0e40\u0e04\u0e25\u0e47\u0e14"}
{"id": "line_984", "truth_parsed_k": 288.1, "prediction_parsed_k_with_c_conv": 285.55, "prediction_parsed_k_strict": 285.55, "absolute_error_k_vs_c_conv_pred": 2.5500000000000114, "score_vs_c_conv_pred": 17.5082409334664, "absolute_error_k_vs_strict_pred": 2.5500000000000114, "score_vs_pred_strict": 17.5082409334664, "truth_raw_assistant_text": "The value is 288.10", "prediction_raw_text": "The value is 285.55."}
{"id": "line_985", "truth_parsed_k": 287.74, "prediction_parsed_k_with_c_conv": 284.44, "prediction_parsed_k_strict": 284.44, "absolute_error_k_vs_c_conv_pred": 3.3000000000000114, "score_vs_c_conv_pred": 10.852867429820945, "absolute_error_k_vs_strict_pred": 3.3000000000000114, "score_vs_pred_strict": 10.852867429820945, "truth_raw_assistant_text": "The value is 287.74", "prediction_raw_text": "The value is 284.44 \u0e15\u0e38\u0e25\u0e32\u0e04\u0e21"}
{"id": "line_986", "truth_parsed_k": 287.87, "prediction_parsed_k_with_c_conv": 285.66, "prediction_parsed_k_strict": 285.66, "absolute_error_k_vs_c_conv_pred": 2.2099999999999795, "score_vs_c_conv_pred": 21.16879973737197, "absolute_error_k_vs_strict_pred": 2.2099999999999795, "score_vs_pred_strict": 21.16879973737197, "truth_raw_assistant_text": "The value is 287.87", "prediction_raw_text": "The value is 285.66."}
{"id": "line_987", "truth_parsed_k": 288.21, "prediction_parsed_k_with_c_conv": 285.71, "prediction_parsed_k_strict": 285.71, "absolute_error_k_vs_c_conv_pred": 2.5, "score_vs_c_conv_pred": 18.016336211152296, "absolute_error_k_vs_strict_pred": 2.5, "score_vs_pred_strict": 18.016336211152296, "truth_raw_assistant_text": "The value is 288.21", "prediction_raw_text": "The value is 285.71\u0e19\u0e31\u0e48\u0e07"}
{"id": "line_988", "truth_parsed_k": 289.21, "prediction_parsed_k_with_c_conv": 286.41, "prediction_parsed_k_strict": 286.41, "absolute_error_k_vs_c_conv_pred": 2.7999999999999545, "score_vs_c_conv_pred": 15.102333663296697, "absolute_error_k_vs_strict_pred": 2.7999999999999545, "score_vs_pred_strict": 15.102333663296697, "truth_raw_assistant_text": "The value is 289.21", "prediction_raw_text": "The value is 286.41\u0e43\u0e19\u0e02\u0e13\u0e30\u0e17\u0e35\u0e48"}
{"id": "line_989", "truth_parsed_k": 290.17, "prediction_parsed_k_with_c_conv": 284.75, "prediction_parsed_k_strict": 284.75, "absolute_error_k_vs_c_conv_pred": 5.420000000000016, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.420000000000016, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.17", "prediction_raw_text": "The value is 284.75\uf99c"}
{"id": "line_990", "truth_parsed_k": 290.86, "prediction_parsed_k_with_c_conv": 284.95, "prediction_parsed_k_strict": 284.95, "absolute_error_k_vs_c_conv_pred": 5.910000000000025, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.910000000000025, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.86", "prediction_raw_text": "The value is 284.95\u0e23\u0e16\u0e22\u0e19\u0e15\u0e4c"}
{"id": "line_991", "truth_parsed_k": 291.18, "prediction_parsed_k_with_c_conv": 285.79, "prediction_parsed_k_strict": 285.79, "absolute_error_k_vs_c_conv_pred": 5.389999999999986, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.389999999999986, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.18", "prediction_raw_text": "The value is 285.79\u0e2b\u0e19\u0e36\u0e48\u0e07"}
{"id": "line_992", "truth_parsed_k": 291.06, "prediction_parsed_k_with_c_conv": 284.62, "prediction_parsed_k_strict": 284.62, "absolute_error_k_vs_c_conv_pred": 6.439999999999998, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.439999999999998, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.06", "prediction_raw_text": "The value is 284.62\u0e40\u0e21\u0e37\u0e48\u0e2d\u0e27\u0e31\u0e19\u0e17\u0e35\u0e48"}
{"id": "line_993", "truth_parsed_k": 290.54, "prediction_parsed_k_with_c_conv": 285.3, "prediction_parsed_k_strict": 285.3, "absolute_error_k_vs_c_conv_pred": 5.240000000000009, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.240000000000009, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.54", "prediction_raw_text": "The value is 285.30\u0e25\u0e35\u0e01"}
{"id": "line_994", "truth_parsed_k": 289.58, "prediction_parsed_k_with_c_conv": 285.59, "prediction_parsed_k_strict": 285.59, "absolute_error_k_vs_c_conv_pred": 3.990000000000009, "score_vs_c_conv_pred": 5.910527302361723, "absolute_error_k_vs_strict_pred": 3.990000000000009, "score_vs_pred_strict": 5.910527302361723, "truth_raw_assistant_text": "The value is 289.58", "prediction_raw_text": "The value is 285.59."}
{"id": "line_995", "truth_parsed_k": 288.83, "prediction_parsed_k_with_c_conv": 285.43, "prediction_parsed_k_strict": 285.43, "absolute_error_k_vs_c_conv_pred": 3.3999999999999773, "score_vs_c_conv_pred": 10.07790025446802, "absolute_error_k_vs_strict_pred": 3.3999999999999773, "score_vs_pred_strict": 10.07790025446802, "truth_raw_assistant_text": "The value is 288.83", "prediction_raw_text": "The value is 285.43"}
{"id": "line_996", "truth_parsed_k": 288.26, "prediction_parsed_k_with_c_conv": 285.85, "prediction_parsed_k_strict": 285.85, "absolute_error_k_vs_c_conv_pred": 2.409999999999968, "score_vs_c_conv_pred": 18.955789071465688, "absolute_error_k_vs_strict_pred": 2.409999999999968, "score_vs_pred_strict": 18.955789071465688, "truth_raw_assistant_text": "The value is 288.26", "prediction_raw_text": "The value is 285.85."}
{"id": "line_997", "truth_parsed_k": 287.78, "prediction_parsed_k_with_c_conv": 284.91, "prediction_parsed_k_strict": 284.91, "absolute_error_k_vs_c_conv_pred": 2.8699999999999477, "score_vs_c_conv_pred": 14.465487239993568, "absolute_error_k_vs_strict_pred": 2.8699999999999477, "score_vs_pred_strict": 14.465487239993568, "truth_raw_assistant_text": "The value is 287.78", "prediction_raw_text": "The value is 284.91."}
{"id": "line_998", "truth_parsed_k": 287.85, "prediction_parsed_k_with_c_conv": 284.6, "prediction_parsed_k_strict": 284.6, "absolute_error_k_vs_c_conv_pred": 3.25, "score_vs_c_conv_pred": 11.248878255127204, "absolute_error_k_vs_strict_pred": 3.25, "score_vs_pred_strict": 11.248878255127204, "truth_raw_assistant_text": "The value is 287.85", "prediction_raw_text": "The value is 284.60"}
{"id": "line_999", "truth_parsed_k": 288.53, "prediction_parsed_k_with_c_conv": 285.97, "prediction_parsed_k_strict": 285.97, "absolute_error_k_vs_c_conv_pred": 2.5599999999999454, "score_vs_c_conv_pred": 17.40776226910201, "absolute_error_k_vs_strict_pred": 2.5599999999999454, "score_vs_pred_strict": 17.40776226910201, "truth_raw_assistant_text": "The value is 288.53", "prediction_raw_text": "The value is 285.97\u0e1a\u0e23\u0e34\u0e40\u0e27\u0e13"}
{"id": "line_1000", "truth_parsed_k": 289.55, "prediction_parsed_k_with_c_conv": 285.36, "prediction_parsed_k_strict": 285.36, "absolute_error_k_vs_c_conv_pred": 4.189999999999998, "score_vs_c_conv_pred": 4.632553169804399, "absolute_error_k_vs_strict_pred": 4.189999999999998, "score_vs_pred_strict": 4.632553169804399, "truth_raw_assistant_text": "The value is 289.55", "prediction_raw_text": "The value is 285.36."}
{"id": "line_1001", "truth_parsed_k": 290.48, "prediction_parsed_k_with_c_conv": 285.72, "prediction_parsed_k_strict": 285.72, "absolute_error_k_vs_c_conv_pred": 4.759999999999991, "score_vs_c_conv_pred": 1.2915122870442164, "absolute_error_k_vs_strict_pred": 4.759999999999991, "score_vs_pred_strict": 1.2915122870442164, "truth_raw_assistant_text": "The value is 290.48", "prediction_raw_text": "The value is 285.72\u0e21\u0e32\u0e01\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14"}
{"id": "line_1002", "truth_parsed_k": 291.13, "prediction_parsed_k_with_c_conv": 284.65, "prediction_parsed_k_strict": 284.65, "absolute_error_k_vs_c_conv_pred": 6.480000000000018, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.480000000000018, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.13", "prediction_raw_text": "The value is 284.65."}
{"id": "line_1003", "truth_parsed_k": 291.52, "prediction_parsed_k_with_c_conv": 284.87, "prediction_parsed_k_strict": 284.87, "absolute_error_k_vs_c_conv_pred": 6.649999999999977, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.649999999999977, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.52", "prediction_raw_text": "The value is 284.87"}
{"id": "line_1004", "truth_parsed_k": 291.32, "prediction_parsed_k_with_c_conv": 284.36, "prediction_parsed_k_strict": 284.36, "absolute_error_k_vs_c_conv_pred": 6.9599999999999795, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.9599999999999795, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.32", "prediction_raw_text": "The value is 284.36\u0e22\u0e34\u0e07"}
{"id": "line_1005", "truth_parsed_k": 290.68, "prediction_parsed_k_with_c_conv": 284.74, "prediction_parsed_k_strict": 284.74, "absolute_error_k_vs_c_conv_pred": 5.939999999999998, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.939999999999998, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.68", "prediction_raw_text": "The value is 284.74."}
{"id": "line_1006", "truth_parsed_k": 289.89, "prediction_parsed_k_with_c_conv": 284.99, "prediction_parsed_k_strict": 284.99, "absolute_error_k_vs_c_conv_pred": 4.899999999999977, "score_vs_c_conv_pred": 0.5306230961831027, "absolute_error_k_vs_strict_pred": 4.899999999999977, "score_vs_pred_strict": 0.5306230961831027, "truth_raw_assistant_text": "The value is 289.89", "prediction_raw_text": "The value is 284.99\u0e40\u0e27\u0e47\u0e1a\u0e44\u0e0b\u0e15\u0e4c"}
{"id": "line_1007", "truth_parsed_k": 289.15, "prediction_parsed_k_with_c_conv": 284.98, "prediction_parsed_k_strict": 284.98, "absolute_error_k_vs_c_conv_pred": 4.169999999999959, "score_vs_c_conv_pred": 4.757655605295974, "absolute_error_k_vs_strict_pred": 4.169999999999959, "score_vs_pred_strict": 4.757655605295974, "truth_raw_assistant_text": "The value is 289.15", "prediction_raw_text": "The value is 284.98\u0e01\u0e34\u0e08\u0e01\u0e23\u0e23\u0e21"}
{"id": "line_1008", "truth_parsed_k": 288.42, "prediction_parsed_k_with_c_conv": 285.55, "prediction_parsed_k_strict": 285.55, "absolute_error_k_vs_c_conv_pred": 2.8700000000000045, "score_vs_c_conv_pred": 14.465487239993047, "absolute_error_k_vs_strict_pred": 2.8700000000000045, "score_vs_pred_strict": 14.465487239993047, "truth_raw_assistant_text": "The value is 288.42", "prediction_raw_text": "The value is 285.55\u0e0a\u0e48\u0e2d\u0e07\u0e17\u0e32\u0e07"}
{"id": "line_1009", "truth_parsed_k": 288.04, "prediction_parsed_k_with_c_conv": 285.54, "prediction_parsed_k_strict": 285.54, "absolute_error_k_vs_c_conv_pred": 2.5, "score_vs_c_conv_pred": 18.016336211152296, "absolute_error_k_vs_strict_pred": 2.5, "score_vs_pred_strict": 18.016336211152296, "truth_raw_assistant_text": "The value is 288.04", "prediction_raw_text": "The value is 285.54\u0e1b\u0e23\u0e2a\u0e34"}
{"id": "line_1010", "truth_parsed_k": 288.33, "prediction_parsed_k_with_c_conv": 284.79, "prediction_parsed_k_strict": 284.79, "absolute_error_k_vs_c_conv_pred": 3.5399999999999636, "score_vs_c_conv_pred": 9.02910145980924, "absolute_error_k_vs_strict_pred": 3.5399999999999636, "score_vs_pred_strict": 9.02910145980924, "truth_raw_assistant_text": "The value is 288.33", "prediction_raw_text": "The value is 284.79\ufa4c"}
{"id": "line_1011", "truth_parsed_k": 288.7, "prediction_parsed_k_with_c_conv": 284.86, "prediction_parsed_k_strict": 284.86, "absolute_error_k_vs_c_conv_pred": 3.839999999999975, "score_vs_c_conv_pred": 6.91045347520115, "absolute_error_k_vs_strict_pred": 3.839999999999975, "score_vs_pred_strict": 6.91045347520115, "truth_raw_assistant_text": "The value is 288.70", "prediction_raw_text": "The value is 284.86\u0e41\u0e15\u0e01\u0e15\u0e48\u0e32\u0e07\u0e01\u0e31\u0e19"}
{"id": "line_1012", "truth_parsed_k": 289.58, "prediction_parsed_k_with_c_conv": 286.95, "prediction_parsed_k_strict": 286.95, "absolute_error_k_vs_c_conv_pred": 2.6299999999999955, "score_vs_c_conv_pred": 16.714717562232785, "absolute_error_k_vs_strict_pred": 2.6299999999999955, "score_vs_pred_strict": 16.714717562232785, "truth_raw_assistant_text": "The value is 289.58", "prediction_raw_text": "The value is 286.95\u0e25\u0e49\u0e32\u0e19"}
{"id": "line_1013", "truth_parsed_k": 290.49, "prediction_parsed_k_with_c_conv": 285.95, "prediction_parsed_k_strict": 285.95, "absolute_error_k_vs_c_conv_pred": 4.5400000000000205, "score_vs_c_conv_pred": 2.532405162359508, "absolute_error_k_vs_strict_pred": 4.5400000000000205, "score_vs_pred_strict": 2.532405162359508, "truth_raw_assistant_text": "The value is 290.49", "prediction_raw_text": "The value is 285.95\u0e19\u0e49\u0e2d\u0e22"}
{"id": "line_1014", "truth_parsed_k": 291.12, "prediction_parsed_k_with_c_conv": 284.88, "prediction_parsed_k_strict": 284.88, "absolute_error_k_vs_c_conv_pred": 6.240000000000009, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.240000000000009, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.12", "prediction_raw_text": "The value is 284.88\u0e22\u0e48"}
{"id": "line_1015", "truth_parsed_k": 291.46, "prediction_parsed_k_with_c_conv": 284.86, "prediction_parsed_k_strict": 284.86, "absolute_error_k_vs_c_conv_pred": 6.599999999999966, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.599999999999966, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.46", "prediction_raw_text": "The value is 284.86"}
{"id": "line_1016", "truth_parsed_k": 291.34, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 6.399999999999977, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.399999999999977, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.34", "prediction_raw_text": "The value is 284.94\u0e1b\u0e23\u0e30\u0e27\u0e31\u0e15\u0e34\u0e28\u0e32\u0e2a\u0e15\u0e23\u0e4c"}
{"id": "line_1017", "truth_parsed_k": 290.63, "prediction_parsed_k_with_c_conv": 284.76, "prediction_parsed_k_strict": 284.76, "absolute_error_k_vs_c_conv_pred": 5.8700000000000045, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.8700000000000045, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.63", "prediction_raw_text": "The value is 284.76\u0e1a\u0e32\u0e04\u0e32\u0e23\u0e48\u0e32"}
{"id": "line_1018", "truth_parsed_k": 289.82, "prediction_parsed_k_with_c_conv": 284.87, "prediction_parsed_k_strict": 284.87, "absolute_error_k_vs_c_conv_pred": 4.949999999999989, "score_vs_c_conv_pred": 0.26400457350821416, "absolute_error_k_vs_strict_pred": 4.949999999999989, "score_vs_pred_strict": 0.26400457350821416, "truth_raw_assistant_text": "The value is 289.82", "prediction_raw_text": "The value is 284.87."}
{"id": "line_1019", "truth_parsed_k": 289.1, "prediction_parsed_k_with_c_conv": 284.77, "prediction_parsed_k_strict": 284.77, "absolute_error_k_vs_c_conv_pred": 4.330000000000041, "score_vs_c_conv_pred": 3.7727412201812527, "absolute_error_k_vs_strict_pred": 4.330000000000041, "score_vs_pred_strict": 3.7727412201812527, "truth_raw_assistant_text": "The value is 289.10", "prediction_raw_text": "The value is 284.77\u0e23\u0e39\u0e49\u0e08"}
{"id": "line_1020", "truth_parsed_k": 288.41, "prediction_parsed_k_with_c_conv": 285.66, "prediction_parsed_k_strict": 285.66, "absolute_error_k_vs_c_conv_pred": 2.75, "score_vs_c_conv_pred": 15.56662535132074, "absolute_error_k_vs_strict_pred": 2.75, "score_vs_pred_strict": 15.56662535132074, "truth_raw_assistant_text": "The value is 288.41", "prediction_raw_text": "The value is 285.66."}
{"id": "line_1021", "truth_parsed_k": 288.13, "prediction_parsed_k_with_c_conv": 285.64, "prediction_parsed_k_strict": 285.64, "absolute_error_k_vs_c_conv_pred": 2.490000000000009, "score_vs_c_conv_pred": 18.119115919156847, "absolute_error_k_vs_strict_pred": 2.490000000000009, "score_vs_pred_strict": 18.119115919156847, "truth_raw_assistant_text": "The value is 288.13", "prediction_raw_text": "The value is 285.64."}
{"id": "line_1022", "truth_parsed_k": 288.42, "prediction_parsed_k_with_c_conv": 284.98, "prediction_parsed_k_strict": 284.98, "absolute_error_k_vs_c_conv_pred": 3.4399999999999977, "score_vs_c_conv_pred": 9.774052169676517, "absolute_error_k_vs_strict_pred": 3.4399999999999977, "score_vs_pred_strict": 9.774052169676517, "truth_raw_assistant_text": "The value is 288.42", "prediction_raw_text": "The value is 284.98\u0e01\u0e23\u0e30\u0e40\u0e1b\u0e4b"}
{"id": "line_1023", "truth_parsed_k": 288.72, "prediction_parsed_k_with_c_conv": 284.6, "prediction_parsed_k_strict": 284.6, "absolute_error_k_vs_c_conv_pred": 4.1200000000000045, "score_vs_c_conv_pred": 5.072978304372855, "absolute_error_k_vs_strict_pred": 4.1200000000000045, "score_vs_pred_strict": 5.072978304372855, "truth_raw_assistant_text": "The value is 288.72", "prediction_raw_text": "The value is 284.60\u0e01\u0e33\u0e01\u0e31\u0e1a"}
{"id": "line_1024", "truth_parsed_k": 289.63, "prediction_parsed_k_with_c_conv": 284.81, "prediction_parsed_k_strict": 284.81, "absolute_error_k_vs_c_conv_pred": 4.819999999999993, "score_vs_c_conv_pred": 0.962780829551213, "absolute_error_k_vs_strict_pred": 4.819999999999993, "score_vs_pred_strict": 0.962780829551213, "truth_raw_assistant_text": "The value is 289.63", "prediction_raw_text": "The value is 284.81);"}
{"id": "line_1025", "truth_parsed_k": 290.49, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 5.670000000000016, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.670000000000016, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.49", "prediction_raw_text": "The value is 284.82\ufa68"}
{"id": "line_1026", "truth_parsed_k": 291.27, "prediction_parsed_k_with_c_conv": 284.88, "prediction_parsed_k_strict": 284.88, "absolute_error_k_vs_c_conv_pred": 6.389999999999986, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.389999999999986, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.27", "prediction_raw_text": "The value is 284.88."}
{"id": "line_1027", "truth_parsed_k": 291.59, "prediction_parsed_k_with_c_conv": 285.82, "prediction_parsed_k_strict": 285.82, "absolute_error_k_vs_c_conv_pred": 5.769999999999982, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.769999999999982, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.59", "prediction_raw_text": "The value is 285.82\u0e42\u0e0a\u0e27\u0e4c"}
{"id": "line_1028", "truth_parsed_k": 291.34, "prediction_parsed_k_with_c_conv": 284.86, "prediction_parsed_k_strict": 284.86, "absolute_error_k_vs_c_conv_pred": 6.479999999999961, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.479999999999961, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.34", "prediction_raw_text": "The value is 284.86\u0e0b\u0e48"}
{"id": "line_1029", "truth_parsed_k": 290.79, "prediction_parsed_k_with_c_conv": 285.62, "prediction_parsed_k_strict": 285.62, "absolute_error_k_vs_c_conv_pred": 5.170000000000016, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.170000000000016, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.79", "prediction_raw_text": "The value is 285.62"}
{"id": "line_1030", "truth_parsed_k": 289.85, "prediction_parsed_k_with_c_conv": 284.98, "prediction_parsed_k_strict": 284.98, "absolute_error_k_vs_c_conv_pred": 4.8700000000000045, "score_vs_c_conv_pred": 0.6918705888144028, "absolute_error_k_vs_strict_pred": 4.8700000000000045, "score_vs_pred_strict": 0.6918705888144028, "truth_raw_assistant_text": "The value is 289.85", "prediction_raw_text": "The value is 284.98"}
{"id": "line_1031", "truth_parsed_k": 289.07, "prediction_parsed_k_with_c_conv": 284.67, "prediction_parsed_k_strict": 284.67, "absolute_error_k_vs_c_conv_pred": 4.399999999999977, "score_vs_c_conv_pred": 3.352915888962116, "absolute_error_k_vs_strict_pred": 4.399999999999977, "score_vs_pred_strict": 3.352915888962116, "truth_raw_assistant_text": "The value is 289.07", "prediction_raw_text": "The value is 284.67."}
{"id": "line_1032", "truth_parsed_k": 288.62, "prediction_parsed_k_with_c_conv": 286.77, "prediction_parsed_k_strict": 286.77, "absolute_error_k_vs_c_conv_pred": 1.8500000000000227, "score_vs_c_conv_pred": 25.677760106528247, "absolute_error_k_vs_strict_pred": 1.8500000000000227, "score_vs_pred_strict": 25.677760106528247, "truth_raw_assistant_text": "The value is 288.62", "prediction_raw_text": "The value is 286.77\u0e40\u0e17\u0e48\u0e32\u0e44\u0e2b\u0e23\u0e48"}
{"id": "line_1033", "truth_parsed_k": 288.3, "prediction_parsed_k_with_c_conv": 285.53, "prediction_parsed_k_strict": 285.53, "absolute_error_k_vs_c_conv_pred": 2.7700000000000387, "score_vs_c_conv_pred": 15.379946955126034, "absolute_error_k_vs_strict_pred": 2.7700000000000387, "score_vs_pred_strict": 15.379946955126034, "truth_raw_assistant_text": "The value is 288.30", "prediction_raw_text": "The value is 285.53\u0644\u0651\u064e"}
{"id": "line_1034", "truth_parsed_k": 288.3, "prediction_parsed_k_with_c_conv": 285.89, "prediction_parsed_k_strict": 285.89, "absolute_error_k_vs_c_conv_pred": 2.410000000000025, "score_vs_c_conv_pred": 18.9557890714651, "absolute_error_k_vs_strict_pred": 2.410000000000025, "score_vs_pred_strict": 18.9557890714651, "truth_raw_assistant_text": "The value is 288.30", "prediction_raw_text": "The value is 285.89."}
{"id": "line_1035", "truth_parsed_k": 288.84, "prediction_parsed_k_with_c_conv": 285.95, "prediction_parsed_k_strict": 285.95, "absolute_error_k_vs_c_conv_pred": 2.8899999999999864, "score_vs_c_conv_pred": 14.28626361130939, "absolute_error_k_vs_strict_pred": 2.8899999999999864, "score_vs_pred_strict": 14.28626361130939, "truth_raw_assistant_text": "The value is 288.84", "prediction_raw_text": "The value is 285.95\u0e40\u0e1e\u0e34\u0e48\u0e07"}
{"id": "line_1036", "truth_parsed_k": 289.61, "prediction_parsed_k_with_c_conv": 285.97, "prediction_parsed_k_strict": 285.97, "absolute_error_k_vs_c_conv_pred": 3.6399999999999864, "score_vs_c_conv_pred": 8.30420571442686, "absolute_error_k_vs_strict_pred": 3.6399999999999864, "score_vs_pred_strict": 8.30420571442686, "truth_raw_assistant_text": "The value is 289.61", "prediction_raw_text": "The value is 285.97\u0e23\u0e31\u0e10"}
{"id": "line_1037", "truth_parsed_k": 290.56, "prediction_parsed_k_with_c_conv": 285.78, "prediction_parsed_k_strict": 285.78, "absolute_error_k_vs_c_conv_pred": 4.78000000000003, "score_vs_c_conv_pred": 1.18148863732912, "absolute_error_k_vs_strict_pred": 4.78000000000003, "score_vs_pred_strict": 1.18148863732912, "truth_raw_assistant_text": "The value is 290.56", "prediction_raw_text": "The value is 285.78\u0e2d\u0e48\u0e32\u0e19"}
{"id": "line_1038", "truth_parsed_k": 291.3, "prediction_parsed_k_with_c_conv": 285.72, "prediction_parsed_k_strict": 285.72, "absolute_error_k_vs_c_conv_pred": 5.579999999999984, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.579999999999984, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.30", "prediction_raw_text": "The value is 285.72\u0e08\u0e34"}
{"id": "line_1039", "truth_parsed_k": 291.51, "prediction_parsed_k_with_c_conv": 284.81, "prediction_parsed_k_strict": 284.81, "absolute_error_k_vs_c_conv_pred": 6.699999999999989, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.699999999999989, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.51", "prediction_raw_text": "The value is 284.81"}
{"id": "line_1040", "truth_parsed_k": 291.31, "prediction_parsed_k_with_c_conv": 285.88, "prediction_parsed_k_strict": 285.88, "absolute_error_k_vs_c_conv_pred": 5.430000000000007, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.430000000000007, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.31", "prediction_raw_text": "The value is 285.88\u0e40\u0e14\u0e35\u0e22\u0e27\u0e01\u0e31\u0e19"}
{"id": "line_1041", "truth_parsed_k": 290.65, "prediction_parsed_k_with_c_conv": 285.57, "prediction_parsed_k_strict": 285.57, "absolute_error_k_vs_c_conv_pred": 5.079999999999984, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.079999999999984, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.65", "prediction_raw_text": "The value is 285.57\u2329"}
{"id": "line_1042", "truth_parsed_k": 289.69, "prediction_parsed_k_with_c_conv": 284.95, "prediction_parsed_k_strict": 284.95, "absolute_error_k_vs_c_conv_pred": 4.740000000000009, "score_vs_c_conv_pred": 1.4019873170983632, "absolute_error_k_vs_strict_pred": 4.740000000000009, "score_vs_pred_strict": 1.4019873170983632, "truth_raw_assistant_text": "The value is 289.69", "prediction_raw_text": "The value is 284.95\u0e2a\u0e21\u0e40\u0e14\u0e47"}
{"id": "line_1043", "truth_parsed_k": 289.05, "prediction_parsed_k_with_c_conv": 285.94, "prediction_parsed_k_strict": 285.94, "absolute_error_k_vs_c_conv_pred": 3.1100000000000136, "score_vs_c_conv_pred": 12.389731202387344, "absolute_error_k_vs_strict_pred": 3.1100000000000136, "score_vs_pred_strict": 12.389731202387344, "truth_raw_assistant_text": "The value is 289.05", "prediction_raw_text": "The value is 285.94\u0e14\u0e49\u0e32\u0e19\u0e25"}
{"id": "line_1044", "truth_parsed_k": 288.37, "prediction_parsed_k_with_c_conv": 285.86, "prediction_parsed_k_strict": 285.86, "absolute_error_k_vs_c_conv_pred": 2.509999999999991, "score_vs_c_conv_pred": 17.91394730146002, "absolute_error_k_vs_strict_pred": 2.509999999999991, "score_vs_pred_strict": 17.91394730146002, "truth_raw_assistant_text": "The value is 288.37", "prediction_raw_text": "The value is 285.86"}
{"id": "line_1045", "truth_parsed_k": 288.26, "prediction_parsed_k_with_c_conv": 285.05, "prediction_parsed_k_strict": 285.05, "absolute_error_k_vs_c_conv_pred": 3.2099999999999795, "score_vs_c_conv_pred": 11.569934228803058, "absolute_error_k_vs_strict_pred": 3.2099999999999795, "score_vs_pred_strict": 11.569934228803058, "truth_raw_assistant_text": "The value is 288.26", "prediction_raw_text": "The value is 285.05\u0e08\u0e31\u0e19\u0e17\u0e23\u0e4c"}
{"id": "line_1046", "truth_parsed_k": 288.3, "prediction_parsed_k_with_c_conv": 284.78, "prediction_parsed_k_strict": 284.78, "absolute_error_k_vs_c_conv_pred": 3.5200000000000387, "score_vs_c_conv_pred": 9.176452037088579, "absolute_error_k_vs_strict_pred": 3.5200000000000387, "score_vs_pred_strict": 9.176452037088579, "truth_raw_assistant_text": "The value is 288.30", "prediction_raw_text": "The value is 284.78_ComCallableWrapper"}
{"id": "line_1047", "truth_parsed_k": 288.64, "prediction_parsed_k_with_c_conv": 285.62, "prediction_parsed_k_strict": 285.62, "absolute_error_k_vs_c_conv_pred": 3.019999999999982, "score_vs_c_conv_pred": 13.149512816791553, "absolute_error_k_vs_strict_pred": 3.019999999999982, "score_vs_pred_strict": 13.149512816791553, "truth_raw_assistant_text": "The value is 288.64", "prediction_raw_text": "The value is 285.62\u0e1a\u0e31\u0e0d"}
{"id": "line_1048", "truth_parsed_k": 289.49, "prediction_parsed_k_with_c_conv": 286.9, "prediction_parsed_k_strict": 286.9, "absolute_error_k_vs_c_conv_pred": 2.590000000000032, "score_vs_c_conv_pred": 17.10855640408796, "absolute_error_k_vs_strict_pred": 2.590000000000032, "score_vs_pred_strict": 17.10855640408796, "truth_raw_assistant_text": "The value is 289.49", "prediction_raw_text": "The value is 286.90\uf9ac"}
{"id": "line_1049", "truth_parsed_k": 290.33, "prediction_parsed_k_with_c_conv": 284.66, "prediction_parsed_k_strict": 284.66, "absolute_error_k_vs_c_conv_pred": 5.669999999999959, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.669999999999959, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.33", "prediction_raw_text": "The value is 284.66\u0e44\u0e21\u0e48\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16"}
{"id": "line_1050", "truth_parsed_k": 291.01, "prediction_parsed_k_with_c_conv": 284.91, "prediction_parsed_k_strict": 284.91, "absolute_error_k_vs_c_conv_pred": 6.099999999999966, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.099999999999966, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.01", "prediction_raw_text": "The value is 284.91."}
{"id": "line_1051", "truth_parsed_k": 291.48, "prediction_parsed_k_with_c_conv": 286.78, "prediction_parsed_k_strict": 286.78, "absolute_error_k_vs_c_conv_pred": 4.7000000000000455, "score_vs_c_conv_pred": 1.6243064399998453, "absolute_error_k_vs_strict_pred": 4.7000000000000455, "score_vs_pred_strict": 1.6243064399998453, "truth_raw_assistant_text": "The value is 291.48", "prediction_raw_text": "The value is 286.78\u0e1a\u0e23\u0e34\u0e29"}
{"id": "line_1052", "truth_parsed_k": 291.2, "prediction_parsed_k_with_c_conv": 285.38, "prediction_parsed_k_strict": 285.38, "absolute_error_k_vs_c_conv_pred": 5.819999999999993, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.819999999999993, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.20", "prediction_raw_text": "The value is 285.38\ufa09"}
{"id": "line_1053", "truth_parsed_k": 290.6, "prediction_parsed_k_with_c_conv": 285.92, "prediction_parsed_k_strict": 285.92, "absolute_error_k_vs_c_conv_pred": 4.680000000000007, "score_vs_c_conv_pred": 1.7361581098770285, "absolute_error_k_vs_strict_pred": 4.680000000000007, "score_vs_pred_strict": 1.7361581098770285, "truth_raw_assistant_text": "The value is 290.60", "prediction_raw_text": "The value is 285.92\u0e20\u0e32\u0e22\u0e43\u0e15\u0e49"}
{"id": "line_1054", "truth_parsed_k": 289.78, "prediction_parsed_k_with_c_conv": 284.79, "prediction_parsed_k_strict": 284.79, "absolute_error_k_vs_c_conv_pred": 4.989999999999952, "score_vs_c_conv_pred": 0.052594256575844156, "absolute_error_k_vs_strict_pred": 4.989999999999952, "score_vs_pred_strict": 0.052594256575844156, "truth_raw_assistant_text": "The value is 289.78", "prediction_raw_text": "The value is 284.79\u0e40\u0e27\u0e34"}
{"id": "line_1055", "truth_parsed_k": 288.99, "prediction_parsed_k_with_c_conv": 285.25, "prediction_parsed_k_strict": 285.25, "absolute_error_k_vs_c_conv_pred": 3.740000000000009, "score_vs_c_conv_pred": 7.598313373665089, "absolute_error_k_vs_strict_pred": 3.740000000000009, "score_vs_pred_strict": 7.598313373665089, "truth_raw_assistant_text": "The value is 288.99", "prediction_raw_text": "The value is 285.25\u0642\u064f"}
{"id": "line_1056", "truth_parsed_k": 288.4, "prediction_parsed_k_with_c_conv": 285.02, "prediction_parsed_k_strict": 285.02, "absolute_error_k_vs_c_conv_pred": 3.3799999999999955, "score_vs_c_conv_pred": 10.231119733596717, "absolute_error_k_vs_strict_pred": 3.3799999999999955, "score_vs_pred_strict": 10.231119733596717, "truth_raw_assistant_text": "The value is 288.40", "prediction_raw_text": "The value is 285.02"}
{"id": "line_1057", "truth_parsed_k": 288.02, "prediction_parsed_k_with_c_conv": 286.96, "prediction_parsed_k_strict": 286.96, "absolute_error_k_vs_c_conv_pred": 1.0600000000000023, "score_vs_c_conv_pred": 39.433400621654805, "absolute_error_k_vs_strict_pred": 1.0600000000000023, "score_vs_pred_strict": 39.433400621654805, "truth_raw_assistant_text": "The value is 288.02", "prediction_raw_text": "The value is 286.96\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49"}
{"id": "line_1058", "truth_parsed_k": 288.07, "prediction_parsed_k_with_c_conv": 284.87, "prediction_parsed_k_strict": 284.87, "absolute_error_k_vs_c_conv_pred": 3.1999999999999886, "score_vs_c_conv_pred": 11.65079990761112, "absolute_error_k_vs_strict_pred": 3.1999999999999886, "score_vs_pred_strict": 11.65079990761112, "truth_raw_assistant_text": "The value is 288.07", "prediction_raw_text": "The value is 284.87\u0e44\u0e1b\u0e22\u0e31\u0e07"}
{"id": "line_1059", "truth_parsed_k": 288.69, "prediction_parsed_k_with_c_conv": 284.8, "prediction_parsed_k_strict": 284.8, "absolute_error_k_vs_c_conv_pred": 3.8899999999999864, "score_vs_c_conv_pred": 6.573002307246901, "absolute_error_k_vs_strict_pred": 3.8899999999999864, "score_vs_pred_strict": 6.573002307246901, "truth_raw_assistant_text": "The value is 288.69", "prediction_raw_text": "The value is 284.80\u0e1d\u0e23\u0e31\u0e48\u0e07\u0e40\u0e28"}
{"id": "line_1060", "truth_parsed_k": 289.62, "prediction_parsed_k_with_c_conv": 285.57, "prediction_parsed_k_strict": 285.57, "absolute_error_k_vs_c_conv_pred": 4.050000000000011, "score_vs_c_conv_pred": 5.520726756855332, "absolute_error_k_vs_strict_pred": 4.050000000000011, "score_vs_pred_strict": 5.520726756855332, "truth_raw_assistant_text": "The value is 289.62", "prediction_raw_text": "The value is 285.57."}
{"id": "line_1061", "truth_parsed_k": 290.33, "prediction_parsed_k_with_c_conv": 286.54, "prediction_parsed_k_strict": 286.54, "absolute_error_k_vs_c_conv_pred": 3.7899999999999636, "score_vs_c_conv_pred": 7.252187132609622, "absolute_error_k_vs_strict_pred": 3.7899999999999636, "score_vs_pred_strict": 7.252187132609622, "truth_raw_assistant_text": "The value is 290.33", "prediction_raw_text": "The value is 286.54\u0e42\u0e15\u0e49"}
{"id": "line_1062", "truth_parsed_k": 291.04, "prediction_parsed_k_with_c_conv": 285.6, "prediction_parsed_k_strict": 285.6, "absolute_error_k_vs_c_conv_pred": 5.439999999999998, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.439999999999998, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.04", "prediction_raw_text": "The value is 285.60\uf9d1"}
{"id": "line_1063", "truth_parsed_k": 291.36, "prediction_parsed_k_with_c_conv": 285.95, "prediction_parsed_k_strict": 285.95, "absolute_error_k_vs_c_conv_pred": 5.410000000000025, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.410000000000025, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.36", "prediction_raw_text": "The value is 285.95"}
{"id": "line_1064", "truth_parsed_k": 291.34, "prediction_parsed_k_with_c_conv": 285.98, "prediction_parsed_k_strict": 285.98, "absolute_error_k_vs_c_conv_pred": 5.359999999999957, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.359999999999957, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.34", "prediction_raw_text": "The value is 285.98\u0e40\u0e1e\u0e37\u0e48\u0e2d"}
{"id": "line_1065", "truth_parsed_k": 290.53, "prediction_parsed_k_with_c_conv": 286.36, "prediction_parsed_k_strict": 286.36, "absolute_error_k_vs_c_conv_pred": 4.169999999999959, "score_vs_c_conv_pred": 4.757655605295974, "absolute_error_k_vs_strict_pred": 4.169999999999959, "score_vs_pred_strict": 4.757655605295974, "truth_raw_assistant_text": "The value is 290.53", "prediction_raw_text": "The value is 286.36"}
{"id": "line_1066", "truth_parsed_k": 289.82, "prediction_parsed_k_with_c_conv": 285.91, "prediction_parsed_k_strict": 285.91, "absolute_error_k_vs_c_conv_pred": 3.909999999999968, "score_vs_c_conv_pred": 6.439197058986501, "absolute_error_k_vs_strict_pred": 3.909999999999968, "score_vs_pred_strict": 6.439197058986501, "truth_raw_assistant_text": "The value is 289.82", "prediction_raw_text": "The value is 285.91"}
{"id": "line_1067", "truth_parsed_k": 289.06, "prediction_parsed_k_with_c_conv": 285.54, "prediction_parsed_k_strict": 285.54, "absolute_error_k_vs_c_conv_pred": 3.519999999999982, "score_vs_c_conv_pred": 9.176452037089, "absolute_error_k_vs_strict_pred": 3.519999999999982, "score_vs_pred_strict": 9.176452037089, "truth_raw_assistant_text": "The value is 289.06", "prediction_raw_text": "The value is 285.54\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e02\u0e36\u0e49\u0e19"}
{"id": "line_1068", "truth_parsed_k": 288.45, "prediction_parsed_k_with_c_conv": 285.9, "prediction_parsed_k_strict": 285.9, "absolute_error_k_vs_c_conv_pred": 2.5500000000000114, "score_vs_c_conv_pred": 17.5082409334664, "absolute_error_k_vs_strict_pred": 2.5500000000000114, "score_vs_pred_strict": 17.5082409334664, "truth_raw_assistant_text": "The value is 288.45", "prediction_raw_text": "The value is 285.90RGBO"}
{"id": "line_1069", "truth_parsed_k": 287.96, "prediction_parsed_k_with_c_conv": 287.4, "prediction_parsed_k_strict": 287.4, "absolute_error_k_vs_c_conv_pred": 0.5600000000000023, "score_vs_c_conv_pred": 54.19221519323817, "absolute_error_k_vs_strict_pred": 0.5600000000000023, "score_vs_pred_strict": 54.19221519323817, "truth_raw_assistant_text": "The value is 287.96", "prediction_raw_text": "The value is 287.40\u0e17\u0e35\u0e48\u0e2d\u0e22\u0e39\u0e48"}
{"id": "line_1070", "truth_parsed_k": 287.95, "prediction_parsed_k_with_c_conv": 285.59, "prediction_parsed_k_strict": 285.59, "absolute_error_k_vs_c_conv_pred": 2.3600000000000136, "score_vs_c_conv_pred": 19.492226149747992, "absolute_error_k_vs_strict_pred": 2.3600000000000136, "score_vs_pred_strict": 19.492226149747992, "truth_raw_assistant_text": "The value is 287.95", "prediction_raw_text": "The value is 285.59."}
{"id": "line_1071", "truth_parsed_k": 288.51, "prediction_parsed_k_with_c_conv": 285.84, "prediction_parsed_k_strict": 285.84, "absolute_error_k_vs_c_conv_pred": 2.670000000000016, "score_vs_c_conv_pred": 16.326555874691216, "absolute_error_k_vs_strict_pred": 2.670000000000016, "score_vs_pred_strict": 16.326555874691216, "truth_raw_assistant_text": "The value is 288.51", "prediction_raw_text": "The value is 285.84."}
{"id": "line_1072", "truth_parsed_k": 289.68, "prediction_parsed_k_with_c_conv": 285.91, "prediction_parsed_k_strict": 285.91, "absolute_error_k_vs_c_conv_pred": 3.769999999999982, "score_vs_c_conv_pred": 7.390104211585157, "absolute_error_k_vs_strict_pred": 3.769999999999982, "score_vs_pred_strict": 7.390104211585157, "truth_raw_assistant_text": "The value is 289.68", "prediction_raw_text": "The value is 285.91\u0e01\u0e32\u0e23\u0e25\u0e07\u0e17\u0e38\u0e19"}
{"id": "line_1073", "truth_parsed_k": 290.52, "prediction_parsed_k_with_c_conv": 287.29, "prediction_parsed_k_strict": 287.29, "absolute_error_k_vs_c_conv_pred": 3.2299999999999613, "score_vs_c_conv_pred": 11.408927765251143, "absolute_error_k_vs_strict_pred": 3.2299999999999613, "score_vs_pred_strict": 11.408927765251143, "truth_raw_assistant_text": "The value is 290.52", "prediction_raw_text": "The value is 287.29"}
{"id": "line_1074", "truth_parsed_k": 291.27, "prediction_parsed_k_with_c_conv": 285.49, "prediction_parsed_k_strict": 285.49, "absolute_error_k_vs_c_conv_pred": 5.779999999999973, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.779999999999973, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.27", "prediction_raw_text": "The value is 285.49"}
{"id": "line_1075", "truth_parsed_k": 291.63, "prediction_parsed_k_with_c_conv": 287.89, "prediction_parsed_k_strict": 287.89, "absolute_error_k_vs_c_conv_pred": 3.740000000000009, "score_vs_c_conv_pred": 7.598313373665089, "absolute_error_k_vs_strict_pred": 3.740000000000009, "score_vs_pred_strict": 7.598313373665089, "truth_raw_assistant_text": "The value is 291.63", "prediction_raw_text": "The value is 287.89."}
{"id": "line_1076", "truth_parsed_k": 291.37, "prediction_parsed_k_with_c_conv": 284.86, "prediction_parsed_k_strict": 284.86, "absolute_error_k_vs_c_conv_pred": 6.509999999999991, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.509999999999991, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.37", "prediction_raw_text": "The value is 284.86"}
{"id": "line_1077", "truth_parsed_k": 290.69, "prediction_parsed_k_with_c_conv": 285.66, "prediction_parsed_k_strict": 285.66, "absolute_error_k_vs_c_conv_pred": 5.029999999999973, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.029999999999973, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.69", "prediction_raw_text": "The value is 285.66."}
{"id": "line_1078", "truth_parsed_k": 290.0, "prediction_parsed_k_with_c_conv": 285.68, "prediction_parsed_k_strict": 285.68, "absolute_error_k_vs_c_conv_pred": 4.319999999999993, "score_vs_c_conv_pred": 3.8332541688072874, "absolute_error_k_vs_strict_pred": 4.319999999999993, "score_vs_pred_strict": 3.8332541688072874, "truth_raw_assistant_text": "The value is 290.00", "prediction_raw_text": "The value is 285.68"}
{"id": "line_1079", "truth_parsed_k": 289.13, "prediction_parsed_k_with_c_conv": 284.8, "prediction_parsed_k_strict": 284.8, "absolute_error_k_vs_c_conv_pred": 4.329999999999984, "score_vs_c_conv_pred": 3.772741220181597, "absolute_error_k_vs_strict_pred": 4.329999999999984, "score_vs_pred_strict": 3.772741220181597, "truth_raw_assistant_text": "The value is 289.13", "prediction_raw_text": "The value is 284.80\ufb38"}
{"id": "line_1080", "truth_parsed_k": 288.4, "prediction_parsed_k_with_c_conv": 288.78, "prediction_parsed_k_strict": 288.78, "absolute_error_k_vs_c_conv_pred": 0.37999999999999545, "score_vs_c_conv_pred": 62.40157274739041, "absolute_error_k_vs_strict_pred": 0.37999999999999545, "score_vs_pred_strict": 62.40157274739041, "truth_raw_assistant_text": "The value is 288.40", "prediction_raw_text": "The value is 288.78\u0646\u064f"}
{"id": "line_1081", "truth_parsed_k": 288.12, "prediction_parsed_k_with_c_conv": 285.69, "prediction_parsed_k_strict": 285.69, "absolute_error_k_vs_c_conv_pred": 2.430000000000007, "score_vs_c_conv_pred": 18.744171080179186, "absolute_error_k_vs_strict_pred": 2.430000000000007, "score_vs_pred_strict": 18.744171080179186, "truth_raw_assistant_text": "The value is 288.12", "prediction_raw_text": "The value is 285.69\uf945"}
{"id": "line_1082", "truth_parsed_k": 288.16, "prediction_parsed_k_with_c_conv": 285.9, "prediction_parsed_k_strict": 285.9, "absolute_error_k_vs_c_conv_pred": 2.2600000000000477, "score_vs_c_conv_pred": 20.598264607343488, "absolute_error_k_vs_strict_pred": 2.2600000000000477, "score_vs_pred_strict": 20.598264607343488, "truth_raw_assistant_text": "The value is 288.16", "prediction_raw_text": "The value is 285.90"}
{"id": "line_1083", "truth_parsed_k": 288.62, "prediction_parsed_k_with_c_conv": 284.87, "prediction_parsed_k_strict": 284.87, "absolute_error_k_vs_c_conv_pred": 3.75, "score_vs_c_conv_pred": 7.528731291521307, "absolute_error_k_vs_strict_pred": 3.75, "score_vs_pred_strict": 7.528731291521307, "truth_raw_assistant_text": "The value is 288.62", "prediction_raw_text": "The value is 284.87\u0e40\u0e19\u0e2d\u0e23\u0e4c"}
{"id": "line_1084", "truth_parsed_k": 289.65, "prediction_parsed_k_with_c_conv": 288.46, "prediction_parsed_k_strict": 288.46, "absolute_error_k_vs_c_conv_pred": 1.1899999999999977, "score_vs_c_conv_pred": 36.630334216240726, "absolute_error_k_vs_strict_pred": 1.1899999999999977, "score_vs_pred_strict": 36.630334216240726, "truth_raw_assistant_text": "The value is 289.65", "prediction_raw_text": "The value is 288.46."}
{"id": "line_1085", "truth_parsed_k": 290.6, "prediction_parsed_k_with_c_conv": 285.86, "prediction_parsed_k_strict": 285.86, "absolute_error_k_vs_c_conv_pred": 4.740000000000009, "score_vs_c_conv_pred": 1.4019873170983632, "absolute_error_k_vs_strict_pred": 4.740000000000009, "score_vs_pred_strict": 1.4019873170983632, "truth_raw_assistant_text": "The value is 290.60", "prediction_raw_text": "The value is 285.86\u0e2d\u0e35\u0e01\u0e04\u0e23\u0e31\u0e49\u0e07"}
{"id": "line_1086", "truth_parsed_k": 291.46, "prediction_parsed_k_with_c_conv": 286.71, "prediction_parsed_k_strict": 286.71, "absolute_error_k_vs_c_conv_pred": 4.75, "score_vs_c_conv_pred": 1.3466931481699862, "absolute_error_k_vs_strict_pred": 4.75, "score_vs_pred_strict": 1.3466931481699862, "truth_raw_assistant_text": "The value is 291.46", "prediction_raw_text": "The value is 286.71."}
{"id": "line_1087", "truth_parsed_k": 291.57, "prediction_parsed_k_with_c_conv": 285.56, "prediction_parsed_k_strict": 285.56, "absolute_error_k_vs_c_conv_pred": 6.009999999999991, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.009999999999991, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.57", "prediction_raw_text": "The value is 285.56\uf9ff"}
{"id": "line_1088", "truth_parsed_k": 291.55, "prediction_parsed_k_with_c_conv": 285.84, "prediction_parsed_k_strict": 285.84, "absolute_error_k_vs_c_conv_pred": 5.710000000000036, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.710000000000036, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.55", "prediction_raw_text": "The value is 285.84"}
{"id": "line_1089", "truth_parsed_k": 290.85, "prediction_parsed_k_with_c_conv": 285.58, "prediction_parsed_k_strict": 285.58, "absolute_error_k_vs_c_conv_pred": 5.270000000000039, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.270000000000039, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.85", "prediction_raw_text": "The value is 285.58 cu\u0309a"}
{"id": "line_1090", "truth_parsed_k": 289.98, "prediction_parsed_k_with_c_conv": 285.8, "prediction_parsed_k_strict": 285.8, "absolute_error_k_vs_c_conv_pred": 4.180000000000007, "score_vs_c_conv_pred": 4.69503173798671, "absolute_error_k_vs_strict_pred": 4.180000000000007, "score_vs_pred_strict": 4.69503173798671, "truth_raw_assistant_text": "The value is 289.98", "prediction_raw_text": "The value is 285.80"}
{"id": "line_1091", "truth_parsed_k": 289.27, "prediction_parsed_k_with_c_conv": 284.83, "prediction_parsed_k_strict": 284.83, "absolute_error_k_vs_c_conv_pred": 4.439999999999998, "score_vs_c_conv_pred": 3.115922052920661, "absolute_error_k_vs_strict_pred": 4.439999999999998, "score_vs_pred_strict": 3.115922052920661, "truth_raw_assistant_text": "The value is 289.27", "prediction_raw_text": "The value is 284.83\u0645\u064e\u0627"}
{"id": "line_1092", "truth_parsed_k": 288.49, "prediction_parsed_k_with_c_conv": 287.98, "prediction_parsed_k_strict": 287.98, "absolute_error_k_vs_c_conv_pred": 0.5099999999999909, "score_vs_c_conv_pred": 56.2332107093644, "absolute_error_k_vs_strict_pred": 0.5099999999999909, "score_vs_pred_strict": 56.2332107093644, "truth_raw_assistant_text": "The value is 288.49", "prediction_raw_text": "The value is 287.98\u0e21\u0e35\u0e1b\u0e31\u0e0d\u0e2b\u0e32"}
{"id": "line_1093", "truth_parsed_k": 288.26, "prediction_parsed_k_with_c_conv": 285.74, "prediction_parsed_k_strict": 285.74, "absolute_error_k_vs_c_conv_pred": 2.519999999999982, "score_vs_c_conv_pred": 17.811946229476117, "absolute_error_k_vs_strict_pred": 2.519999999999982, "score_vs_pred_strict": 17.811946229476117, "truth_raw_assistant_text": "The value is 288.26", "prediction_raw_text": "The value is 285.74."}
{"id": "line_1094", "truth_parsed_k": 288.32, "prediction_parsed_k_with_c_conv": 288.57, "prediction_parsed_k_strict": 288.57, "absolute_error_k_vs_c_conv_pred": 0.25, "score_vs_c_conv_pred": 70.41629275170907, "absolute_error_k_vs_strict_pred": 0.25, "score_vs_pred_strict": 70.41629275170907, "truth_raw_assistant_text": "The value is 288.32", "prediction_raw_text": "The value is 288.57."}
{"id": "line_1095", "truth_parsed_k": 288.88, "prediction_parsed_k_with_c_conv": 285.68, "prediction_parsed_k_strict": 285.68, "absolute_error_k_vs_c_conv_pred": 3.1999999999999886, "score_vs_c_conv_pred": 11.65079990761112, "absolute_error_k_vs_strict_pred": 3.1999999999999886, "score_vs_pred_strict": 11.65079990761112, "truth_raw_assistant_text": "The value is 288.88", "prediction_raw_text": "The value is 285.68"}
{"id": "line_1096", "truth_parsed_k": 289.92, "prediction_parsed_k_with_c_conv": 288.41, "prediction_parsed_k_strict": 288.41, "absolute_error_k_vs_c_conv_pred": 1.509999999999991, "score_vs_c_conv_pred": 30.765195885620365, "absolute_error_k_vs_strict_pred": 1.509999999999991, "score_vs_pred_strict": 30.765195885620365, "truth_raw_assistant_text": "The value is 289.92", "prediction_raw_text": "The value is 288.41."}
{"id": "line_1097", "truth_parsed_k": 290.68, "prediction_parsed_k_with_c_conv": 288.27, "prediction_parsed_k_strict": 288.27, "absolute_error_k_vs_c_conv_pred": 2.410000000000025, "score_vs_c_conv_pred": 18.9557890714651, "absolute_error_k_vs_strict_pred": 2.410000000000025, "score_vs_pred_strict": 18.9557890714651, "truth_raw_assistant_text": "The value is 290.68", "prediction_raw_text": "The value is 288.27\u0e40\u0e0a\u0e34"}
{"id": "line_1098", "truth_parsed_k": 291.41, "prediction_parsed_k_with_c_conv": 286.92, "prediction_parsed_k_strict": 286.92, "absolute_error_k_vs_c_conv_pred": 4.490000000000009, "score_vs_c_conv_pred": 2.822583086765784, "absolute_error_k_vs_strict_pred": 4.490000000000009, "score_vs_pred_strict": 2.822583086765784, "truth_raw_assistant_text": "The value is 291.41", "prediction_raw_text": "The value is 286.92\u0e40\u0e08\u0e23\u0e34\u0e0d"}
{"id": "line_1099", "truth_parsed_k": 291.84, "prediction_parsed_k_with_c_conv": 287.58, "prediction_parsed_k_strict": 287.58, "absolute_error_k_vs_c_conv_pred": 4.259999999999991, "score_vs_c_conv_pred": 4.199215644648069, "absolute_error_k_vs_strict_pred": 4.259999999999991, "score_vs_pred_strict": 4.199215644648069, "truth_raw_assistant_text": "The value is 291.84", "prediction_raw_text": "The value is 287.58\u0e40\u0e14\u0e35"}
{"id": "line_1100", "truth_parsed_k": 291.64, "prediction_parsed_k_with_c_conv": 284.79, "prediction_parsed_k_strict": 284.79, "absolute_error_k_vs_c_conv_pred": 6.849999999999966, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.849999999999966, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.64", "prediction_raw_text": "The value is 284.79\u0e40\u0e2a\u0e23\u0e47\u0e08"}
{"id": "line_1101", "truth_parsed_k": 290.99, "prediction_parsed_k_with_c_conv": 285.64, "prediction_parsed_k_strict": 285.64, "absolute_error_k_vs_c_conv_pred": 5.350000000000023, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.350000000000023, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.99", "prediction_raw_text": "The value is 285.64."}
{"id": "line_1102", "truth_parsed_k": 290.26, "prediction_parsed_k_with_c_conv": 284.73, "prediction_parsed_k_strict": 284.73, "absolute_error_k_vs_c_conv_pred": 5.529999999999973, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.529999999999973, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.26", "prediction_raw_text": "The value is 284.73\u0e15\u0e31\u0e49\u0e07\u0e2d\u0e22\u0e39\u0e48"}
{"id": "line_1103", "truth_parsed_k": 289.5, "prediction_parsed_k_with_c_conv": 286.64, "prediction_parsed_k_strict": 286.64, "absolute_error_k_vs_c_conv_pred": 2.8600000000000136, "score_vs_c_conv_pred": 14.555548363428105, "absolute_error_k_vs_strict_pred": 2.8600000000000136, "score_vs_pred_strict": 14.555548363428105, "truth_raw_assistant_text": "The value is 289.50", "prediction_raw_text": "The value is 286.64."}
{"id": "line_1104", "truth_parsed_k": 289.03, "prediction_parsed_k_with_c_conv": 288.76, "prediction_parsed_k_strict": 288.76, "absolute_error_k_vs_c_conv_pred": 0.2699999999999818, "score_vs_c_conv_pred": 69.01710786994701, "absolute_error_k_vs_strict_pred": 0.2699999999999818, "score_vs_pred_strict": 69.01710786994701, "truth_raw_assistant_text": "The value is 289.03", "prediction_raw_text": "The value is 288.76\u0e27\u0e34\u0e40\u0e04\u0e23"}
{"id": "line_1105", "truth_parsed_k": 288.74, "prediction_parsed_k_with_c_conv": 285.87, "prediction_parsed_k_strict": 285.87, "absolute_error_k_vs_c_conv_pred": 2.8700000000000045, "score_vs_c_conv_pred": 14.465487239993047, "absolute_error_k_vs_strict_pred": 2.8700000000000045, "score_vs_pred_strict": 14.465487239993047, "truth_raw_assistant_text": "The value is 288.74", "prediction_raw_text": "The value is 285.87"}
{"id": "line_1106", "truth_parsed_k": 288.57, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 3.599999999999966, "score_vs_c_conv_pred": 8.591826614963693, "absolute_error_k_vs_strict_pred": 3.599999999999966, "score_vs_pred_strict": 8.591826614963693, "truth_raw_assistant_text": "The value is 288.57", "prediction_raw_text": "The value is 284.97 \u0e2b\u0e23\u0e37\u0e2d"}
{"id": "line_1107", "truth_parsed_k": 289.24, "prediction_parsed_k_with_c_conv": 286.87, "prediction_parsed_k_strict": 286.87, "absolute_error_k_vs_c_conv_pred": 2.3700000000000045, "score_vs_c_conv_pred": 19.3840804188251, "absolute_error_k_vs_strict_pred": 2.3700000000000045, "score_vs_pred_strict": 19.3840804188251, "truth_raw_assistant_text": "The value is 289.24", "prediction_raw_text": "The value is 286.87"}
{"id": "line_1108", "truth_parsed_k": 290.16, "prediction_parsed_k_with_c_conv": 286.6, "prediction_parsed_k_strict": 286.6, "absolute_error_k_vs_c_conv_pred": 3.5600000000000023, "score_vs_c_conv_pred": 8.8825527944019, "absolute_error_k_vs_strict_pred": 3.5600000000000023, "score_vs_pred_strict": 8.8825527944019, "truth_raw_assistant_text": "The value is 290.16", "prediction_raw_text": "The value is 286.60 \u0e2a\u0e34\u0e07\u0e2b\u0e32"}
{"id": "line_1109", "truth_parsed_k": 291.16, "prediction_parsed_k_with_c_conv": 284.96, "prediction_parsed_k_strict": 284.96, "absolute_error_k_vs_c_conv_pred": 6.2000000000000455, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.2000000000000455, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.16", "prediction_raw_text": "The value is 284.96\uf97e"}
{"id": "line_1110", "truth_parsed_k": 291.84, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 6.899999999999977, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.899999999999977, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.84", "prediction_raw_text": "The value is 284.94."}
{"id": "line_1111", "truth_parsed_k": 292.04, "prediction_parsed_k_with_c_conv": 284.67, "prediction_parsed_k_strict": 284.67, "absolute_error_k_vs_c_conv_pred": 7.3700000000000045, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.3700000000000045, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.04", "prediction_raw_text": "The value is 284.67\u0e17\u0e38\u0e01\u0e2d\u0e22\u0e48\u0e32\u0e07"}
{"id": "line_1112", "truth_parsed_k": 291.8, "prediction_parsed_k_with_c_conv": 285.88, "prediction_parsed_k_strict": 285.88, "absolute_error_k_vs_c_conv_pred": 5.920000000000016, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.920000000000016, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.80", "prediction_raw_text": "The value is 285.88"}
{"id": "line_1113", "truth_parsed_k": 291.09, "prediction_parsed_k_with_c_conv": 285.85, "prediction_parsed_k_strict": 285.85, "absolute_error_k_vs_c_conv_pred": 5.239999999999952, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.239999999999952, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.09", "prediction_raw_text": "The value is 285.85."}
{"id": "line_1114", "truth_parsed_k": 290.19, "prediction_parsed_k_with_c_conv": 285.64, "prediction_parsed_k_strict": 285.64, "absolute_error_k_vs_c_conv_pred": 4.550000000000011, "score_vs_c_conv_pred": 2.4747429330321147, "absolute_error_k_vs_strict_pred": 4.550000000000011, "score_vs_pred_strict": 2.4747429330321147, "truth_raw_assistant_text": "The value is 290.19", "prediction_raw_text": "The value is 285.64."}
{"id": "line_1115", "truth_parsed_k": 289.46, "prediction_parsed_k_with_c_conv": 284.76, "prediction_parsed_k_strict": 284.76, "absolute_error_k_vs_c_conv_pred": 4.699999999999989, "score_vs_c_conv_pred": 1.6243064400001561, "absolute_error_k_vs_strict_pred": 4.699999999999989, "score_vs_pred_strict": 1.6243064400001561, "truth_raw_assistant_text": "The value is 289.46", "prediction_raw_text": "The value is 284.76"}
{"id": "line_1116", "truth_parsed_k": 288.79, "prediction_parsed_k_with_c_conv": 285.56, "prediction_parsed_k_strict": 285.56, "absolute_error_k_vs_c_conv_pred": 3.230000000000018, "score_vs_c_conv_pred": 11.408927765250688, "absolute_error_k_vs_strict_pred": 3.230000000000018, "score_vs_pred_strict": 11.408927765250688, "truth_raw_assistant_text": "The value is 288.79", "prediction_raw_text": "The value is 285.56."}
{"id": "line_1117", "truth_parsed_k": 288.58, "prediction_parsed_k_with_c_conv": 285.97, "prediction_parsed_k_strict": 285.97, "absolute_error_k_vs_c_conv_pred": 2.609999999999957, "score_vs_c_conv_pred": 16.910916978861145, "absolute_error_k_vs_strict_pred": 2.609999999999957, "score_vs_pred_strict": 16.910916978861145, "truth_raw_assistant_text": "The value is 288.58", "prediction_raw_text": "The value is 285.97\u0e21\u0e35\u0e01\u0e32\u0e23"}
{"id": "line_1118", "truth_parsed_k": 288.75, "prediction_parsed_k_with_c_conv": 285.9, "prediction_parsed_k_strict": 285.9, "absolute_error_k_vs_c_conv_pred": 2.8500000000000227, "score_vs_c_conv_pred": 14.645911705964421, "absolute_error_k_vs_strict_pred": 2.8500000000000227, "score_vs_pred_strict": 14.645911705964421, "truth_raw_assistant_text": "The value is 288.75", "prediction_raw_text": "The value is 285.90"}
{"id": "line_1119", "truth_parsed_k": 288.99, "prediction_parsed_k_with_c_conv": 288.9, "prediction_parsed_k_strict": 288.9, "absolute_error_k_vs_c_conv_pred": 0.09000000000003183, "score_vs_c_conv_pred": 85.39615547822646, "absolute_error_k_vs_strict_pred": 0.09000000000003183, "score_vs_pred_strict": 85.39615547822646, "truth_raw_assistant_text": "The value is 288.99", "prediction_raw_text": "The value is 288.90\uf9a9"}
{"id": "line_1120", "truth_parsed_k": 289.72, "prediction_parsed_k_with_c_conv": 288.08, "prediction_parsed_k_strict": 288.08, "absolute_error_k_vs_c_conv_pred": 1.6400000000000432, "score_vs_c_conv_pred": 28.704972341770517, "absolute_error_k_vs_strict_pred": 1.6400000000000432, "score_vs_pred_strict": 28.704972341770517, "truth_raw_assistant_text": "The value is 289.72", "prediction_raw_text": "The value is 288.08\u0644\u064e"}
{"id": "line_1121", "truth_parsed_k": 290.7, "prediction_parsed_k_with_c_conv": 286.68, "prediction_parsed_k_strict": 286.68, "absolute_error_k_vs_c_conv_pred": 4.019999999999982, "score_vs_c_conv_pred": 5.714921714864262, "absolute_error_k_vs_strict_pred": 4.019999999999982, "score_vs_pred_strict": 5.714921714864262, "truth_raw_assistant_text": "The value is 290.70", "prediction_raw_text": "The value is 286.68\u0e1e\u0e19\u0e31\u0e19"}
{"id": "line_1122", "truth_parsed_k": 291.35, "prediction_parsed_k_with_c_conv": 285.52, "prediction_parsed_k_strict": 285.52, "absolute_error_k_vs_c_conv_pred": 5.830000000000041, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.830000000000041, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.35", "prediction_raw_text": "The value is 285.52."}
{"id": "line_1123", "truth_parsed_k": 291.63, "prediction_parsed_k_with_c_conv": 285.67, "prediction_parsed_k_strict": 285.67, "absolute_error_k_vs_c_conv_pred": 5.9599999999999795, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.9599999999999795, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.63", "prediction_raw_text": "The value is 285.67."}
{"id": "line_1124", "truth_parsed_k": 291.39, "prediction_parsed_k_with_c_conv": 285.3, "prediction_parsed_k_strict": 285.3, "absolute_error_k_vs_c_conv_pred": 6.089999999999975, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.089999999999975, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.39", "prediction_raw_text": "The value is 285.30."}
{"id": "line_1125", "truth_parsed_k": 290.85, "prediction_parsed_k_with_c_conv": 288.97, "prediction_parsed_k_strict": 288.97, "absolute_error_k_vs_c_conv_pred": 1.8799999999999955, "score_vs_c_conv_pred": 25.271798888201523, "absolute_error_k_vs_strict_pred": 1.8799999999999955, "score_vs_pred_strict": 25.271798888201523, "truth_raw_assistant_text": "The value is 290.85", "prediction_raw_text": "The value is 288.97\u0e1b\u0e0f\u0e34"}
{"id": "line_1126", "truth_parsed_k": 290.1, "prediction_parsed_k_with_c_conv": 285.95, "prediction_parsed_k_strict": 285.95, "absolute_error_k_vs_c_conv_pred": 4.150000000000034, "score_vs_c_conv_pred": 4.883341951585729, "absolute_error_k_vs_strict_pred": 4.150000000000034, "score_vs_pred_strict": 4.883341951585729, "truth_raw_assistant_text": "The value is 290.10", "prediction_raw_text": "The value is 285.95."}
{"id": "line_1127", "truth_parsed_k": 289.32, "prediction_parsed_k_with_c_conv": 285.88, "prediction_parsed_k_strict": 285.88, "absolute_error_k_vs_c_conv_pred": 3.4399999999999977, "score_vs_c_conv_pred": 9.774052169676517, "absolute_error_k_vs_strict_pred": 3.4399999999999977, "score_vs_pred_strict": 9.774052169676517, "truth_raw_assistant_text": "The value is 289.32", "prediction_raw_text": "The value is 285.88."}
{"id": "line_1128", "truth_parsed_k": 288.59, "prediction_parsed_k_with_c_conv": 285.77, "prediction_parsed_k_strict": 285.77, "absolute_error_k_vs_c_conv_pred": 2.819999999999993, "score_vs_c_conv_pred": 14.918835503106397, "absolute_error_k_vs_strict_pred": 2.819999999999993, "score_vs_pred_strict": 14.918835503106397, "truth_raw_assistant_text": "The value is 288.59", "prediction_raw_text": "The value is 285.77."}
{"id": "line_1129", "truth_parsed_k": 288.24, "prediction_parsed_k_with_c_conv": 285.7, "prediction_parsed_k_strict": 285.7, "absolute_error_k_vs_c_conv_pred": 2.5400000000000205, "score_vs_c_conv_pred": 17.609095923295158, "absolute_error_k_vs_strict_pred": 2.5400000000000205, "score_vs_pred_strict": 17.609095923295158, "truth_raw_assistant_text": "The value is 288.24", "prediction_raw_text": "The value is 285.70\u0e40\u0e15\u0e34\u0e1a\u0e42\u0e15"}
{"id": "line_1130", "truth_parsed_k": 288.33, "prediction_parsed_k_with_c_conv": 285.53, "prediction_parsed_k_strict": 285.53, "absolute_error_k_vs_c_conv_pred": 2.8000000000000114, "score_vs_c_conv_pred": 15.102333663296175, "absolute_error_k_vs_strict_pred": 2.8000000000000114, "score_vs_pred_strict": 15.102333663296175, "truth_raw_assistant_text": "The value is 288.33", "prediction_raw_text": "The value is 285.53."}
{"id": "line_1131", "truth_parsed_k": 288.94, "prediction_parsed_k_with_c_conv": 286.9, "prediction_parsed_k_strict": 286.9, "absolute_error_k_vs_c_conv_pred": 2.0400000000000205, "score_vs_c_conv_pred": 23.20434740707088, "absolute_error_k_vs_strict_pred": 2.0400000000000205, "score_vs_pred_strict": 23.20434740707088, "truth_raw_assistant_text": "The value is 288.94", "prediction_raw_text": "The value is 286.90\u0e40\u0e1e\u0e35\u0e22\u0e07\u0e41\u0e04\u0e48"}
{"id": "line_1132", "truth_parsed_k": 289.88, "prediction_parsed_k_with_c_conv": 287.35, "prediction_parsed_k_strict": 287.35, "absolute_error_k_vs_c_conv_pred": 2.5299999999999727, "score_vs_c_conv_pred": 17.710330068113056, "absolute_error_k_vs_strict_pred": 2.5299999999999727, "score_vs_pred_strict": 17.710330068113056, "truth_raw_assistant_text": "The value is 289.88", "prediction_raw_text": "The value is 287.35."}
{"id": "line_1133", "truth_parsed_k": 290.67, "prediction_parsed_k_with_c_conv": 286.88, "prediction_parsed_k_strict": 286.88, "absolute_error_k_vs_c_conv_pred": 3.7900000000000205, "score_vs_c_conv_pred": 7.252187132609222, "absolute_error_k_vs_strict_pred": 3.7900000000000205, "score_vs_pred_strict": 7.252187132609222, "truth_raw_assistant_text": "The value is 290.67", "prediction_raw_text": "The value is 286.88\u0e01\u0e32\u0e23\u0e04\u0e49\u0e32"}
{"id": "line_1134", "truth_parsed_k": 291.39, "prediction_parsed_k_with_c_conv": 285.75, "prediction_parsed_k_strict": 285.75, "absolute_error_k_vs_c_conv_pred": 5.639999999999986, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.639999999999986, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.39", "prediction_raw_text": "The value is 285.75."}
{"id": "line_1135", "truth_parsed_k": 291.55, "prediction_parsed_k_with_c_conv": 287.9, "prediction_parsed_k_strict": 287.9, "absolute_error_k_vs_c_conv_pred": 3.650000000000034, "score_vs_c_conv_pred": 8.232777951574855, "absolute_error_k_vs_strict_pred": 3.650000000000034, "score_vs_pred_strict": 8.232777951574855, "truth_raw_assistant_text": "The value is 291.55", "prediction_raw_text": "The value is 287.90\u0e27\u0e34\u0e18\u0e35\u0e01\u0e32\u0e23"}
{"id": "line_1136", "truth_parsed_k": 291.51, "prediction_parsed_k_with_c_conv": 288.99, "prediction_parsed_k_strict": 288.99, "absolute_error_k_vs_c_conv_pred": 2.519999999999982, "score_vs_c_conv_pred": 17.811946229476117, "absolute_error_k_vs_strict_pred": 2.519999999999982, "score_vs_pred_strict": 17.811946229476117, "truth_raw_assistant_text": "The value is 291.51", "prediction_raw_text": "The value is 288.99\u0e27\u0e34\u0e19"}
{"id": "line_1137", "truth_parsed_k": 290.96, "prediction_parsed_k_with_c_conv": 288.5, "prediction_parsed_k_strict": 288.5, "absolute_error_k_vs_c_conv_pred": 2.4599999999999795, "score_vs_c_conv_pred": 18.429829952686983, "absolute_error_k_vs_strict_pred": 2.4599999999999795, "score_vs_pred_strict": 18.429829952686983, "truth_raw_assistant_text": "The value is 290.96", "prediction_raw_text": "The value is 288.50\u0e21\u0e31\u0e01\u0e08\u0e30"}
{"id": "line_1138", "truth_parsed_k": 290.05, "prediction_parsed_k_with_c_conv": 288.06, "prediction_parsed_k_strict": 288.06, "absolute_error_k_vs_c_conv_pred": 1.990000000000009, "score_vs_c_conv_pred": 23.83354076959402, "absolute_error_k_vs_strict_pred": 1.990000000000009, "score_vs_pred_strict": 23.83354076959402, "truth_raw_assistant_text": "The value is 290.05", "prediction_raw_text": "The value is 288.06\u0e1c\u0e25\u0e34\u0e15\u0e20\u0e31\u0e13\u0e11\u0e4c"}
{"id": "line_1139", "truth_parsed_k": 289.12, "prediction_parsed_k_with_c_conv": 284.89, "prediction_parsed_k_strict": 284.89, "absolute_error_k_vs_c_conv_pred": 4.230000000000018, "score_vs_c_conv_pred": 4.38407850435194, "absolute_error_k_vs_strict_pred": 4.230000000000018, "score_vs_pred_strict": 4.38407850435194, "truth_raw_assistant_text": "The value is 289.12", "prediction_raw_text": "The value is 284.89."}
{"id": "line_1140", "truth_parsed_k": 288.74, "prediction_parsed_k_with_c_conv": 285.95, "prediction_parsed_k_strict": 285.95, "absolute_error_k_vs_c_conv_pred": 2.7900000000000205, "score_vs_c_conv_pred": 15.19455379043646, "absolute_error_k_vs_strict_pred": 2.7900000000000205, "score_vs_pred_strict": 15.19455379043646, "truth_raw_assistant_text": "The value is 288.74", "prediction_raw_text": "The value is 285.95\u0e2a\u0e2b\u0e23\u0e31\u0e10"}
{"id": "line_1141", "truth_parsed_k": 288.6, "prediction_parsed_k_with_c_conv": 285.81, "prediction_parsed_k_strict": 285.81, "absolute_error_k_vs_c_conv_pred": 2.7900000000000205, "score_vs_c_conv_pred": 15.19455379043646, "absolute_error_k_vs_strict_pred": 2.7900000000000205, "score_vs_pred_strict": 15.19455379043646, "truth_raw_assistant_text": "The value is 288.60", "prediction_raw_text": "The value is 285.81"}
{"id": "line_1142", "truth_parsed_k": 288.66, "prediction_parsed_k_with_c_conv": 284.98, "prediction_parsed_k_strict": 284.98, "absolute_error_k_vs_c_conv_pred": 3.680000000000007, "score_vs_c_conv_pred": 8.019624457567708, "absolute_error_k_vs_strict_pred": 3.680000000000007, "score_vs_pred_strict": 8.019624457567708, "truth_raw_assistant_text": "The value is 288.66", "prediction_raw_text": "The value is 284.98\u0e08\u0e31"}
{"id": "line_1143", "truth_parsed_k": 289.15, "prediction_parsed_k_with_c_conv": 285.96, "prediction_parsed_k_strict": 285.96, "absolute_error_k_vs_c_conv_pred": 3.1899999999999977, "score_vs_c_conv_pred": 11.731909158108634, "absolute_error_k_vs_strict_pred": 3.1899999999999977, "score_vs_pred_strict": 11.731909158108634, "truth_raw_assistant_text": "The value is 289.15", "prediction_raw_text": "The value is 285.96\u0e2b\u0e25\u0e31\u0e01\u0e10\u0e32\u0e19"}
{"id": "line_1144", "truth_parsed_k": 289.91, "prediction_parsed_k_with_c_conv": 286.22, "prediction_parsed_k_strict": 286.22, "absolute_error_k_vs_c_conv_pred": 3.6899999999999977, "score_vs_c_conv_pred": 7.9489465933544, "absolute_error_k_vs_strict_pred": 3.6899999999999977, "score_vs_pred_strict": 7.9489465933544, "truth_raw_assistant_text": "The value is 289.91", "prediction_raw_text": "The value is 286.22 \u0641\u0650"}
{"id": "line_1145", "truth_parsed_k": 290.73, "prediction_parsed_k_with_c_conv": 286.51, "prediction_parsed_k_strict": 286.51, "absolute_error_k_vs_c_conv_pred": 4.220000000000027, "score_vs_c_conv_pred": 4.445982556456385, "absolute_error_k_vs_strict_pred": 4.220000000000027, "score_vs_pred_strict": 4.445982556456385, "truth_raw_assistant_text": "The value is 290.73", "prediction_raw_text": "The value is 286.51\u0e1c\u0e39\u0e49\u0e1b\u0e48\u0e27\u0e22"}
{"id": "line_1146", "truth_parsed_k": 291.5, "prediction_parsed_k_with_c_conv": 285.43, "prediction_parsed_k_strict": 285.43, "absolute_error_k_vs_c_conv_pred": 6.069999999999993, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.069999999999993, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.50", "prediction_raw_text": "The value is 285.43\u0e41\u0e0a\u0e21\u0e1b\u0e4c"}
{"id": "line_1147", "truth_parsed_k": 291.78, "prediction_parsed_k_with_c_conv": 285.6, "prediction_parsed_k_strict": 285.6, "absolute_error_k_vs_c_conv_pred": 6.17999999999995, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.17999999999995, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.78", "prediction_raw_text": "The value is 285.60\u0e44\u0e14\u0e49\u0e27\u0e48\u0e32"}
{"id": "line_1148", "truth_parsed_k": 291.6, "prediction_parsed_k_with_c_conv": 285.92, "prediction_parsed_k_strict": 285.92, "absolute_error_k_vs_c_conv_pred": 5.680000000000007, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.680000000000007, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.60", "prediction_raw_text": "The value is 285.92."}
{"id": "line_1149", "truth_parsed_k": 290.98, "prediction_parsed_k_with_c_conv": 288.82, "prediction_parsed_k_strict": 288.82, "absolute_error_k_vs_c_conv_pred": 2.160000000000025, "score_vs_c_conv_pred": 21.751685066803127, "absolute_error_k_vs_strict_pred": 2.160000000000025, "score_vs_pred_strict": 21.751685066803127, "truth_raw_assistant_text": "The value is 290.98", "prediction_raw_text": "The value is 288.82"}
{"id": "line_1150", "truth_parsed_k": 290.09, "prediction_parsed_k_with_c_conv": 286.3, "prediction_parsed_k_strict": 286.3, "absolute_error_k_vs_c_conv_pred": 3.7899999999999636, "score_vs_c_conv_pred": 7.252187132609622, "absolute_error_k_vs_strict_pred": 3.7899999999999636, "score_vs_pred_strict": 7.252187132609622, "truth_raw_assistant_text": "The value is 290.09", "prediction_raw_text": "The value is 286.30\u0e40\u0e25\u0e37\u0e2d"}
{"id": "line_1151", "truth_parsed_k": 289.45, "prediction_parsed_k_with_c_conv": 285.82, "prediction_parsed_k_strict": 285.82, "absolute_error_k_vs_c_conv_pred": 3.6299999999999955, "score_vs_c_conv_pred": 8.375823444956499, "absolute_error_k_vs_strict_pred": 3.6299999999999955, "score_vs_pred_strict": 8.375823444956499, "truth_raw_assistant_text": "The value is 289.45", "prediction_raw_text": "The value is 285.82\u0e43\u0e2b\u0e49\u0e40\u0e1b\u0e47\u0e19"}
{"id": "line_1152", "truth_parsed_k": 289.0, "prediction_parsed_k_with_c_conv": 285.87, "prediction_parsed_k_strict": 285.87, "absolute_error_k_vs_c_conv_pred": 3.1299999999999955, "score_vs_c_conv_pred": 12.223763029508095, "absolute_error_k_vs_strict_pred": 3.1299999999999955, "score_vs_pred_strict": 12.223763029508095, "truth_raw_assistant_text": "The value is 289.00", "prediction_raw_text": "The value is 285.87\u0e41\u0e22\u0e48"}
{"id": "line_1153", "truth_parsed_k": 288.75, "prediction_parsed_k_with_c_conv": 285.87, "prediction_parsed_k_strict": 285.87, "absolute_error_k_vs_c_conv_pred": 2.8799999999999955, "score_vs_c_conv_pred": 14.37572632085965, "absolute_error_k_vs_strict_pred": 2.8799999999999955, "score_vs_pred_strict": 14.37572632085965, "truth_raw_assistant_text": "The value is 288.75", "prediction_raw_text": "The value is 285.87\u0e04\u0e38\u0e22"}
{"id": "line_1154", "truth_parsed_k": 288.93, "prediction_parsed_k_with_c_conv": 285.92, "prediction_parsed_k_strict": 285.92, "absolute_error_k_vs_c_conv_pred": 3.009999999999991, "score_vs_c_conv_pred": 13.235271649804936, "absolute_error_k_vs_strict_pred": 3.009999999999991, "score_vs_pred_strict": 13.235271649804936, "truth_raw_assistant_text": "The value is 288.93", "prediction_raw_text": "The value is 285.92\u0e20\u0e32\u0e1e\u0e22\u0e19\u0e15\u0e23\u0e4c"}
{"id": "line_1155", "truth_parsed_k": 289.24, "prediction_parsed_k_with_c_conv": 285.91, "prediction_parsed_k_strict": 285.91, "absolute_error_k_vs_c_conv_pred": 3.329999999999984, "score_vs_c_conv_pred": 10.618026544851222, "absolute_error_k_vs_strict_pred": 3.329999999999984, "score_vs_pred_strict": 10.618026544851222, "truth_raw_assistant_text": "The value is 289.24", "prediction_raw_text": "The value is 285.91\u0e41\u0e02\u0e47\u0e07"}
{"id": "line_1156", "truth_parsed_k": 290.03, "prediction_parsed_k_with_c_conv": 285.76, "prediction_parsed_k_strict": 285.76, "absolute_error_k_vs_c_conv_pred": 4.269999999999982, "score_vs_c_conv_pred": 4.137875639066491, "absolute_error_k_vs_strict_pred": 4.269999999999982, "score_vs_pred_strict": 4.137875639066491, "truth_raw_assistant_text": "The value is 290.03", "prediction_raw_text": "The value is 285.76\u0e40\u0e19\u0e47"}
{"id": "line_1157", "truth_parsed_k": 291.02, "prediction_parsed_k_with_c_conv": 285.58, "prediction_parsed_k_strict": 285.58, "absolute_error_k_vs_c_conv_pred": 5.439999999999998, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.439999999999998, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.02", "prediction_raw_text": "The value is 285.58\u0e40\u0e04\u0e25\u0e47\u0e14"}
{"id": "line_1158", "truth_parsed_k": 291.73, "prediction_parsed_k_with_c_conv": 288.84, "prediction_parsed_k_strict": 288.84, "absolute_error_k_vs_c_conv_pred": 2.890000000000043, "score_vs_c_conv_pred": 14.286263611308891, "absolute_error_k_vs_strict_pred": 2.890000000000043, "score_vs_pred_strict": 14.286263611308891, "truth_raw_assistant_text": "The value is 291.73", "prediction_raw_text": "The value is 288.84."}
{"id": "line_1159", "truth_parsed_k": 292.09, "prediction_parsed_k_with_c_conv": 285.77, "prediction_parsed_k_strict": 285.77, "absolute_error_k_vs_c_conv_pred": 6.319999999999993, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.319999999999993, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.09", "prediction_raw_text": "The value is 285.77"}
{"id": "line_1160", "truth_parsed_k": 291.92, "prediction_parsed_k_with_c_conv": 285.62, "prediction_parsed_k_strict": 285.62, "absolute_error_k_vs_c_conv_pred": 6.300000000000011, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.300000000000011, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.92", "prediction_raw_text": "The value is 285.62\u0e21\u0e31\u0e49\u0e22"}
{"id": "line_1161", "truth_parsed_k": 291.23, "prediction_parsed_k_with_c_conv": 288.53, "prediction_parsed_k_strict": 288.53, "absolute_error_k_vs_c_conv_pred": 2.7000000000000455, "score_vs_c_conv_pred": 16.03906292182582, "absolute_error_k_vs_strict_pred": 2.7000000000000455, "score_vs_pred_strict": 16.03906292182582, "truth_raw_assistant_text": "The value is 291.23", "prediction_raw_text": "The value is 288.53."}
{"id": "line_1162", "truth_parsed_k": 290.47, "prediction_parsed_k_with_c_conv": 286.8, "prediction_parsed_k_strict": 286.8, "absolute_error_k_vs_c_conv_pred": 3.670000000000016, "score_vs_c_conv_pred": 8.090488316374921, "absolute_error_k_vs_strict_pred": 3.670000000000016, "score_vs_pred_strict": 8.090488316374921, "truth_raw_assistant_text": "The value is 290.47", "prediction_raw_text": "The value is 286.80\u0e1b\u0e23\u0e30\u0e15\u0e39"}
{"id": "line_1163", "truth_parsed_k": 289.77, "prediction_parsed_k_with_c_conv": 285.79, "prediction_parsed_k_strict": 285.79, "absolute_error_k_vs_c_conv_pred": 3.9799999999999613, "score_vs_c_conv_pred": 5.976046193094131, "absolute_error_k_vs_strict_pred": 3.9799999999999613, "score_vs_pred_strict": 5.976046193094131, "truth_raw_assistant_text": "The value is 289.77", "prediction_raw_text": "The value is 285.79\u0e1d\u0e31\u0e48\u0e07"}
{"id": "line_1164", "truth_parsed_k": 289.17, "prediction_parsed_k_with_c_conv": 286.44, "prediction_parsed_k_strict": 286.44, "absolute_error_k_vs_c_conv_pred": 2.730000000000018, "score_vs_c_conv_pred": 15.754606923131975, "absolute_error_k_vs_strict_pred": 2.730000000000018, "score_vs_pred_strict": 15.754606923131975, "truth_raw_assistant_text": "The value is 289.17", "prediction_raw_text": "The value is 286.44."}
{"id": "line_1165", "truth_parsed_k": 288.86, "prediction_parsed_k_with_c_conv": 285.46, "prediction_parsed_k_strict": 285.46, "absolute_error_k_vs_c_conv_pred": 3.400000000000034, "score_vs_c_conv_pred": 10.077900254467576, "absolute_error_k_vs_strict_pred": 3.400000000000034, "score_vs_pred_strict": 10.077900254467576, "truth_raw_assistant_text": "The value is 288.86", "prediction_raw_text": "The value is 285.46\u0e27\u0e31\u0e12\u0e19"}
{"id": "line_1166", "truth_parsed_k": 289.05, "prediction_parsed_k_with_c_conv": 284.94154352367, "prediction_parsed_k_strict": 284.94154352367, "absolute_error_k_vs_c_conv_pred": 4.108456476329991, "score_vs_c_conv_pred": 5.146304644731659, "absolute_error_k_vs_strict_pred": 4.108456476329991, "score_vs_pred_strict": 5.146304644731659, "truth_raw_assistant_text": "The value is 289.05", "prediction_raw_text": "The value is 284.94154352367."}
{"id": "line_1167", "truth_parsed_k": 289.61, "prediction_parsed_k_with_c_conv": 285.7, "prediction_parsed_k_strict": 285.7, "absolute_error_k_vs_c_conv_pred": 3.910000000000025, "score_vs_c_conv_pred": 6.439197058986112, "absolute_error_k_vs_strict_pred": 3.910000000000025, "score_vs_pred_strict": 6.439197058986112, "truth_raw_assistant_text": "The value is 289.61", "prediction_raw_text": "The value is 285.70."}
{"id": "line_1168", "truth_parsed_k": 290.31, "prediction_parsed_k_with_c_conv": 284.73, "prediction_parsed_k_strict": 284.73, "absolute_error_k_vs_c_conv_pred": 5.579999999999984, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.579999999999984, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.31", "prediction_raw_text": "The value is 284.73."}
{"id": "line_1169", "truth_parsed_k": 291.17, "prediction_parsed_k_with_c_conv": 285.6, "prediction_parsed_k_strict": 285.6, "absolute_error_k_vs_c_conv_pred": 5.569999999999993, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.569999999999993, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.17", "prediction_raw_text": "The value is 285.60\uf92e"}
{"id": "line_1170", "truth_parsed_k": 292.09, "prediction_parsed_k_with_c_conv": 285.42, "prediction_parsed_k_strict": 285.42, "absolute_error_k_vs_c_conv_pred": 6.669999999999959, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.669999999999959, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.09", "prediction_raw_text": "The value is 285.42\ufa1a"}
{"id": "line_1171", "truth_parsed_k": 292.27, "prediction_parsed_k_with_c_conv": 285.76, "prediction_parsed_k_strict": 285.76, "absolute_error_k_vs_c_conv_pred": 6.509999999999991, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.509999999999991, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.27", "prediction_raw_text": "The value is 285.76\u0e01\u0e47\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16"}
{"id": "line_1172", "truth_parsed_k": 292.01, "prediction_parsed_k_with_c_conv": 285.95, "prediction_parsed_k_strict": 285.95, "absolute_error_k_vs_c_conv_pred": 6.060000000000002, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.060000000000002, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.01", "prediction_raw_text": "The value is 285.95"}
{"id": "line_1173", "truth_parsed_k": 291.28, "prediction_parsed_k_with_c_conv": 284.85, "prediction_parsed_k_strict": 284.85, "absolute_error_k_vs_c_conv_pred": 6.42999999999995, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.42999999999995, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.28", "prediction_raw_text": "The value is 284.85"}
{"id": "line_1174", "truth_parsed_k": 290.46, "prediction_parsed_k_with_c_conv": 285.99, "prediction_parsed_k_strict": 285.99, "absolute_error_k_vs_c_conv_pred": 4.46999999999997, "score_vs_c_conv_pred": 2.939535499945378, "absolute_error_k_vs_strict_pred": 4.46999999999997, "score_vs_pred_strict": 2.939535499945378, "truth_raw_assistant_text": "The value is 290.46", "prediction_raw_text": "The value is 285.99 \u0648\u064e"}
{"id": "line_1175", "truth_parsed_k": 289.57, "prediction_parsed_k_with_c_conv": 285.24, "prediction_parsed_k_strict": 285.24, "absolute_error_k_vs_c_conv_pred": 4.329999999999984, "score_vs_c_conv_pred": 3.772741220181597, "absolute_error_k_vs_strict_pred": 4.329999999999984, "score_vs_pred_strict": 3.772741220181597, "truth_raw_assistant_text": "The value is 289.57", "prediction_raw_text": "The value is 285.24."}
{"id": "line_1176", "truth_parsed_k": 288.79, "prediction_parsed_k_with_c_conv": 286.68, "prediction_parsed_k_strict": 286.68, "absolute_error_k_vs_c_conv_pred": 2.1100000000000136, "score_vs_c_conv_pred": 22.347467130089584, "absolute_error_k_vs_strict_pred": 2.1100000000000136, "score_vs_pred_strict": 22.347467130089584, "truth_raw_assistant_text": "The value is 288.79", "prediction_raw_text": "The value is 286.68\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e2a\u0e23\u0e23"}
{"id": "line_1177", "truth_parsed_k": 288.76, "prediction_parsed_k_with_c_conv": 284.7, "prediction_parsed_k_strict": 284.7, "absolute_error_k_vs_c_conv_pred": 4.060000000000002, "score_vs_c_conv_pred": 5.456305073482948, "absolute_error_k_vs_strict_pred": 4.060000000000002, "score_vs_pred_strict": 5.456305073482948, "truth_raw_assistant_text": "The value is 288.76", "prediction_raw_text": "The value is 284.70\u0e2a\u0e44\u0e15\u0e25\u0e4c"}
{"id": "line_1178", "truth_parsed_k": 288.86, "prediction_parsed_k_with_c_conv": 284.81, "prediction_parsed_k_strict": 284.81, "absolute_error_k_vs_c_conv_pred": 4.050000000000011, "score_vs_c_conv_pred": 5.520726756855332, "absolute_error_k_vs_strict_pred": 4.050000000000011, "score_vs_pred_strict": 5.520726756855332, "truth_raw_assistant_text": "The value is 288.86", "prediction_raw_text": "The value is 284.81\u0e0b\u0e38"}
{"id": "line_1179", "truth_parsed_k": 289.35, "prediction_parsed_k_with_c_conv": 285.97, "prediction_parsed_k_strict": 285.97, "absolute_error_k_vs_c_conv_pred": 3.3799999999999955, "score_vs_c_conv_pred": 10.231119733596717, "absolute_error_k_vs_strict_pred": 3.3799999999999955, "score_vs_pred_strict": 10.231119733596717, "truth_raw_assistant_text": "The value is 289.35", "prediction_raw_text": "The value is 285.97."}
{"id": "line_1180", "truth_parsed_k": 290.19, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 5.25, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.25, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.19", "prediction_raw_text": "The value is 284.94"}
{"id": "line_1181", "truth_parsed_k": 291.0, "prediction_parsed_k_with_c_conv": 285.81, "prediction_parsed_k_strict": 285.81, "absolute_error_k_vs_c_conv_pred": 5.189999999999998, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.189999999999998, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.00", "prediction_raw_text": "The value is 285.81\u0e43\u0e2b\u0e49\u0e40\u0e1b\u0e47\u0e19"}
{"id": "line_1182", "truth_parsed_k": 291.68, "prediction_parsed_k_with_c_conv": 285.82, "prediction_parsed_k_strict": 285.82, "absolute_error_k_vs_c_conv_pred": 5.860000000000014, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.860000000000014, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.68", "prediction_raw_text": "The value is 285.82\u0e40\u0e1a\u0e2d\u0e23\u0e4c"}
{"id": "line_1183", "truth_parsed_k": 291.9, "prediction_parsed_k_with_c_conv": 285.71, "prediction_parsed_k_strict": 285.71, "absolute_error_k_vs_c_conv_pred": 6.189999999999998, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.189999999999998, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.90", "prediction_raw_text": "The value is 285.71\u0e1d\u0e23\u0e31\u0e48\u0e07\u0e40\u0e28\u0e2a"}
{"id": "line_1184", "truth_parsed_k": 291.7, "prediction_parsed_k_with_c_conv": 284.57, "prediction_parsed_k_strict": 284.57, "absolute_error_k_vs_c_conv_pred": 7.1299999999999955, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.1299999999999955, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.70", "prediction_raw_text": "The value is 284.57."}
{"id": "line_1185", "truth_parsed_k": 291.05, "prediction_parsed_k_with_c_conv": 285.85, "prediction_parsed_k_strict": 285.85, "absolute_error_k_vs_c_conv_pred": 5.199999999999989, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.199999999999989, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.05", "prediction_raw_text": "The value is 285.85."}
{"id": "line_1186", "truth_parsed_k": 290.35, "prediction_parsed_k_with_c_conv": 285.92, "prediction_parsed_k_strict": 285.92, "absolute_error_k_vs_c_conv_pred": 4.430000000000007, "score_vs_c_conv_pred": 3.1749752581800905, "absolute_error_k_vs_strict_pred": 4.430000000000007, "score_vs_pred_strict": 3.1749752581800905, "truth_raw_assistant_text": "The value is 290.35", "prediction_raw_text": "The value is 285.92."}
{"id": "line_1187", "truth_parsed_k": 289.47, "prediction_parsed_k_with_c_conv": 284.92, "prediction_parsed_k_strict": 284.92, "absolute_error_k_vs_c_conv_pred": 4.550000000000011, "score_vs_c_conv_pred": 2.4747429330321147, "absolute_error_k_vs_strict_pred": 4.550000000000011, "score_vs_pred_strict": 2.4747429330321147, "truth_raw_assistant_text": "The value is 289.47", "prediction_raw_text": "The value is 284.92"}
{"id": "line_1188", "truth_parsed_k": 288.83, "prediction_parsed_k_with_c_conv": 284.78, "prediction_parsed_k_strict": 284.78, "absolute_error_k_vs_c_conv_pred": 4.050000000000011, "score_vs_c_conv_pred": 5.520726756855332, "absolute_error_k_vs_strict_pred": 4.050000000000011, "score_vs_pred_strict": 5.520726756855332, "truth_raw_assistant_text": "The value is 288.83", "prediction_raw_text": "The value is 284.78\u0e40\u0e25\u0e22\u0e17\u0e35\u0e40\u0e14\u0e35\u0e22\u0e27"}
{"id": "line_1189", "truth_parsed_k": 288.72, "prediction_parsed_k_with_c_conv": 285.81, "prediction_parsed_k_strict": 285.81, "absolute_error_k_vs_c_conv_pred": 2.910000000000025, "score_vs_c_conv_pred": 14.108224940898095, "absolute_error_k_vs_strict_pred": 2.910000000000025, "score_vs_pred_strict": 14.108224940898095, "truth_raw_assistant_text": "The value is 288.72", "prediction_raw_text": "The value is 285.81\uf9a8"}
{"id": "line_1190", "truth_parsed_k": 288.67, "prediction_parsed_k_with_c_conv": 284.84, "prediction_parsed_k_strict": 284.84, "absolute_error_k_vs_c_conv_pred": 3.830000000000041, "score_vs_c_conv_pred": 6.97845414468643, "absolute_error_k_vs_strict_pred": 3.830000000000041, "score_vs_pred_strict": 6.97845414468643, "truth_raw_assistant_text": "The value is 288.67", "prediction_raw_text": "The value is 284.84\u0e04\u0e34"}
{"id": "line_1191", "truth_parsed_k": 289.06, "prediction_parsed_k_with_c_conv": 285.27, "prediction_parsed_k_strict": 285.27, "absolute_error_k_vs_c_conv_pred": 3.7900000000000205, "score_vs_c_conv_pred": 7.252187132609222, "absolute_error_k_vs_strict_pred": 3.7900000000000205, "score_vs_pred_strict": 7.252187132609222, "truth_raw_assistant_text": "The value is 289.06", "prediction_raw_text": "The value is 285.27 \u0e41\u0e25\u0e49\u0e27"}
{"id": "line_1192", "truth_parsed_k": 290.12, "prediction_parsed_k_with_c_conv": 286.91, "prediction_parsed_k_strict": 286.91, "absolute_error_k_vs_c_conv_pred": 3.2099999999999795, "score_vs_c_conv_pred": 11.569934228803058, "absolute_error_k_vs_strict_pred": 3.2099999999999795, "score_vs_pred_strict": 11.569934228803058, "truth_raw_assistant_text": "The value is 290.12", "prediction_raw_text": "The value is 286.91\u0e40\u0e23\u0e34\u0e48\u0e21\u0e15\u0e49\u0e19"}
{"id": "line_1193", "truth_parsed_k": 290.85, "prediction_parsed_k_with_c_conv": 285.97, "prediction_parsed_k_strict": 285.97, "absolute_error_k_vs_c_conv_pred": 4.8799999999999955, "score_vs_c_conv_pred": 0.638014069249393, "absolute_error_k_vs_strict_pred": 4.8799999999999955, "score_vs_pred_strict": 0.638014069249393, "truth_raw_assistant_text": "The value is 290.85", "prediction_raw_text": "The value is 285.97\u0e2b\u0e31\u0e19"}
{"id": "line_1194", "truth_parsed_k": 291.63, "prediction_parsed_k_with_c_conv": 285.45, "prediction_parsed_k_strict": 285.45, "absolute_error_k_vs_c_conv_pred": 6.180000000000007, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.180000000000007, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.63", "prediction_raw_text": "The value is 285.45\u0e04\u0e31\u0e19"}
{"id": "line_1195", "truth_parsed_k": 291.91, "prediction_parsed_k_with_c_conv": 285.55, "prediction_parsed_k_strict": 285.55, "absolute_error_k_vs_c_conv_pred": 6.360000000000014, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.360000000000014, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.91", "prediction_raw_text": "The value is 285.55."}
{"id": "line_1196", "truth_parsed_k": 291.74, "prediction_parsed_k_with_c_conv": 284.84, "prediction_parsed_k_strict": 284.84, "absolute_error_k_vs_c_conv_pred": 6.900000000000034, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.900000000000034, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.74", "prediction_raw_text": "The value is 284.84\u0e2d\u0e49\u0e32\u0e07"}
{"id": "line_1197", "truth_parsed_k": 291.19, "prediction_parsed_k_with_c_conv": 285.91, "prediction_parsed_k_strict": 285.91, "absolute_error_k_vs_c_conv_pred": 5.279999999999973, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.279999999999973, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.19", "prediction_raw_text": "The value is 285.91\u0e01\u0e48\u0e2d\u0e19\u0e2b\u0e19\u0e49\u0e32"}
{"id": "line_1198", "truth_parsed_k": 290.16, "prediction_parsed_k_with_c_conv": 284.84, "prediction_parsed_k_strict": 284.84, "absolute_error_k_vs_c_conv_pred": 5.32000000000005, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.32000000000005, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.16", "prediction_raw_text": "The value is 284.84"}
{"id": "line_1199", "truth_parsed_k": 289.54, "prediction_parsed_k_with_c_conv": 285.74, "prediction_parsed_k_strict": 285.74, "absolute_error_k_vs_c_conv_pred": 3.8000000000000114, "score_vs_c_conv_pred": 7.183492577992245, "absolute_error_k_vs_strict_pred": 3.8000000000000114, "score_vs_pred_strict": 7.183492577992245, "truth_raw_assistant_text": "The value is 289.54", "prediction_raw_text": "The value is 285.74\u0e2d\u0e35\u0e40\u0e21"}
{"id": "line_1200", "truth_parsed_k": 288.96, "prediction_parsed_k_with_c_conv": 287.8, "prediction_parsed_k_strict": 287.8, "absolute_error_k_vs_c_conv_pred": 1.1599999999999682, "score_vs_c_conv_pred": 37.25178296876349, "absolute_error_k_vs_strict_pred": 1.1599999999999682, "score_vs_pred_strict": 37.25178296876349, "truth_raw_assistant_text": "The value is 288.96", "prediction_raw_text": "The value is 287.80\u0e41\u0e2b\u0e25\u0e48\u0e07"}
{"id": "line_1201", "truth_parsed_k": 288.65, "prediction_parsed_k_with_c_conv": 285.62, "prediction_parsed_k_strict": 285.62, "absolute_error_k_vs_c_conv_pred": 3.0299999999999727, "score_vs_c_conv_pred": 13.064026234499714, "absolute_error_k_vs_strict_pred": 3.0299999999999727, "score_vs_pred_strict": 13.064026234499714, "truth_raw_assistant_text": "The value is 288.65", "prediction_raw_text": "The value is 285.62\ufa22"}
{"id": "line_1202", "truth_parsed_k": 288.69, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 3.75, "score_vs_c_conv_pred": 7.528731291521307, "absolute_error_k_vs_strict_pred": 3.75, "score_vs_pred_strict": 7.528731291521307, "truth_raw_assistant_text": "The value is 288.69", "prediction_raw_text": "The value is 284.94."}
{"id": "line_1203", "truth_parsed_k": 289.36, "prediction_parsed_k_with_c_conv": 285.86, "prediction_parsed_k_strict": 285.86, "absolute_error_k_vs_c_conv_pred": 3.5, "score_vs_c_conv_pred": 9.324613350647704, "absolute_error_k_vs_strict_pred": 3.5, "score_vs_pred_strict": 9.324613350647704, "truth_raw_assistant_text": "The value is 289.36", "prediction_raw_text": "The value is 285.86."}
{"id": "line_1204", "truth_parsed_k": 290.06, "prediction_parsed_k_with_c_conv": 285.4, "prediction_parsed_k_strict": 285.4, "absolute_error_k_vs_c_conv_pred": 4.660000000000025, "score_vs_c_conv_pred": 1.8484763157018635, "absolute_error_k_vs_strict_pred": 4.660000000000025, "score_vs_pred_strict": 1.8484763157018635, "truth_raw_assistant_text": "The value is 290.06", "prediction_raw_text": "The value is 285.40"}
{"id": "line_1205", "truth_parsed_k": 290.89, "prediction_parsed_k_with_c_conv": 286.88, "prediction_parsed_k_strict": 286.88, "absolute_error_k_vs_c_conv_pred": 4.009999999999991, "score_vs_c_conv_pred": 5.779965830562417, "absolute_error_k_vs_strict_pred": 4.009999999999991, "score_vs_pred_strict": 5.779965830562417, "truth_raw_assistant_text": "The value is 290.89", "prediction_raw_text": "The value is 286.88\u0e01\u0e32\u0e23\u0e40\u0e21\u0e37\u0e2d\u0e07"}
{"id": "line_1206", "truth_parsed_k": 291.53, "prediction_parsed_k_with_c_conv": 286.79, "prediction_parsed_k_strict": 286.79, "absolute_error_k_vs_c_conv_pred": 4.739999999999952, "score_vs_c_conv_pred": 1.401987317098674, "absolute_error_k_vs_strict_pred": 4.739999999999952, "score_vs_pred_strict": 1.401987317098674, "truth_raw_assistant_text": "The value is 291.53", "prediction_raw_text": "The value is 286.79\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e19"}
{"id": "line_1207", "truth_parsed_k": 291.96, "prediction_parsed_k_with_c_conv": 284.7, "prediction_parsed_k_strict": 284.7, "absolute_error_k_vs_c_conv_pred": 7.259999999999991, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.259999999999991, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.96", "prediction_raw_text": "The value is 284.70\u0e0d\u0e35\u0e48\u0e1b"}
{"id": "line_1208", "truth_parsed_k": 291.79, "prediction_parsed_k_with_c_conv": 284.98, "prediction_parsed_k_strict": 284.98, "absolute_error_k_vs_c_conv_pred": 6.810000000000002, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.810000000000002, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.79", "prediction_raw_text": "The value is 284.98\u0644\u064e"}
{"id": "line_1209", "truth_parsed_k": 291.16, "prediction_parsed_k_with_c_conv": 285.45, "prediction_parsed_k_strict": 285.45, "absolute_error_k_vs_c_conv_pred": 5.710000000000036, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.710000000000036, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.16", "prediction_raw_text": "The value is 285.45"}
{"id": "line_1210", "truth_parsed_k": 290.31, "prediction_parsed_k_with_c_conv": 285.71, "prediction_parsed_k_strict": 285.71, "absolute_error_k_vs_c_conv_pred": 4.600000000000023, "score_vs_c_conv_pred": 2.1882694779875256, "absolute_error_k_vs_strict_pred": 4.600000000000023, "score_vs_pred_strict": 2.1882694779875256, "truth_raw_assistant_text": "The value is 290.31", "prediction_raw_text": "The value is 285.71"}
{"id": "line_1211", "truth_parsed_k": 289.57, "prediction_parsed_k_with_c_conv": 284.81, "prediction_parsed_k_strict": 284.81, "absolute_error_k_vs_c_conv_pred": 4.759999999999991, "score_vs_c_conv_pred": 1.2915122870442164, "absolute_error_k_vs_strict_pred": 4.759999999999991, "score_vs_pred_strict": 1.2915122870442164, "truth_raw_assistant_text": "The value is 289.57", "prediction_raw_text": "The value is 284.81\u0e1e\u0e35\u0e48"}
{"id": "line_1212", "truth_parsed_k": 289.06, "prediction_parsed_k_with_c_conv": 284.66, "prediction_parsed_k_strict": 284.66, "absolute_error_k_vs_c_conv_pred": 4.399999999999977, "score_vs_c_conv_pred": 3.352915888962116, "absolute_error_k_vs_strict_pred": 4.399999999999977, "score_vs_pred_strict": 3.352915888962116, "truth_raw_assistant_text": "The value is 289.06", "prediction_raw_text": "The value is 284.66\u0e40\u0e23\u0e37\u0e2d"}
{"id": "line_1213", "truth_parsed_k": 288.6, "prediction_parsed_k_with_c_conv": 284.85, "prediction_parsed_k_strict": 284.85, "absolute_error_k_vs_c_conv_pred": 3.75, "score_vs_c_conv_pred": 7.528731291521307, "absolute_error_k_vs_strict_pred": 3.75, "score_vs_pred_strict": 7.528731291521307, "truth_raw_assistant_text": "The value is 288.60", "prediction_raw_text": "The value is 284.85\u0e19\u0e48\u0e32\u0e08\u0e30"}
{"id": "line_1214", "truth_parsed_k": 288.97, "prediction_parsed_k_with_c_conv": 285.88, "prediction_parsed_k_strict": 285.88, "absolute_error_k_vs_c_conv_pred": 3.090000000000032, "score_vs_c_conv_pred": 12.556728641830572, "absolute_error_k_vs_strict_pred": 3.090000000000032, "score_vs_pred_strict": 12.556728641830572, "truth_raw_assistant_text": "The value is 288.97", "prediction_raw_text": "The value is 285.88\u0e2d\u0e19\u0e38\u0e0d\u0e32"}
{"id": "line_1215", "truth_parsed_k": 289.62, "prediction_parsed_k_with_c_conv": 285.98, "prediction_parsed_k_strict": 285.98, "absolute_error_k_vs_c_conv_pred": 3.6399999999999864, "score_vs_c_conv_pred": 8.30420571442686, "absolute_error_k_vs_strict_pred": 3.6399999999999864, "score_vs_pred_strict": 8.30420571442686, "truth_raw_assistant_text": "The value is 289.62", "prediction_raw_text": "The value is 285.98\u0e40\u0e17\u0e35\u0e22\u0e21"}
{"id": "line_1216", "truth_parsed_k": 290.27, "prediction_parsed_k_with_c_conv": 287.49, "prediction_parsed_k_strict": 287.49, "absolute_error_k_vs_c_conv_pred": 2.7799999999999727, "score_vs_c_conv_pred": 15.287090825853355, "absolute_error_k_vs_strict_pred": 2.7799999999999727, "score_vs_pred_strict": 15.287090825853355, "truth_raw_assistant_text": "The value is 290.27", "prediction_raw_text": "The value is 287.49\u0e0b\u0e49\u0e33"}
{"id": "line_1217", "truth_parsed_k": 291.17, "prediction_parsed_k_with_c_conv": 285.62, "prediction_parsed_k_strict": 285.62, "absolute_error_k_vs_c_conv_pred": 5.550000000000011, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.550000000000011, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.17", "prediction_raw_text": "The value is 285.62 \u0e21\u0e35\u0e19\u0e32"}
{"id": "line_1218", "truth_parsed_k": 291.87, "prediction_parsed_k_with_c_conv": 285.72, "prediction_parsed_k_strict": 285.72, "absolute_error_k_vs_c_conv_pred": 6.149999999999977, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.149999999999977, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.87", "prediction_raw_text": "The value is 285.72\u0e2a\u0e34\u0e19\u0e04\u0e49\u0e32"}
{"id": "line_1219", "truth_parsed_k": 292.23, "prediction_parsed_k_with_c_conv": 286.9, "prediction_parsed_k_strict": 286.9, "absolute_error_k_vs_c_conv_pred": 5.330000000000041, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.330000000000041, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.23", "prediction_raw_text": "The value is 286.90 \u0e2d\u0e22\u0e48\u0e32\u0e07\u0e44\u0e23"}
{"id": "line_1220", "truth_parsed_k": 292.01, "prediction_parsed_k_with_c_conv": 285.99, "prediction_parsed_k_strict": 285.99, "absolute_error_k_vs_c_conv_pred": 6.019999999999982, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.019999999999982, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.01", "prediction_raw_text": "The value is 285.99 \u0e40\u0e0a\u0e48\u0e19"}
{"id": "line_1221", "truth_parsed_k": 291.49, "prediction_parsed_k_with_c_conv": 286.61, "prediction_parsed_k_strict": 286.61, "absolute_error_k_vs_c_conv_pred": 4.8799999999999955, "score_vs_c_conv_pred": 0.638014069249393, "absolute_error_k_vs_strict_pred": 4.8799999999999955, "score_vs_pred_strict": 0.638014069249393, "truth_raw_assistant_text": "The value is 291.49", "prediction_raw_text": "The value is 286.61\u0e1b\u0e23\u0e30\u0e2b\u0e22\u0e31\u0e14"}
{"id": "line_1222", "truth_parsed_k": 290.59, "prediction_parsed_k_with_c_conv": 288.76, "prediction_parsed_k_strict": 288.76, "absolute_error_k_vs_c_conv_pred": 1.829999999999984, "score_vs_c_conv_pred": 25.951841361259586, "absolute_error_k_vs_strict_pred": 1.829999999999984, "score_vs_pred_strict": 25.951841361259586, "truth_raw_assistant_text": "The value is 290.59", "prediction_raw_text": "The value is 288.76\u0e43\u0e19\u0e1e\u0e37\u0e49\u0e19\u0e17\u0e35\u0e48"}
{"id": "line_1223", "truth_parsed_k": 289.95, "prediction_parsed_k_with_c_conv": 284.91, "prediction_parsed_k_strict": 284.91, "absolute_error_k_vs_c_conv_pred": 5.039999999999964, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.039999999999964, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 289.95", "prediction_raw_text": "The value is 284.91\u0e40\u0e17\u0e49\u0e32"}
{"id": "line_1224", "truth_parsed_k": 289.42, "prediction_parsed_k_with_c_conv": 286.84, "prediction_parsed_k_strict": 286.84, "absolute_error_k_vs_c_conv_pred": 2.580000000000041, "score_vs_c_conv_pred": 17.207922755976888, "absolute_error_k_vs_strict_pred": 2.580000000000041, "score_vs_pred_strict": 17.207922755976888, "truth_raw_assistant_text": "The value is 289.42", "prediction_raw_text": "The value is 286.84\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e01\u0e31\u0e1a"}
{"id": "line_1225", "truth_parsed_k": 289.3, "prediction_parsed_k_with_c_conv": 286.84, "prediction_parsed_k_strict": 286.84, "absolute_error_k_vs_c_conv_pred": 2.4600000000000364, "score_vs_c_conv_pred": 18.429829952686394, "absolute_error_k_vs_strict_pred": 2.4600000000000364, "score_vs_pred_strict": 18.429829952686394, "truth_raw_assistant_text": "The value is 289.30", "prediction_raw_text": "The value is 286.84."}
{"id": "line_1226", "truth_parsed_k": 289.22, "prediction_parsed_k_with_c_conv": 286.38, "prediction_parsed_k_strict": 286.38, "absolute_error_k_vs_c_conv_pred": 2.840000000000032, "score_vs_c_conv_pred": 14.736579302753228, "absolute_error_k_vs_strict_pred": 2.840000000000032, "score_vs_pred_strict": 14.736579302753228, "truth_raw_assistant_text": "The value is 289.22", "prediction_raw_text": "The value is 286.38"}
{"id": "line_1227", "truth_parsed_k": 289.89, "prediction_parsed_k_with_c_conv": 284.91, "prediction_parsed_k_strict": 284.91, "absolute_error_k_vs_c_conv_pred": 4.979999999999961, "score_vs_c_conv_pred": 0.10529143739819169, "absolute_error_k_vs_strict_pred": 4.979999999999961, "score_vs_pred_strict": 0.10529143739819169, "truth_raw_assistant_text": "The value is 289.89", "prediction_raw_text": "The value is 284.91."}
{"id": "line_1228", "truth_parsed_k": 290.61, "prediction_parsed_k_with_c_conv": 285.51, "prediction_parsed_k_strict": 285.51, "absolute_error_k_vs_c_conv_pred": 5.100000000000023, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.100000000000023, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.61", "prediction_raw_text": "The value is 285.51\u0e15\u0e49\u0e2d\u0e19\u0e23\u0e31\u0e1a"}
{"id": "line_1229", "truth_parsed_k": 291.43, "prediction_parsed_k_with_c_conv": 285.92, "prediction_parsed_k_strict": 285.92, "absolute_error_k_vs_c_conv_pred": 5.509999999999991, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.509999999999991, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.43", "prediction_raw_text": "The value is 285.92\u0e40\u0e04\u0e23\u0e37\u0e2d\u0e02"}
{"id": "line_1230", "truth_parsed_k": 292.12, "prediction_parsed_k_with_c_conv": 285.9, "prediction_parsed_k_strict": 285.9, "absolute_error_k_vs_c_conv_pred": 6.220000000000027, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.220000000000027, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.12", "prediction_raw_text": "The value is 285.90"}
{"id": "line_1231", "truth_parsed_k": 292.51, "prediction_parsed_k_with_c_conv": 286.88, "prediction_parsed_k_strict": 286.88, "absolute_error_k_vs_c_conv_pred": 5.6299999999999955, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.6299999999999955, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.51", "prediction_raw_text": "The value is 286.88\u0e17\u0e35\u0e48\u0e40\u0e1b\u0e47\u0e19"}
{"id": "line_1232", "truth_parsed_k": 292.25, "prediction_parsed_k_with_c_conv": 285.78, "prediction_parsed_k_strict": 285.78, "absolute_error_k_vs_c_conv_pred": 6.470000000000027, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.470000000000027, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.25", "prediction_raw_text": "The value is 285.78\u0e0a\u0e49\u0e32"}
{"id": "line_1233", "truth_parsed_k": 291.56, "prediction_parsed_k_with_c_conv": 285.54, "prediction_parsed_k_strict": 285.54, "absolute_error_k_vs_c_conv_pred": 6.019999999999982, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.019999999999982, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.56", "prediction_raw_text": "The value is 285.54."}
{"id": "line_1234", "truth_parsed_k": 290.74, "prediction_parsed_k_with_c_conv": 285.4, "prediction_parsed_k_strict": 285.4, "absolute_error_k_vs_c_conv_pred": 5.340000000000032, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.340000000000032, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.74", "prediction_raw_text": "The value is 285.40\u0e25\u0e37\u0e21"}
{"id": "line_1235", "truth_parsed_k": 290.1, "prediction_parsed_k_with_c_conv": 286.81, "prediction_parsed_k_strict": 286.81, "absolute_error_k_vs_c_conv_pred": 3.2900000000000205, "score_vs_c_conv_pred": 10.931605061376516, "absolute_error_k_vs_strict_pred": 3.2900000000000205, "score_vs_pred_strict": 10.931605061376516, "truth_raw_assistant_text": "The value is 290.10", "prediction_raw_text": "The value is 286.81\u0e21\u0e34\u0e16\u0e38\u0e19"}
{"id": "line_1236", "truth_parsed_k": 289.45, "prediction_parsed_k_with_c_conv": 286.55, "prediction_parsed_k_strict": 286.55, "absolute_error_k_vs_c_conv_pred": 2.8999999999999773, "score_vs_c_conv_pred": 14.197097136438963, "absolute_error_k_vs_strict_pred": 2.8999999999999773, "score_vs_pred_strict": 14.197097136438963, "truth_raw_assistant_text": "The value is 289.45", "prediction_raw_text": "The value is 286.55\u0e08\u0e23\u0e34\u0e07\u0e46"}
{"id": "line_1237", "truth_parsed_k": 289.4, "prediction_parsed_k_with_c_conv": 285.49, "prediction_parsed_k_strict": 285.49, "absolute_error_k_vs_c_conv_pred": 3.909999999999968, "score_vs_c_conv_pred": 6.439197058986501, "absolute_error_k_vs_strict_pred": 3.909999999999968, "score_vs_pred_strict": 6.439197058986501, "truth_raw_assistant_text": "The value is 289.40", "prediction_raw_text": "The value is 285.49\u0e22\u0e48"}
{"id": "line_1238", "truth_parsed_k": 289.26, "prediction_parsed_k_with_c_conv": 284.74, "prediction_parsed_k_strict": 284.74, "absolute_error_k_vs_c_conv_pred": 4.519999999999982, "score_vs_c_conv_pred": 2.648101369578293, "absolute_error_k_vs_strict_pred": 4.519999999999982, "score_vs_pred_strict": 2.648101369578293, "truth_raw_assistant_text": "The value is 289.26", "prediction_raw_text": "The value is 284.74."}
{"id": "line_1239", "truth_parsed_k": 289.78, "prediction_parsed_k_with_c_conv": 284.96, "prediction_parsed_k_strict": 284.96, "absolute_error_k_vs_c_conv_pred": 4.819999999999993, "score_vs_c_conv_pred": 0.962780829551213, "absolute_error_k_vs_strict_pred": 4.819999999999993, "score_vs_pred_strict": 0.962780829551213, "truth_raw_assistant_text": "The value is 289.78", "prediction_raw_text": "The value is 284.96 \u0e01\u0e38\u0e21"}
{"id": "line_1240", "truth_parsed_k": 290.57, "prediction_parsed_k_with_c_conv": 285.86, "prediction_parsed_k_strict": 285.86, "absolute_error_k_vs_c_conv_pred": 4.7099999999999795, "score_vs_c_conv_pred": 1.5685543480219444, "absolute_error_k_vs_strict_pred": 4.7099999999999795, "score_vs_pred_strict": 1.5685543480219444, "truth_raw_assistant_text": "The value is 290.57", "prediction_raw_text": "The value is 285.86."}
{"id": "line_1241", "truth_parsed_k": 291.48, "prediction_parsed_k_with_c_conv": 285.56, "prediction_parsed_k_strict": 285.56, "absolute_error_k_vs_c_conv_pred": 5.920000000000016, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.920000000000016, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.48", "prediction_raw_text": "The value is 285.56\u0e14\u0e49\u0e27\u0e22\u0e04\u0e27\u0e32\u0e21"}
{"id": "line_1242", "truth_parsed_k": 292.23, "prediction_parsed_k_with_c_conv": 286.52, "prediction_parsed_k_strict": 286.52, "absolute_error_k_vs_c_conv_pred": 5.710000000000036, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.710000000000036, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.23", "prediction_raw_text": "The value is 286.52\u0e1b\u0e23\u0e30\u0e17\u0e31\u0e1a"}
{"id": "line_1243", "truth_parsed_k": 292.53, "prediction_parsed_k_with_c_conv": 285.94, "prediction_parsed_k_strict": 285.94, "absolute_error_k_vs_c_conv_pred": 6.589999999999975, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.589999999999975, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.53", "prediction_raw_text": "The value is 285.94\u0e43\u0e19\u0e1b\u0e35"}
{"id": "line_1244", "truth_parsed_k": 292.3, "prediction_parsed_k_with_c_conv": 285.5, "prediction_parsed_k_strict": 285.5, "absolute_error_k_vs_c_conv_pred": 6.800000000000011, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.800000000000011, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.30", "prediction_raw_text": "The value is 285.50\u0e23\u0e31\u0e1a\u0e23\u0e2d\u0e07"}
{"id": "line_1245", "truth_parsed_k": 291.58, "prediction_parsed_k_with_c_conv": 285.89, "prediction_parsed_k_strict": 285.89, "absolute_error_k_vs_c_conv_pred": 5.689999999999998, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.689999999999998, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.58", "prediction_raw_text": "The value is 285.89."}
{"id": "line_1246", "truth_parsed_k": 290.71, "prediction_parsed_k_with_c_conv": 286.56, "prediction_parsed_k_strict": 286.56, "absolute_error_k_vs_c_conv_pred": 4.149999999999977, "score_vs_c_conv_pred": 4.883341951586084, "absolute_error_k_vs_strict_pred": 4.149999999999977, "score_vs_pred_strict": 4.883341951586084, "truth_raw_assistant_text": "The value is 290.71", "prediction_raw_text": "The value is 286.56."}
{"id": "line_1247", "truth_parsed_k": 289.96, "prediction_parsed_k_with_c_conv": 285.9, "prediction_parsed_k_strict": 285.9, "absolute_error_k_vs_c_conv_pred": 4.060000000000002, "score_vs_c_conv_pred": 5.456305073482948, "absolute_error_k_vs_strict_pred": 4.060000000000002, "score_vs_pred_strict": 5.456305073482948, "truth_raw_assistant_text": "The value is 289.96", "prediction_raw_text": "The value is 285.90\u0e40\u0e04\u0e25\u0e47\u0e14"}
{"id": "line_1248", "truth_parsed_k": 289.33, "prediction_parsed_k_with_c_conv": 288.74, "prediction_parsed_k_strict": 288.74, "absolute_error_k_vs_c_conv_pred": 0.589999999999975, "score_vs_c_conv_pred": 53.03797060980708, "absolute_error_k_vs_strict_pred": 0.589999999999975, "score_vs_pred_strict": 53.03797060980708, "truth_raw_assistant_text": "The value is 289.33", "prediction_raw_text": "The value is 288.74\uf98e"}
{"id": "line_1249", "truth_parsed_k": 289.0, "prediction_parsed_k_with_c_conv": 285.79, "prediction_parsed_k_strict": 285.79, "absolute_error_k_vs_c_conv_pred": 3.2099999999999795, "score_vs_c_conv_pred": 11.569934228803058, "absolute_error_k_vs_strict_pred": 3.2099999999999795, "score_vs_pred_strict": 11.569934228803058, "truth_raw_assistant_text": "The value is 289.00", "prediction_raw_text": "The value is 285.79\u0e40\u0e08\u0e23\u0e34"}
{"id": "line_1250", "truth_parsed_k": 289.2, "prediction_parsed_k_with_c_conv": 284.92, "prediction_parsed_k_strict": 284.92, "absolute_error_k_vs_c_conv_pred": 4.279999999999973, "score_vs_c_conv_pred": 4.076675042708677, "absolute_error_k_vs_strict_pred": 4.279999999999973, "score_vs_pred_strict": 4.076675042708677, "truth_raw_assistant_text": "The value is 289.20", "prediction_raw_text": "The value is 284.92\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27"}
{"id": "line_1251", "truth_parsed_k": 289.68, "prediction_parsed_k_with_c_conv": 285.99, "prediction_parsed_k_strict": 285.99, "absolute_error_k_vs_c_conv_pred": 3.6899999999999977, "score_vs_c_conv_pred": 7.9489465933544, "absolute_error_k_vs_strict_pred": 3.6899999999999977, "score_vs_pred_strict": 7.9489465933544, "truth_raw_assistant_text": "The value is 289.68", "prediction_raw_text": "The value is 285.99\u00e2\u0301"}
{"id": "line_1252", "truth_parsed_k": 290.39, "prediction_parsed_k_with_c_conv": 285.76, "prediction_parsed_k_strict": 285.76, "absolute_error_k_vs_c_conv_pred": 4.6299999999999955, "score_vs_c_conv_pred": 2.0178369419290965, "absolute_error_k_vs_strict_pred": 4.6299999999999955, "score_vs_pred_strict": 2.0178369419290965, "truth_raw_assistant_text": "The value is 290.39", "prediction_raw_text": "The value is 285.76."}
{"id": "line_1253", "truth_parsed_k": 291.31, "prediction_parsed_k_with_c_conv": 285.77, "prediction_parsed_k_strict": 285.77, "absolute_error_k_vs_c_conv_pred": 5.5400000000000205, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.5400000000000205, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.31", "prediction_raw_text": "The value is 285.77\u0e15\u0e39\u0e49"}
{"id": "line_1254", "truth_parsed_k": 291.97, "prediction_parsed_k_with_c_conv": 286.36, "prediction_parsed_k_strict": 286.36, "absolute_error_k_vs_c_conv_pred": 5.610000000000014, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.610000000000014, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.97", "prediction_raw_text": "The value is 286.36\u0e40\u0e2a\u0e35\u0e48\u0e22\u0e07"}
{"id": "line_1255", "truth_parsed_k": 292.24, "prediction_parsed_k_with_c_conv": 285.47, "prediction_parsed_k_strict": 285.47, "absolute_error_k_vs_c_conv_pred": 6.769999999999982, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.769999999999982, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.24", "prediction_raw_text": "The value is 285.47\u0e2a\u0e23\u0e38"}
{"id": "line_1256", "truth_parsed_k": 292.01, "prediction_parsed_k_with_c_conv": 285.74, "prediction_parsed_k_strict": 285.74, "absolute_error_k_vs_c_conv_pred": 6.269999999999982, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.269999999999982, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.01", "prediction_raw_text": "The value is 285.74."}
{"id": "line_1257", "truth_parsed_k": 291.38, "prediction_parsed_k_with_c_conv": 285.61, "prediction_parsed_k_strict": 285.61, "absolute_error_k_vs_c_conv_pred": 5.769999999999982, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.769999999999982, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.38", "prediction_raw_text": "The value is 285.61\u0e1f\u0e2d\u0e23\u0e4c\u0e21"}
{"id": "line_1258", "truth_parsed_k": 290.55, "prediction_parsed_k_with_c_conv": 285.88, "prediction_parsed_k_strict": 285.88, "absolute_error_k_vs_c_conv_pred": 4.670000000000016, "score_vs_c_conv_pred": 1.7922586526814, "absolute_error_k_vs_strict_pred": 4.670000000000016, "score_vs_pred_strict": 1.7922586526814, "truth_raw_assistant_text": "The value is 290.55", "prediction_raw_text": "The value is 285.88 \u0e01\u0e47"}
{"id": "line_1259", "truth_parsed_k": 289.91, "prediction_parsed_k_with_c_conv": 284.98, "prediction_parsed_k_strict": 284.98, "absolute_error_k_vs_c_conv_pred": 4.930000000000007, "score_vs_c_conv_pred": 0.3703354157400529, "absolute_error_k_vs_strict_pred": 4.930000000000007, "score_vs_pred_strict": 0.3703354157400529, "truth_raw_assistant_text": "The value is 289.91", "prediction_raw_text": "The value is 284.98"}
{"id": "line_1260", "truth_parsed_k": 289.3, "prediction_parsed_k_with_c_conv": 285.94, "prediction_parsed_k_strict": 285.94, "absolute_error_k_vs_c_conv_pred": 3.3600000000000136, "score_vs_c_conv_pred": 10.385216009938524, "absolute_error_k_vs_strict_pred": 3.3600000000000136, "score_vs_pred_strict": 10.385216009938524, "truth_raw_assistant_text": "The value is 289.30", "prediction_raw_text": "The value is 285.94."}
{"id": "line_1261", "truth_parsed_k": 288.92, "prediction_parsed_k_with_c_conv": 285.86, "prediction_parsed_k_strict": 285.86, "absolute_error_k_vs_c_conv_pred": 3.0600000000000023, "score_vs_c_conv_pred": 12.809182842181798, "absolute_error_k_vs_strict_pred": 3.0600000000000023, "score_vs_pred_strict": 12.809182842181798, "truth_raw_assistant_text": "The value is 288.92", "prediction_raw_text": "The value is 285.86\ufa00"}
{"id": "line_1262", "truth_parsed_k": 289.09, "prediction_parsed_k_with_c_conv": 285.6, "prediction_parsed_k_strict": 285.6, "absolute_error_k_vs_c_conv_pred": 3.4899999999999523, "score_vs_c_conv_pred": 9.39900083111943, "absolute_error_k_vs_strict_pred": 3.4899999999999523, "score_vs_pred_strict": 9.39900083111943, "truth_raw_assistant_text": "The value is 289.09", "prediction_raw_text": "The value is 285.60\u0e08\u0e30\u0e21\u0e35"}
{"id": "line_1263", "truth_parsed_k": 289.28, "prediction_parsed_k_with_c_conv": 287.8, "prediction_parsed_k_strict": 287.8, "absolute_error_k_vs_c_conv_pred": 1.4799999999999613, "score_vs_c_conv_pred": 31.263881448593455, "absolute_error_k_vs_strict_pred": 1.4799999999999613, "score_vs_pred_strict": 31.263881448593455, "truth_raw_assistant_text": "The value is 289.28", "prediction_raw_text": "The value is 287.80"}
{"id": "line_1264", "truth_parsed_k": 290.54, "prediction_parsed_k_with_c_conv": 286.81, "prediction_parsed_k_strict": 286.81, "absolute_error_k_vs_c_conv_pred": 3.730000000000018, "score_vs_c_conv_pred": 7.668075720471757, "absolute_error_k_vs_strict_pred": 3.730000000000018, "score_vs_pred_strict": 7.668075720471757, "truth_raw_assistant_text": "The value is 290.54", "prediction_raw_text": "The value is 286.81\u0e2a\u0e19\u0e31\u0e1a\u0e2a\u0e19\u0e38\u0e19"}
{"id": "line_1265", "truth_parsed_k": 291.26, "prediction_parsed_k_with_c_conv": 284.89, "prediction_parsed_k_strict": 284.89, "absolute_error_k_vs_c_conv_pred": 6.3700000000000045, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.3700000000000045, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.26", "prediction_raw_text": "The value is 284.89."}
{"id": "line_1266", "truth_parsed_k": 292.0, "prediction_parsed_k_with_c_conv": 285.97, "prediction_parsed_k_strict": 285.97, "absolute_error_k_vs_c_conv_pred": 6.029999999999973, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.029999999999973, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.00", "prediction_raw_text": "The value is 285.97\u0e08\u0e31"}
{"id": "line_1267", "truth_parsed_k": 292.23, "prediction_parsed_k_with_c_conv": 286.98, "prediction_parsed_k_strict": 286.98, "absolute_error_k_vs_c_conv_pred": 5.25, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.25, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.23", "prediction_raw_text": "The value is 286.98\u0e14\u0e48\u0e27\u0e19"}
{"id": "line_1268", "truth_parsed_k": 292.08, "prediction_parsed_k_with_c_conv": 286.92, "prediction_parsed_k_strict": 286.92, "absolute_error_k_vs_c_conv_pred": 5.159999999999968, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.159999999999968, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.08", "prediction_raw_text": "The value is 286.92\u45f4"}
{"id": "line_1269", "truth_parsed_k": 291.39, "prediction_parsed_k_with_c_conv": 285.46, "prediction_parsed_k_strict": 285.46, "absolute_error_k_vs_c_conv_pred": 5.930000000000007, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.930000000000007, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.39", "prediction_raw_text": "The value is 285.46."}
{"id": "line_1270", "truth_parsed_k": 290.56, "prediction_parsed_k_with_c_conv": 285.99, "prediction_parsed_k_strict": 285.99, "absolute_error_k_vs_c_conv_pred": 4.569999999999993, "score_vs_c_conv_pred": 2.359787578917749, "absolute_error_k_vs_strict_pred": 4.569999999999993, "score_vs_pred_strict": 2.359787578917749, "truth_raw_assistant_text": "The value is 290.56", "prediction_raw_text": "The value is 285.99\u0e40\u0e21\u0e19\u0e39"}
{"id": "line_1271", "truth_parsed_k": 289.87, "prediction_parsed_k_with_c_conv": 285.92, "prediction_parsed_k_strict": 285.92, "absolute_error_k_vs_c_conv_pred": 3.9499999999999886, "score_vs_c_conv_pred": 6.173564810743271, "absolute_error_k_vs_strict_pred": 3.9499999999999886, "score_vs_pred_strict": 6.173564810743271, "truth_raw_assistant_text": "The value is 289.87", "prediction_raw_text": "The value is 285.92\u0e08\u0e30\u0e17\u0e33\u0e43\u0e2b\u0e49"}
{"id": "line_1272", "truth_parsed_k": 289.33, "prediction_parsed_k_with_c_conv": 286.82, "prediction_parsed_k_strict": 286.82, "absolute_error_k_vs_c_conv_pred": 2.509999999999991, "score_vs_c_conv_pred": 17.91394730146002, "absolute_error_k_vs_strict_pred": 2.509999999999991, "score_vs_pred_strict": 17.91394730146002, "truth_raw_assistant_text": "The value is 289.33", "prediction_raw_text": "The value is 286.82\u0e01\u0e32\u0e23\u0e28\u0e36\u0e01\u0e29\u0e32"}
{"id": "line_1273", "truth_parsed_k": 288.99, "prediction_parsed_k_with_c_conv": 286.96, "prediction_parsed_k_strict": 286.96, "absolute_error_k_vs_c_conv_pred": 2.0300000000000296, "score_vs_c_conv_pred": 23.329015457551083, "absolute_error_k_vs_strict_pred": 2.0300000000000296, "score_vs_pred_strict": 23.329015457551083, "truth_raw_assistant_text": "The value is 288.99", "prediction_raw_text": "The value is 286.96."}
{"id": "line_1274", "truth_parsed_k": 289.07, "prediction_parsed_k_with_c_conv": 286.86, "prediction_parsed_k_strict": 286.86, "absolute_error_k_vs_c_conv_pred": 2.2099999999999795, "score_vs_c_conv_pred": 21.16879973737197, "absolute_error_k_vs_strict_pred": 2.2099999999999795, "score_vs_pred_strict": 21.16879973737197, "truth_raw_assistant_text": "The value is 289.07", "prediction_raw_text": "The value is 286.86 \u0e01\u0e38\u0e21\u0e20\u0e32\u0e1e\u0e31\u0e19"}
{"id": "line_1275", "truth_parsed_k": 289.64, "prediction_parsed_k_with_c_conv": 284.95, "prediction_parsed_k_strict": 284.95, "absolute_error_k_vs_c_conv_pred": 4.689999999999998, "score_vs_c_conv_pred": 1.6801742003018982, "absolute_error_k_vs_strict_pred": 4.689999999999998, "score_vs_pred_strict": 1.6801742003018982, "truth_raw_assistant_text": "The value is 289.64", "prediction_raw_text": "The value is 284.95."}
{"id": "line_1276", "truth_parsed_k": 290.55, "prediction_parsed_k_with_c_conv": 288.98, "prediction_parsed_k_strict": 288.98, "absolute_error_k_vs_c_conv_pred": 1.5699999999999932, "score_vs_c_conv_pred": 29.794703436952663, "absolute_error_k_vs_strict_pred": 1.5699999999999932, "score_vs_pred_strict": 29.794703436952663, "truth_raw_assistant_text": "The value is 290.55", "prediction_raw_text": "The value is 288.98\u0e2d\u0e22\u0e32\u0e01\u0e43\u0e2b\u0e49"}
{"id": "line_1277", "truth_parsed_k": 291.37, "prediction_parsed_k_with_c_conv": 285.87, "prediction_parsed_k_strict": 285.87, "absolute_error_k_vs_c_conv_pred": 5.5, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.5, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.37", "prediction_raw_text": "The value is 285.87"}
{"id": "line_1278", "truth_parsed_k": 292.02, "prediction_parsed_k_with_c_conv": 286.86, "prediction_parsed_k_strict": 286.86, "absolute_error_k_vs_c_conv_pred": 5.159999999999968, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.159999999999968, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.02", "prediction_raw_text": "The value is 286.86."}
{"id": "line_1279", "truth_parsed_k": 292.39, "prediction_parsed_k_with_c_conv": 285.9, "prediction_parsed_k_strict": 285.9, "absolute_error_k_vs_c_conv_pred": 6.490000000000009, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.490000000000009, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.39", "prediction_raw_text": "The value is 285.90\u0e2d\u0e07\u0e04\u0e4c\u0e01\u0e23"}
{"id": "line_1280", "truth_parsed_k": 292.16, "prediction_parsed_k_with_c_conv": 286.9, "prediction_parsed_k_strict": 286.9, "absolute_error_k_vs_c_conv_pred": 5.260000000000048, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.260000000000048, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.16", "prediction_raw_text": "The value is 286.90"}
{"id": "line_1281", "truth_parsed_k": 291.41, "prediction_parsed_k_with_c_conv": 284.69, "prediction_parsed_k_strict": 284.69, "absolute_error_k_vs_c_conv_pred": 6.720000000000027, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.720000000000027, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.41", "prediction_raw_text": "The value is 284.69\u0e2a\u0e31\u0e1b\u0e14\u0e32\u0e2b\u0e4c"}
{"id": "line_1282", "truth_parsed_k": 290.66, "prediction_parsed_k_with_c_conv": 285.97, "prediction_parsed_k_strict": 285.97, "absolute_error_k_vs_c_conv_pred": 4.689999999999998, "score_vs_c_conv_pred": 1.6801742003018982, "absolute_error_k_vs_strict_pred": 4.689999999999998, "score_vs_pred_strict": 1.6801742003018982, "truth_raw_assistant_text": "The value is 290.66", "prediction_raw_text": "The value is 285.97\u0e0a\u0e31\u0e14"}
{"id": "line_1283", "truth_parsed_k": 289.91, "prediction_parsed_k_with_c_conv": 285.54, "prediction_parsed_k_strict": 285.54, "absolute_error_k_vs_c_conv_pred": 4.3700000000000045, "score_vs_c_conv_pred": 3.5320401690034364, "absolute_error_k_vs_strict_pred": 4.3700000000000045, "score_vs_pred_strict": 3.5320401690034364, "truth_raw_assistant_text": "The value is 289.91", "prediction_raw_text": "The value is 285.54."}
{"id": "line_1284", "truth_parsed_k": 289.19, "prediction_parsed_k_with_c_conv": 286.92, "prediction_parsed_k_strict": 286.92, "absolute_error_k_vs_c_conv_pred": 2.269999999999982, "score_vs_c_conv_pred": 20.4855939375035, "absolute_error_k_vs_strict_pred": 2.269999999999982, "score_vs_pred_strict": 20.4855939375035, "truth_raw_assistant_text": "The value is 289.19", "prediction_raw_text": "The value is 286.92\u0e2d\u0e38\u0e1b"}
{"id": "line_1285", "truth_parsed_k": 289.21, "prediction_parsed_k_with_c_conv": 284.94862683317, "prediction_parsed_k_strict": 284.94862683317, "absolute_error_k_vs_c_conv_pred": 4.261373166829969, "score_vs_c_conv_pred": 4.190784357916666, "absolute_error_k_vs_strict_pred": 4.261373166829969, "score_vs_pred_strict": 4.190784357916666, "truth_raw_assistant_text": "The value is 289.21", "prediction_raw_text": "The value is 284.94862683317."}
{"id": "line_1286", "truth_parsed_k": 289.04, "prediction_parsed_k_with_c_conv": 285.96, "prediction_parsed_k_strict": 285.96, "absolute_error_k_vs_c_conv_pred": 3.080000000000041, "score_vs_c_conv_pred": 12.640617341470406, "absolute_error_k_vs_strict_pred": 3.080000000000041, "score_vs_pred_strict": 12.640617341470406, "truth_raw_assistant_text": "The value is 289.04", "prediction_raw_text": "The value is 285.96."}
{"id": "line_1287", "truth_parsed_k": 289.65, "prediction_parsed_k_with_c_conv": 286.64, "prediction_parsed_k_strict": 286.64, "absolute_error_k_vs_c_conv_pred": 3.009999999999991, "score_vs_c_conv_pred": 13.235271649804936, "absolute_error_k_vs_strict_pred": 3.009999999999991, "score_vs_pred_strict": 13.235271649804936, "truth_raw_assistant_text": "The value is 289.65", "prediction_raw_text": "The value is 286.64."}
{"id": "line_1288", "truth_parsed_k": 290.61, "prediction_parsed_k_with_c_conv": 285.59, "prediction_parsed_k_strict": 285.59, "absolute_error_k_vs_c_conv_pred": 5.020000000000039, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.020000000000039, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.61", "prediction_raw_text": "The value is 285.59."}
{"id": "line_1289", "truth_parsed_k": 291.42, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 6.600000000000023, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.600000000000023, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.42", "prediction_raw_text": "The value is 284.82\u0e40\u0e2b\u0e25\u0e48\u0e32"}
{"id": "line_1290", "truth_parsed_k": 292.1, "prediction_parsed_k_with_c_conv": 285.65, "prediction_parsed_k_strict": 285.65, "absolute_error_k_vs_c_conv_pred": 6.4500000000000455, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.4500000000000455, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.10", "prediction_raw_text": "The value is 285.65."}
{"id": "line_1291", "truth_parsed_k": 292.42, "prediction_parsed_k_with_c_conv": 285.66, "prediction_parsed_k_strict": 285.66, "absolute_error_k_vs_c_conv_pred": 6.759999999999991, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.759999999999991, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.42", "prediction_raw_text": "The value is 285.66\u0e41\u0e02\u0e48\u0e07"}
{"id": "line_1292", "truth_parsed_k": 292.42, "prediction_parsed_k_with_c_conv": 285.68, "prediction_parsed_k_strict": 285.68, "absolute_error_k_vs_c_conv_pred": 6.740000000000009, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.740000000000009, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.42", "prediction_raw_text": "The value is 285.68\u0e21\u0e31\u0e48\u0e19\u0e43\u0e08"}
{"id": "line_1293", "truth_parsed_k": 291.71, "prediction_parsed_k_with_c_conv": 285.82, "prediction_parsed_k_strict": 285.82, "absolute_error_k_vs_c_conv_pred": 5.889999999999986, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.889999999999986, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.71", "prediction_raw_text": "The value is 285.82\u0e01\u0e23\u0e13\u0e35"}
{"id": "line_1294", "truth_parsed_k": 290.88, "prediction_parsed_k_with_c_conv": 286.29, "prediction_parsed_k_strict": 286.29, "absolute_error_k_vs_c_conv_pred": 4.589999999999975, "score_vs_c_conv_pred": 2.2453208781553435, "absolute_error_k_vs_strict_pred": 4.589999999999975, "score_vs_pred_strict": 2.2453208781553435, "truth_raw_assistant_text": "The value is 290.88", "prediction_raw_text": "The value is 286.29."}
{"id": "line_1295", "truth_parsed_k": 290.07, "prediction_parsed_k_with_c_conv": 285.58, "prediction_parsed_k_strict": 285.58, "absolute_error_k_vs_c_conv_pred": 4.490000000000009, "score_vs_c_conv_pred": 2.822583086765784, "absolute_error_k_vs_strict_pred": 4.490000000000009, "score_vs_pred_strict": 2.822583086765784, "truth_raw_assistant_text": "The value is 290.07", "prediction_raw_text": "The value is 285.58\u0e44\u0e21\u0e48\u0e40\u0e04\u0e22"}
{"id": "line_1296", "truth_parsed_k": 289.57, "prediction_parsed_k_with_c_conv": 288.06, "prediction_parsed_k_strict": 288.06, "absolute_error_k_vs_c_conv_pred": 1.509999999999991, "score_vs_c_conv_pred": 30.765195885620365, "absolute_error_k_vs_strict_pred": 1.509999999999991, "score_vs_pred_strict": 30.765195885620365, "truth_raw_assistant_text": "The value is 289.57", "prediction_raw_text": "The value is 288.06."}
{"id": "line_1297", "truth_parsed_k": 289.24, "prediction_parsed_k_with_c_conv": 284.95, "prediction_parsed_k_strict": 284.95, "absolute_error_k_vs_c_conv_pred": 4.2900000000000205, "score_vs_c_conv_pred": 4.01561322333206, "absolute_error_k_vs_strict_pred": 4.2900000000000205, "score_vs_pred_strict": 4.01561322333206, "truth_raw_assistant_text": "The value is 289.24", "prediction_raw_text": "The value is 284.95."}
{"id": "line_1298", "truth_parsed_k": 289.54, "prediction_parsed_k_with_c_conv": 286.7, "prediction_parsed_k_strict": 286.7, "absolute_error_k_vs_c_conv_pred": 2.840000000000032, "score_vs_c_conv_pred": 14.736579302753228, "absolute_error_k_vs_strict_pred": 2.840000000000032, "score_vs_pred_strict": 14.736579302753228, "truth_raw_assistant_text": "The value is 289.54", "prediction_raw_text": "The value is 286.70\u0e25\u0e31\u0e01\u0e29\u0e13\u0e30"}
{"id": "line_1299", "truth_parsed_k": 289.76, "prediction_parsed_k_with_c_conv": 284.7, "prediction_parsed_k_strict": 284.7, "absolute_error_k_vs_c_conv_pred": 5.060000000000002, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.060000000000002, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 289.76", "prediction_raw_text": "The value is 284.70\u0e43\u0e15\u0e49"}
{"id": "line_1300", "truth_parsed_k": 290.59, "prediction_parsed_k_with_c_conv": 286.56, "prediction_parsed_k_strict": 286.56, "absolute_error_k_vs_c_conv_pred": 4.029999999999973, "score_vs_c_conv_pred": 5.650034332126874, "absolute_error_k_vs_strict_pred": 4.029999999999973, "score_vs_pred_strict": 5.650034332126874, "truth_raw_assistant_text": "The value is 290.59", "prediction_raw_text": "The value is 286.56."}
{"id": "line_1301", "truth_parsed_k": 291.44, "prediction_parsed_k_with_c_conv": 286.97, "prediction_parsed_k_strict": 286.97, "absolute_error_k_vs_c_conv_pred": 4.46999999999997, "score_vs_c_conv_pred": 2.939535499945378, "absolute_error_k_vs_strict_pred": 4.46999999999997, "score_vs_pred_strict": 2.939535499945378, "truth_raw_assistant_text": "The value is 291.44", "prediction_raw_text": "The value is 286.97\u0e17\u0e31\u0e28\u0e19"}
{"id": "line_1302", "truth_parsed_k": 292.14, "prediction_parsed_k_with_c_conv": 285.42, "prediction_parsed_k_strict": 285.42, "absolute_error_k_vs_c_conv_pred": 6.71999999999997, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.71999999999997, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.14", "prediction_raw_text": "The value is 285.42\u0e1a\u0e49\u0e32\u0e07"}
{"id": "line_1303", "truth_parsed_k": 292.57, "prediction_parsed_k_with_c_conv": 285.94, "prediction_parsed_k_strict": 285.94, "absolute_error_k_vs_c_conv_pred": 6.6299999999999955, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.6299999999999955, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.57", "prediction_raw_text": "The value is 285.94."}
{"id": "line_1304", "truth_parsed_k": 292.38, "prediction_parsed_k_with_c_conv": 285.98, "prediction_parsed_k_strict": 285.98, "absolute_error_k_vs_c_conv_pred": 6.399999999999977, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.399999999999977, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.38", "prediction_raw_text": "The value is 285.98\u0e20\u0e32\u0e1e\u0e22\u0e19\u0e15\u0e23\u0e4c"}
{"id": "line_1305", "truth_parsed_k": 291.65, "prediction_parsed_k_with_c_conv": 285.55, "prediction_parsed_k_strict": 285.55, "absolute_error_k_vs_c_conv_pred": 6.099999999999966, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.099999999999966, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.65", "prediction_raw_text": "The value is 285.55\u0e2d\u0e31\u0e15\u0e42\u0e19\u0e21\u0e31\u0e15\u0e34"}
{"id": "line_1306", "truth_parsed_k": 290.79, "prediction_parsed_k_with_c_conv": 286.7, "prediction_parsed_k_strict": 286.7, "absolute_error_k_vs_c_conv_pred": 4.090000000000032, "score_vs_c_conv_pred": 5.2639596070672035, "absolute_error_k_vs_strict_pred": 4.090000000000032, "score_vs_pred_strict": 5.2639596070672035, "truth_raw_assistant_text": "The value is 290.79", "prediction_raw_text": "The value is 286.70\u0e2b\u0e38\u0e49\u0e19"}
{"id": "line_1307", "truth_parsed_k": 289.99, "prediction_parsed_k_with_c_conv": 285.9, "prediction_parsed_k_strict": 285.9, "absolute_error_k_vs_c_conv_pred": 4.090000000000032, "score_vs_c_conv_pred": 5.2639596070672035, "absolute_error_k_vs_strict_pred": 4.090000000000032, "score_vs_pred_strict": 5.2639596070672035, "truth_raw_assistant_text": "The value is 289.99", "prediction_raw_text": "The value is 285.90\u0e17\u0e31\u0e01\u0e29\u0e30"}
{"id": "line_1308", "truth_parsed_k": 289.54, "prediction_parsed_k_with_c_conv": 285.6, "prediction_parsed_k_strict": 285.6, "absolute_error_k_vs_c_conv_pred": 3.9399999999999977, "score_vs_c_conv_pred": 6.239727622422064, "absolute_error_k_vs_strict_pred": 3.9399999999999977, "score_vs_pred_strict": 6.239727622422064, "truth_raw_assistant_text": "The value is 289.54", "prediction_raw_text": "The value is 285.60\u0e25\u0e31\u0e1a"}
{"id": "line_1309", "truth_parsed_k": 289.01, "prediction_parsed_k_with_c_conv": 285.95, "prediction_parsed_k_strict": 285.95, "absolute_error_k_vs_c_conv_pred": 3.0600000000000023, "score_vs_c_conv_pred": 12.809182842181798, "absolute_error_k_vs_strict_pred": 3.0600000000000023, "score_vs_pred_strict": 12.809182842181798, "truth_raw_assistant_text": "The value is 289.01", "prediction_raw_text": "The value is 285.95."}
{"id": "line_1310", "truth_parsed_k": 289.26, "prediction_parsed_k_with_c_conv": 285.91, "prediction_parsed_k_strict": 285.91, "absolute_error_k_vs_c_conv_pred": 3.349999999999966, "score_vs_c_conv_pred": 10.462596094238219, "absolute_error_k_vs_strict_pred": 3.349999999999966, "score_vs_pred_strict": 10.462596094238219, "truth_raw_assistant_text": "The value is 289.26", "prediction_raw_text": "The value is 285.91"}
{"id": "line_1311", "truth_parsed_k": 289.69, "prediction_parsed_k_with_c_conv": 285.56, "prediction_parsed_k_strict": 285.56, "absolute_error_k_vs_c_conv_pred": 4.1299999999999955, "score_vs_c_conv_pred": 5.009617685007994, "absolute_error_k_vs_strict_pred": 4.1299999999999955, "score_vs_pred_strict": 5.009617685007994, "truth_raw_assistant_text": "The value is 289.69", "prediction_raw_text": "The value is 285.56\u0e43\u0e19\u0e02\u0e13\u0e30\u0e17\u0e35\u0e48"}
{"id": "line_1312", "truth_parsed_k": 290.7, "prediction_parsed_k_with_c_conv": 286.32, "prediction_parsed_k_strict": 286.32, "absolute_error_k_vs_c_conv_pred": 4.3799999999999955, "score_vs_c_conv_pred": 3.4721995870300892, "absolute_error_k_vs_strict_pred": 4.3799999999999955, "score_vs_pred_strict": 3.4721995870300892, "truth_raw_assistant_text": "The value is 290.70", "prediction_raw_text": "The value is 286.32."}
{"id": "line_1313", "truth_parsed_k": 291.36, "prediction_parsed_k_with_c_conv": 286.92, "prediction_parsed_k_strict": 286.92, "absolute_error_k_vs_c_conv_pred": 4.439999999999998, "score_vs_c_conv_pred": 3.115922052920661, "absolute_error_k_vs_strict_pred": 4.439999999999998, "score_vs_pred_strict": 3.115922052920661, "truth_raw_assistant_text": "The value is 291.36", "prediction_raw_text": "The value is 286.92"}
{"id": "line_1314", "truth_parsed_k": 292.16, "prediction_parsed_k_with_c_conv": 285.54, "prediction_parsed_k_strict": 285.54, "absolute_error_k_vs_c_conv_pred": 6.6200000000000045, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.6200000000000045, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.16", "prediction_raw_text": "The value is 285.54"}
{"id": "line_1315", "truth_parsed_k": 292.5, "prediction_parsed_k_with_c_conv": 285.86, "prediction_parsed_k_strict": 285.86, "absolute_error_k_vs_c_conv_pred": 6.639999999999986, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.639999999999986, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.50", "prediction_raw_text": "The value is 285.86\u0e44\u0e14\u0e49\u0e23\u0e31\u0e1a\u0e01\u0e32\u0e23"}
{"id": "line_1316", "truth_parsed_k": 292.37, "prediction_parsed_k_with_c_conv": 285.59, "prediction_parsed_k_strict": 285.59, "absolute_error_k_vs_c_conv_pred": 6.78000000000003, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.78000000000003, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.37", "prediction_raw_text": "The value is 285.59."}
{"id": "line_1317", "truth_parsed_k": 291.65, "prediction_parsed_k_with_c_conv": 286.36, "prediction_parsed_k_strict": 286.36, "absolute_error_k_vs_c_conv_pred": 5.289999999999964, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.289999999999964, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.65", "prediction_raw_text": "The value is 286.36."}
{"id": "line_1318", "truth_parsed_k": 290.65, "prediction_parsed_k_with_c_conv": 285.72, "prediction_parsed_k_strict": 285.72, "absolute_error_k_vs_c_conv_pred": 4.92999999999995, "score_vs_c_conv_pred": 0.37033541574035267, "absolute_error_k_vs_strict_pred": 4.92999999999995, "score_vs_pred_strict": 0.37033541574035267, "truth_raw_assistant_text": "The value is 290.65", "prediction_raw_text": "The value is 285.72\u0e04\u0e27\u0e1a\u0e04\u0e38"}
{"id": "line_1319", "truth_parsed_k": 289.91, "prediction_parsed_k_with_c_conv": 285.96, "prediction_parsed_k_strict": 285.96, "absolute_error_k_vs_c_conv_pred": 3.9500000000000455, "score_vs_c_conv_pred": 6.173564810742905, "absolute_error_k_vs_strict_pred": 3.9500000000000455, "score_vs_pred_strict": 6.173564810742905, "truth_raw_assistant_text": "The value is 289.91", "prediction_raw_text": "The value is 285.96\u0e2d\u0e07\u0e04\u0e4c\u0e01\u0e23"}
{"id": "line_1320", "truth_parsed_k": 289.51, "prediction_parsed_k_with_c_conv": 286.3, "prediction_parsed_k_strict": 286.3, "absolute_error_k_vs_c_conv_pred": 3.2099999999999795, "score_vs_c_conv_pred": 11.569934228803058, "absolute_error_k_vs_strict_pred": 3.2099999999999795, "score_vs_pred_strict": 11.569934228803058, "truth_raw_assistant_text": "The value is 289.51", "prediction_raw_text": "The value is 286.30\ufb4e"}
{"id": "line_1321", "truth_parsed_k": 289.18, "prediction_parsed_k_with_c_conv": 285.88, "prediction_parsed_k_strict": 285.88, "absolute_error_k_vs_c_conv_pred": 3.3000000000000114, "score_vs_c_conv_pred": 10.852867429820945, "absolute_error_k_vs_strict_pred": 3.3000000000000114, "score_vs_pred_strict": 10.852867429820945, "truth_raw_assistant_text": "The value is 289.18", "prediction_raw_text": "The value is 285.88\u0e1e\u0e39\u0e14"}
{"id": "line_1322", "truth_parsed_k": 289.41, "prediction_parsed_k_with_c_conv": 285.53, "prediction_parsed_k_strict": 285.53, "absolute_error_k_vs_c_conv_pred": 3.8800000000000523, "score_vs_c_conv_pred": 6.640155087200861, "absolute_error_k_vs_strict_pred": 3.8800000000000523, "score_vs_pred_strict": 6.640155087200861, "truth_raw_assistant_text": "The value is 289.41", "prediction_raw_text": "The value is 285.53\u0e40\u0e17\u0e48\u0e32\u0e44"}
{"id": "line_1323", "truth_parsed_k": 289.96, "prediction_parsed_k_with_c_conv": 285.84, "prediction_parsed_k_strict": 285.84, "absolute_error_k_vs_c_conv_pred": 4.1200000000000045, "score_vs_c_conv_pred": 5.072978304372855, "absolute_error_k_vs_strict_pred": 4.1200000000000045, "score_vs_pred_strict": 5.072978304372855, "truth_raw_assistant_text": "The value is 289.96", "prediction_raw_text": "The value is 285.84."}
{"id": "line_1324", "truth_parsed_k": 290.76, "prediction_parsed_k_with_c_conv": 286.73, "prediction_parsed_k_strict": 286.73, "absolute_error_k_vs_c_conv_pred": 4.029999999999973, "score_vs_c_conv_pred": 5.650034332126874, "absolute_error_k_vs_strict_pred": 4.029999999999973, "score_vs_pred_strict": 5.650034332126874, "truth_raw_assistant_text": "The value is 290.76", "prediction_raw_text": "The value is 286.73."}
{"id": "line_1325", "truth_parsed_k": 291.58, "prediction_parsed_k_with_c_conv": 286.74, "prediction_parsed_k_strict": 286.74, "absolute_error_k_vs_c_conv_pred": 4.839999999999975, "score_vs_c_conv_pred": 0.8540894577411362, "absolute_error_k_vs_strict_pred": 4.839999999999975, "score_vs_pred_strict": 0.8540894577411362, "truth_raw_assistant_text": "The value is 291.58", "prediction_raw_text": "The value is 286.74."}
{"id": "line_1326", "truth_parsed_k": 292.21, "prediction_parsed_k_with_c_conv": 285.94, "prediction_parsed_k_strict": 285.94, "absolute_error_k_vs_c_conv_pred": 6.269999999999982, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.269999999999982, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.21", "prediction_raw_text": "The value is 285.94\u0e02\u0e49\u0e32\u0e21"}
{"id": "line_1327", "truth_parsed_k": 292.67, "prediction_parsed_k_with_c_conv": 285.92, "prediction_parsed_k_strict": 285.92, "absolute_error_k_vs_c_conv_pred": 6.75, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.75, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.67", "prediction_raw_text": "The value is 285.92."}
{"id": "line_1328", "truth_parsed_k": 292.43, "prediction_parsed_k_with_c_conv": 285.35, "prediction_parsed_k_strict": 285.35, "absolute_error_k_vs_c_conv_pred": 7.079999999999984, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.079999999999984, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.43", "prediction_raw_text": "The value is 285.35\u0e2a\u0e48\u0e27\u0e19\u0e43\u0e2b\u0e0d\u0e48"}
{"id": "line_1329", "truth_parsed_k": 291.73, "prediction_parsed_k_with_c_conv": 284.6, "prediction_parsed_k_strict": 284.6, "absolute_error_k_vs_c_conv_pred": 7.1299999999999955, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.1299999999999955, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.73", "prediction_raw_text": "The value is 284.60\u0e40\u0e01\u0e35\u0e22"}
{"id": "line_1330", "truth_parsed_k": 290.91, "prediction_parsed_k_with_c_conv": 285.45, "prediction_parsed_k_strict": 285.45, "absolute_error_k_vs_c_conv_pred": 5.460000000000036, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.460000000000036, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.91", "prediction_raw_text": "The value is 285.45\u0e41\u0e19\u0e48\u0e19\u0e2d\u0e19"}
{"id": "line_1331", "truth_parsed_k": 290.18, "prediction_parsed_k_with_c_conv": 285.56, "prediction_parsed_k_strict": 285.56, "absolute_error_k_vs_c_conv_pred": 4.6200000000000045, "score_vs_c_conv_pred": 2.0745280171969305, "absolute_error_k_vs_strict_pred": 4.6200000000000045, "score_vs_pred_strict": 2.0745280171969305, "truth_raw_assistant_text": "The value is 290.18", "prediction_raw_text": "The value is 285.56."}
{"id": "line_1332", "truth_parsed_k": 289.52, "prediction_parsed_k_with_c_conv": 285.98, "prediction_parsed_k_strict": 285.98, "absolute_error_k_vs_c_conv_pred": 3.5399999999999636, "score_vs_c_conv_pred": 9.02910145980924, "absolute_error_k_vs_strict_pred": 3.5399999999999636, "score_vs_pred_strict": 9.02910145980924, "truth_raw_assistant_text": "The value is 289.52", "prediction_raw_text": "The value is 285.98\u0e25\u0e39\u0e01\u0e04\u0e49\u0e32"}
{"id": "line_1333", "truth_parsed_k": 289.31, "prediction_parsed_k_with_c_conv": 285.64, "prediction_parsed_k_strict": 285.64, "absolute_error_k_vs_c_conv_pred": 3.670000000000016, "score_vs_c_conv_pred": 8.090488316374921, "absolute_error_k_vs_strict_pred": 3.670000000000016, "score_vs_pred_strict": 8.090488316374921, "truth_raw_assistant_text": "The value is 289.31", "prediction_raw_text": "The value is 285.64\uf978"}
{"id": "line_1334", "truth_parsed_k": 289.58, "prediction_parsed_k_with_c_conv": 285.43, "prediction_parsed_k_strict": 285.43, "absolute_error_k_vs_c_conv_pred": 4.149999999999977, "score_vs_c_conv_pred": 4.883341951586084, "absolute_error_k_vs_strict_pred": 4.149999999999977, "score_vs_pred_strict": 4.883341951586084, "truth_raw_assistant_text": "The value is 289.58", "prediction_raw_text": "The value is 285.43."}
{"id": "line_1335", "truth_parsed_k": 289.87, "prediction_parsed_k_with_c_conv": 286.42, "prediction_parsed_k_strict": 286.42, "absolute_error_k_vs_c_conv_pred": 3.4499999999999886, "score_vs_c_conv_pred": 9.698622846389004, "absolute_error_k_vs_strict_pred": 3.4499999999999886, "score_vs_pred_strict": 9.698622846389004, "truth_raw_assistant_text": "The value is 289.87", "prediction_raw_text": "The value is 286.42\u0647\u064f"}
{"id": "line_1336", "truth_parsed_k": 290.7, "prediction_parsed_k_with_c_conv": 286.67, "prediction_parsed_k_strict": 286.67, "absolute_error_k_vs_c_conv_pred": 4.029999999999973, "score_vs_c_conv_pred": 5.650034332126874, "absolute_error_k_vs_strict_pred": 4.029999999999973, "score_vs_pred_strict": 5.650034332126874, "truth_raw_assistant_text": "The value is 290.70", "prediction_raw_text": "The value is 286.67"}
{"id": "line_1337", "truth_parsed_k": 291.49, "prediction_parsed_k_with_c_conv": 287.76, "prediction_parsed_k_strict": 287.76, "absolute_error_k_vs_c_conv_pred": 3.730000000000018, "score_vs_c_conv_pred": 7.668075720471757, "absolute_error_k_vs_strict_pred": 3.730000000000018, "score_vs_pred_strict": 7.668075720471757, "truth_raw_assistant_text": "The value is 291.49", "prediction_raw_text": "The value is 287.76LIBINT"}
{"id": "line_1338", "truth_parsed_k": 292.39, "prediction_parsed_k_with_c_conv": 285.7, "prediction_parsed_k_strict": 285.7, "absolute_error_k_vs_c_conv_pred": 6.689999999999998, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.689999999999998, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.39", "prediction_raw_text": "The value is 285.70\u0e40\u0e2b\u0e25\u0e37\u0e2d"}
{"id": "line_1339", "truth_parsed_k": 292.63, "prediction_parsed_k_with_c_conv": 284.64, "prediction_parsed_k_strict": 284.64, "absolute_error_k_vs_c_conv_pred": 7.990000000000009, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.990000000000009, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.63", "prediction_raw_text": "The value is 284.64."}
{"id": "line_1340", "truth_parsed_k": 292.52, "prediction_parsed_k_with_c_conv": 284.76, "prediction_parsed_k_strict": 284.76, "absolute_error_k_vs_c_conv_pred": 7.759999999999991, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.759999999999991, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.52", "prediction_raw_text": "The value is 284.76\u0e0a\u0e19\u0e34\u0e14"}
{"id": "line_1341", "truth_parsed_k": 291.77, "prediction_parsed_k_with_c_conv": 285.97, "prediction_parsed_k_strict": 285.97, "absolute_error_k_vs_c_conv_pred": 5.7999999999999545, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.7999999999999545, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.77", "prediction_raw_text": "The value is 285.97\u0e21\u0e31\u0e49\u0e22"}
{"id": "line_1342", "truth_parsed_k": 290.94, "prediction_parsed_k_with_c_conv": 284.91, "prediction_parsed_k_strict": 284.91, "absolute_error_k_vs_c_conv_pred": 6.029999999999973, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.029999999999973, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.94", "prediction_raw_text": "The value is 284.91\u0e2a\u0e19\u0e31\u0e1a"}
{"id": "line_1343", "truth_parsed_k": 290.23, "prediction_parsed_k_with_c_conv": 284.95, "prediction_parsed_k_strict": 284.95, "absolute_error_k_vs_c_conv_pred": 5.28000000000003, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.28000000000003, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.23", "prediction_raw_text": "The value is 284.95\u0e1b\u0e23\u0e30\u0e2a\u0e34\u0e17\u0e18\u0e34"}
{"id": "line_1344", "truth_parsed_k": 289.49, "prediction_parsed_k_with_c_conv": 284.85, "prediction_parsed_k_strict": 284.85, "absolute_error_k_vs_c_conv_pred": 4.639999999999986, "score_vs_c_conv_pred": 1.9612649656465808, "absolute_error_k_vs_strict_pred": 4.639999999999986, "score_vs_pred_strict": 1.9612649656465808, "truth_raw_assistant_text": "The value is 289.49", "prediction_raw_text": "The value is 284.85\u0e40\u0e04\u0e23\u0e14\u0e34\u0e15"}
{"id": "line_1345", "truth_parsed_k": 289.32, "prediction_parsed_k_with_c_conv": 287.67, "prediction_parsed_k_strict": 287.67, "absolute_error_k_vs_c_conv_pred": 1.6499999999999773, "score_vs_c_conv_pred": 28.55283492688805, "absolute_error_k_vs_strict_pred": 1.6499999999999773, "score_vs_pred_strict": 28.55283492688805, "truth_raw_assistant_text": "The value is 289.32", "prediction_raw_text": "The value is 287.67\u0e1e\u0e34\u0e08"}
{"id": "line_1346", "truth_parsed_k": 289.61, "prediction_parsed_k_with_c_conv": 286.66, "prediction_parsed_k_strict": 286.66, "absolute_error_k_vs_c_conv_pred": 2.9499999999999886, "score_vs_c_conv_pred": 13.7556405151264, "absolute_error_k_vs_strict_pred": 2.9499999999999886, "score_vs_pred_strict": 13.7556405151264, "truth_raw_assistant_text": "The value is 289.61", "prediction_raw_text": "The value is 286.66."}
{"id": "line_1347", "truth_parsed_k": 290.03, "prediction_parsed_k_with_c_conv": 287.96, "prediction_parsed_k_strict": 287.96, "absolute_error_k_vs_c_conv_pred": 2.069999999999993, "score_vs_c_conv_pred": 22.83376929991482, "absolute_error_k_vs_strict_pred": 2.069999999999993, "score_vs_pred_strict": 22.83376929991482, "truth_raw_assistant_text": "The value is 290.03", "prediction_raw_text": "The value is 287.96"}
{"id": "line_1348", "truth_parsed_k": 290.9, "prediction_parsed_k_with_c_conv": 285.87, "prediction_parsed_k_strict": 285.87, "absolute_error_k_vs_c_conv_pred": 5.029999999999973, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.029999999999973, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.90", "prediction_raw_text": "The value is 285.87\ufa17"}
{"id": "line_1349", "truth_parsed_k": 291.67, "prediction_parsed_k_with_c_conv": 287.67, "prediction_parsed_k_strict": 287.67, "absolute_error_k_vs_c_conv_pred": 4.0, "score_vs_c_conv_pred": 5.845167438219834, "absolute_error_k_vs_strict_pred": 4.0, "score_vs_pred_strict": 5.845167438219834, "truth_raw_assistant_text": "The value is 291.67", "prediction_raw_text": "The value is 287.67\u00ea\u0323"}
{"id": "line_1350", "truth_parsed_k": 292.44, "prediction_parsed_k_with_c_conv": 285.73, "prediction_parsed_k_strict": 285.73, "absolute_error_k_vs_c_conv_pred": 6.7099999999999795, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.7099999999999795, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.44", "prediction_raw_text": "The value is 285.73."}
{"id": "line_1351", "truth_parsed_k": 292.87, "prediction_parsed_k_with_c_conv": 285.7, "prediction_parsed_k_strict": 285.7, "absolute_error_k_vs_c_conv_pred": 7.170000000000016, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.170000000000016, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.87", "prediction_raw_text": "The value is 285.70"}
{"id": "line_1352", "truth_parsed_k": 292.63, "prediction_parsed_k_with_c_conv": 285.89, "prediction_parsed_k_strict": 285.89, "absolute_error_k_vs_c_conv_pred": 6.740000000000009, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.740000000000009, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.63", "prediction_raw_text": "The value is 285.89."}
{"id": "line_1353", "truth_parsed_k": 291.91, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 7.090000000000032, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.090000000000032, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.91", "prediction_raw_text": "The value is 284.82"}
{"id": "line_1354", "truth_parsed_k": 291.09, "prediction_parsed_k_with_c_conv": 286.42, "prediction_parsed_k_strict": 286.42, "absolute_error_k_vs_c_conv_pred": 4.669999999999959, "score_vs_c_conv_pred": 1.7922586526817108, "absolute_error_k_vs_strict_pred": 4.669999999999959, "score_vs_pred_strict": 1.7922586526817108, "truth_raw_assistant_text": "The value is 291.09", "prediction_raw_text": "The value is 286.42\u1f75"}
{"id": "line_1355", "truth_parsed_k": 290.27, "prediction_parsed_k_with_c_conv": 285.57, "prediction_parsed_k_strict": 285.57, "absolute_error_k_vs_c_conv_pred": 4.699999999999989, "score_vs_c_conv_pred": 1.6243064400001561, "absolute_error_k_vs_strict_pred": 4.699999999999989, "score_vs_pred_strict": 1.6243064400001561, "truth_raw_assistant_text": "The value is 290.27", "prediction_raw_text": "The value is 285.57."}
{"id": "line_1356", "truth_parsed_k": 289.62, "prediction_parsed_k_with_c_conv": 285.72, "prediction_parsed_k_strict": 285.72, "absolute_error_k_vs_c_conv_pred": 3.8999999999999773, "score_vs_c_conv_pred": 6.50601657418135, "absolute_error_k_vs_strict_pred": 3.8999999999999773, "score_vs_pred_strict": 6.50601657418135, "truth_raw_assistant_text": "The value is 289.62", "prediction_raw_text": "The value is 285.72\u0e1e\u0e19\u0e31\u0e19"}
{"id": "line_1357", "truth_parsed_k": 289.62, "prediction_parsed_k_with_c_conv": 285.78, "prediction_parsed_k_strict": 285.78, "absolute_error_k_vs_c_conv_pred": 3.840000000000032, "score_vs_c_conv_pred": 6.910453475200773, "absolute_error_k_vs_strict_pred": 3.840000000000032, "score_vs_pred_strict": 6.910453475200773, "truth_raw_assistant_text": "The value is 289.62", "prediction_raw_text": "The value is 285.78\u0e04\u0e27\u0e32\u0e21\u0e40\u0e2b\u0e47\u0e19"}
{"id": "line_1358", "truth_parsed_k": 289.52, "prediction_parsed_k_with_c_conv": 285.88, "prediction_parsed_k_strict": 285.88, "absolute_error_k_vs_c_conv_pred": 3.6399999999999864, "score_vs_c_conv_pred": 8.30420571442686, "absolute_error_k_vs_strict_pred": 3.6399999999999864, "score_vs_pred_strict": 8.30420571442686, "truth_raw_assistant_text": "The value is 289.52", "prediction_raw_text": "The value is 285.88\u0e01\u0e32\u0e23\u0e04\u0e49\u0e32"}
{"id": "line_1359", "truth_parsed_k": 290.17, "prediction_parsed_k_with_c_conv": 285.55, "prediction_parsed_k_strict": 285.55, "absolute_error_k_vs_c_conv_pred": 4.6200000000000045, "score_vs_c_conv_pred": 2.0745280171969305, "absolute_error_k_vs_strict_pred": 4.6200000000000045, "score_vs_pred_strict": 2.0745280171969305, "truth_raw_assistant_text": "The value is 290.17", "prediction_raw_text": "The value is 285.55."}
{"id": "line_1360", "truth_parsed_k": 290.94, "prediction_parsed_k_with_c_conv": 285.94778688568, "prediction_parsed_k_strict": 285.94778688568, "absolute_error_k_vs_c_conv_pred": 4.992213114320009, "score_vs_c_conv_pred": 0.040945691844584964, "absolute_error_k_vs_strict_pred": 4.992213114320009, "score_vs_pred_strict": 0.040945691844584964, "truth_raw_assistant_text": "The value is 290.94", "prediction_raw_text": "The value is 285.94778688568."}
{"id": "line_1361", "truth_parsed_k": 291.74, "prediction_parsed_k_with_c_conv": 285.67, "prediction_parsed_k_strict": 285.67, "absolute_error_k_vs_c_conv_pred": 6.069999999999993, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.069999999999993, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.74", "prediction_raw_text": "The value is 285.67\u0e40\u0e21\u0e19\u0e39"}
{"id": "line_1362", "truth_parsed_k": 292.49, "prediction_parsed_k_with_c_conv": 284.87, "prediction_parsed_k_strict": 284.87, "absolute_error_k_vs_c_conv_pred": 7.6200000000000045, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.6200000000000045, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.49", "prediction_raw_text": "The value is 284.87."}
{"id": "line_1363", "truth_parsed_k": 292.92, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 7.980000000000018, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.980000000000018, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.92", "prediction_raw_text": "The value is 284.94."}
{"id": "line_1364", "truth_parsed_k": 292.76, "prediction_parsed_k_with_c_conv": 285.94, "prediction_parsed_k_strict": 285.94, "absolute_error_k_vs_c_conv_pred": 6.819999999999993, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.819999999999993, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.76", "prediction_raw_text": "The value is 285.94."}
{"id": "line_1365", "truth_parsed_k": 292.02, "prediction_parsed_k_with_c_conv": 284.7, "prediction_parsed_k_strict": 284.7, "absolute_error_k_vs_c_conv_pred": 7.319999999999993, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.319999999999993, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.02", "prediction_raw_text": "The value is 284.70\u0e1e\u0e34\u0e08\u0e32\u0e23\u0e13\u0e32"}
{"id": "line_1366", "truth_parsed_k": 291.26, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 6.289999999999964, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.289999999999964, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.26", "prediction_raw_text": "The value is 284.97\u0e22\u0e48"}
{"id": "line_1367", "truth_parsed_k": 290.38, "prediction_parsed_k_with_c_conv": 285.79, "prediction_parsed_k_strict": 285.79, "absolute_error_k_vs_c_conv_pred": 4.589999999999975, "score_vs_c_conv_pred": 2.2453208781553435, "absolute_error_k_vs_strict_pred": 4.589999999999975, "score_vs_pred_strict": 2.2453208781553435, "truth_raw_assistant_text": "The value is 290.38", "prediction_raw_text": "The value is 285.79"}
{"id": "line_1368", "truth_parsed_k": 289.9, "prediction_parsed_k_with_c_conv": 287.98, "prediction_parsed_k_strict": 287.98, "absolute_error_k_vs_c_conv_pred": 1.919999999999959, "score_vs_c_conv_pred": 24.73986552643037, "absolute_error_k_vs_strict_pred": 1.919999999999959, "score_vs_pred_strict": 24.73986552643037, "truth_raw_assistant_text": "The value is 289.90", "prediction_raw_text": "The value is 287.98\u0e17\u0e31\u0e01\u0e29\u0e30"}
{"id": "line_1369", "truth_parsed_k": 289.73, "prediction_parsed_k_with_c_conv": 284.98, "prediction_parsed_k_strict": 284.98, "absolute_error_k_vs_c_conv_pred": 4.75, "score_vs_c_conv_pred": 1.3466931481699862, "absolute_error_k_vs_strict_pred": 4.75, "score_vs_pred_strict": 1.3466931481699862, "truth_raw_assistant_text": "The value is 289.73", "prediction_raw_text": "The value is 284.98\u0e1d\u0e31\u0e19"}
{"id": "line_1370", "truth_parsed_k": 289.53, "prediction_parsed_k_with_c_conv": 285.98, "prediction_parsed_k_strict": 285.98, "absolute_error_k_vs_c_conv_pred": 3.5499999999999545, "score_vs_c_conv_pred": 8.955727434013083, "absolute_error_k_vs_strict_pred": 3.5499999999999545, "score_vs_pred_strict": 8.955727434013083, "truth_raw_assistant_text": "The value is 289.53", "prediction_raw_text": "The value is 285.98\u0e17\u0e48\u0e2d\u0e07\u0e40\u0e17\u0e35\u0e48\u0e22\u0e27"}
{"id": "line_1371", "truth_parsed_k": 290.18, "prediction_parsed_k_with_c_conv": 285.52, "prediction_parsed_k_strict": 285.52, "absolute_error_k_vs_c_conv_pred": 4.660000000000025, "score_vs_c_conv_pred": 1.8484763157018635, "absolute_error_k_vs_strict_pred": 4.660000000000025, "score_vs_pred_strict": 1.8484763157018635, "truth_raw_assistant_text": "The value is 290.18", "prediction_raw_text": "The value is 285.52\u0e02\u0e19\u0e2a\u0e48\u0e07"}
{"id": "line_1372", "truth_parsed_k": 291.06, "prediction_parsed_k_with_c_conv": 285.98, "prediction_parsed_k_strict": 285.98, "absolute_error_k_vs_c_conv_pred": 5.079999999999984, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.079999999999984, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.06", "prediction_raw_text": "The value is 285.98\u0e40\u0e15\u0e2d\u0e23\u0e4c"}
{"id": "line_1373", "truth_parsed_k": 291.88, "prediction_parsed_k_with_c_conv": 285.86, "prediction_parsed_k_strict": 285.86, "absolute_error_k_vs_c_conv_pred": 6.019999999999982, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.019999999999982, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.88", "prediction_raw_text": "The value is 285.86\u0e2a\u0e16\u0e34\u0e15\u0e34"}
{"id": "line_1374", "truth_parsed_k": 292.58, "prediction_parsed_k_with_c_conv": 284.93, "prediction_parsed_k_strict": 284.93, "absolute_error_k_vs_c_conv_pred": 7.649999999999977, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.649999999999977, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.58", "prediction_raw_text": "The value is 284.93\u0e1a\u0e38\u0e01"}
{"id": "line_1375", "truth_parsed_k": 292.95, "prediction_parsed_k_with_c_conv": 287.79, "prediction_parsed_k_strict": 287.79, "absolute_error_k_vs_c_conv_pred": 5.159999999999968, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.159999999999968, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.95", "prediction_raw_text": "The value is 287.79\u0e19\u0e49\u0e2d\u0e22"}
{"id": "line_1376", "truth_parsed_k": 292.7, "prediction_parsed_k_with_c_conv": 286.44, "prediction_parsed_k_strict": 286.44, "absolute_error_k_vs_c_conv_pred": 6.259999999999991, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.259999999999991, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.70", "prediction_raw_text": "The value is 286.44\u0e2a\u0e48\u0e27\u0e19\u0e43\u0e2b\u0e0d\u0e48"}
{"id": "line_1377", "truth_parsed_k": 292.05, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 7.110000000000014, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.110000000000014, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.05", "prediction_raw_text": "The value is 284.94"}
{"id": "line_1378", "truth_parsed_k": 291.21, "prediction_parsed_k_with_c_conv": 284.87, "prediction_parsed_k_strict": 284.87, "absolute_error_k_vs_c_conv_pred": 6.339999999999975, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.339999999999975, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.21", "prediction_raw_text": "The value is 284.87"}
{"id": "line_1379", "truth_parsed_k": 290.39, "prediction_parsed_k_with_c_conv": 285.44, "prediction_parsed_k_strict": 285.44, "absolute_error_k_vs_c_conv_pred": 4.949999999999989, "score_vs_c_conv_pred": 0.26400457350821416, "absolute_error_k_vs_strict_pred": 4.949999999999989, "score_vs_pred_strict": 0.26400457350821416, "truth_raw_assistant_text": "The value is 290.39", "prediction_raw_text": "The value is 285.44\u0e14\u0e49\u0e32\u0e19\u0e25\u0e48\u0e32\u0e07"}
{"id": "line_1380", "truth_parsed_k": 289.89, "prediction_parsed_k_with_c_conv": 285.44, "prediction_parsed_k_strict": 285.44, "absolute_error_k_vs_c_conv_pred": 4.449999999999989, "score_vs_c_conv_pred": 3.0569980670319685, "absolute_error_k_vs_strict_pred": 4.449999999999989, "score_vs_pred_strict": 3.0569980670319685, "truth_raw_assistant_text": "The value is 289.89", "prediction_raw_text": "The value is 285.44\uf926"}
{"id": "line_1381", "truth_parsed_k": 289.47, "prediction_parsed_k_with_c_conv": 287.29, "prediction_parsed_k_strict": 287.29, "absolute_error_k_vs_c_conv_pred": 2.180000000000007, "score_vs_c_conv_pred": 21.517014729297866, "absolute_error_k_vs_strict_pred": 2.180000000000007, "score_vs_pred_strict": 21.517014729297866, "truth_raw_assistant_text": "The value is 289.47", "prediction_raw_text": "The value is 287.29."}
{"id": "line_1382", "truth_parsed_k": 289.68, "prediction_parsed_k_with_c_conv": 286.47, "prediction_parsed_k_strict": 286.47, "absolute_error_k_vs_c_conv_pred": 3.2099999999999795, "score_vs_c_conv_pred": 11.569934228803058, "absolute_error_k_vs_strict_pred": 3.2099999999999795, "score_vs_pred_strict": 11.569934228803058, "truth_raw_assistant_text": "The value is 289.68", "prediction_raw_text": "The value is 286.47\u0e01\u0e25\u0e49\u0e32"}
{"id": "line_1383", "truth_parsed_k": 290.19, "prediction_parsed_k_with_c_conv": 285.38, "prediction_parsed_k_strict": 285.38, "absolute_error_k_vs_c_conv_pred": 4.810000000000002, "score_vs_c_conv_pred": 1.017291477328286, "absolute_error_k_vs_strict_pred": 4.810000000000002, "score_vs_pred_strict": 1.017291477328286, "truth_raw_assistant_text": "The value is 290.19", "prediction_raw_text": "The value is 285.38\u0e25\u0e34"}
{"id": "line_1384", "truth_parsed_k": 290.88, "prediction_parsed_k_with_c_conv": 286.97, "prediction_parsed_k_strict": 286.97, "absolute_error_k_vs_c_conv_pred": 3.909999999999968, "score_vs_c_conv_pred": 6.439197058986501, "absolute_error_k_vs_strict_pred": 3.909999999999968, "score_vs_pred_strict": 6.439197058986501, "truth_raw_assistant_text": "The value is 290.88", "prediction_raw_text": "The value is 286.97\uf99a"}
{"id": "line_1385", "truth_parsed_k": 291.77, "prediction_parsed_k_with_c_conv": 288.75, "prediction_parsed_k_strict": 288.75, "absolute_error_k_vs_c_conv_pred": 3.019999999999982, "score_vs_c_conv_pred": 13.149512816791553, "absolute_error_k_vs_strict_pred": 3.019999999999982, "score_vs_pred_strict": 13.149512816791553, "truth_raw_assistant_text": "The value is 291.77", "prediction_raw_text": "The value is 288.75\u0e22\u0e37\u0e19\u0e22\u0e31\u0e19"}
{"id": "line_1386", "truth_parsed_k": 292.61, "prediction_parsed_k_with_c_conv": 285.53, "prediction_parsed_k_strict": 285.53, "absolute_error_k_vs_c_conv_pred": 7.080000000000041, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.080000000000041, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.61", "prediction_raw_text": "The value is 285.53\u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01"}
{"id": "line_1387", "truth_parsed_k": 292.92, "prediction_parsed_k_with_c_conv": 285.87, "prediction_parsed_k_strict": 285.87, "absolute_error_k_vs_c_conv_pred": 7.050000000000011, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.050000000000011, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.92", "prediction_raw_text": "The value is 285.87."}
{"id": "line_1388", "truth_parsed_k": 292.7, "prediction_parsed_k_with_c_conv": 285.29, "prediction_parsed_k_strict": 285.29, "absolute_error_k_vs_c_conv_pred": 7.409999999999968, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.409999999999968, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.70", "prediction_raw_text": "The value is 285.29."}
{"id": "line_1389", "truth_parsed_k": 292.01, "prediction_parsed_k_with_c_conv": 285.58, "prediction_parsed_k_strict": 285.58, "absolute_error_k_vs_c_conv_pred": 6.430000000000007, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.430000000000007, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.01", "prediction_raw_text": "The value is 285.58\u0e2d\u0e22\u0e39\u0e48\u0e17\u0e35\u0e48"}
{"id": "line_1390", "truth_parsed_k": 291.25, "prediction_parsed_k_with_c_conv": 288.51, "prediction_parsed_k_strict": 288.51, "absolute_error_k_vs_c_conv_pred": 2.740000000000009, "score_vs_c_conv_pred": 15.660452104108114, "absolute_error_k_vs_strict_pred": 2.740000000000009, "score_vs_pred_strict": 15.660452104108114, "truth_raw_assistant_text": "The value is 291.25", "prediction_raw_text": "The value is 288.51"}
{"id": "line_1391", "truth_parsed_k": 290.5, "prediction_parsed_k_with_c_conv": 285.36, "prediction_parsed_k_strict": 285.36, "absolute_error_k_vs_c_conv_pred": 5.139999999999986, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.139999999999986, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.50", "prediction_raw_text": "The value is 285.36."}
{"id": "line_1392", "truth_parsed_k": 289.95, "prediction_parsed_k_with_c_conv": 285.57, "prediction_parsed_k_strict": 285.57, "absolute_error_k_vs_c_conv_pred": 4.3799999999999955, "score_vs_c_conv_pred": 3.4721995870300892, "absolute_error_k_vs_strict_pred": 4.3799999999999955, "score_vs_pred_strict": 3.4721995870300892, "truth_raw_assistant_text": "The value is 289.95", "prediction_raw_text": "The value is 285.57."}
{"id": "line_1393", "truth_parsed_k": 289.67, "prediction_parsed_k_with_c_conv": 284.79, "prediction_parsed_k_strict": 284.79, "absolute_error_k_vs_c_conv_pred": 4.8799999999999955, "score_vs_c_conv_pred": 0.638014069249393, "absolute_error_k_vs_strict_pred": 4.8799999999999955, "score_vs_pred_strict": 0.638014069249393, "truth_raw_assistant_text": "The value is 289.67", "prediction_raw_text": "The value is 284.79\u0e2a\u0e34\u0e48\u0e07\u0e41"}
{"id": "line_1394", "truth_parsed_k": 289.88, "prediction_parsed_k_with_c_conv": 286.84, "prediction_parsed_k_strict": 286.84, "absolute_error_k_vs_c_conv_pred": 3.0400000000000205, "score_vs_c_conv_pred": 12.978810179818801, "absolute_error_k_vs_strict_pred": 3.0400000000000205, "score_vs_pred_strict": 12.978810179818801, "truth_raw_assistant_text": "The value is 289.88", "prediction_raw_text": "The value is 286.84."}
{"id": "line_1395", "truth_parsed_k": 290.34, "prediction_parsed_k_with_c_conv": 285.88, "prediction_parsed_k_strict": 285.88, "absolute_error_k_vs_c_conv_pred": 4.4599999999999795, "score_vs_c_conv_pred": 2.9982027362366437, "absolute_error_k_vs_strict_pred": 4.4599999999999795, "score_vs_pred_strict": 2.9982027362366437, "truth_raw_assistant_text": "The value is 290.34", "prediction_raw_text": "The value is 285.88\u0e21\u0e32\u0e41\u0e25\u0e49\u0e27"}
{"id": "line_1396", "truth_parsed_k": 290.93, "prediction_parsed_k_with_c_conv": 285.62, "prediction_parsed_k_strict": 285.62, "absolute_error_k_vs_c_conv_pred": 5.310000000000002, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.310000000000002, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.93", "prediction_raw_text": "The value is 285.62"}
{"id": "line_1397", "truth_parsed_k": 291.84, "prediction_parsed_k_with_c_conv": 285.95, "prediction_parsed_k_strict": 285.95, "absolute_error_k_vs_c_conv_pred": 5.889999999999986, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.889999999999986, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.84", "prediction_raw_text": "The value is 285.95\u0e1b\u0e23\u0e30\u0e27\u0e31\u0e15\u0e34"}
{"id": "line_1398", "truth_parsed_k": 292.69, "prediction_parsed_k_with_c_conv": 285.77, "prediction_parsed_k_strict": 285.77, "absolute_error_k_vs_c_conv_pred": 6.920000000000016, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.920000000000016, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.69", "prediction_raw_text": "The value is 285.77\u0e1c\u0e25\u0e34\u0e15\u0e20\u0e31\u0e13\u0e11\u0e4c"}
{"id": "line_1399", "truth_parsed_k": 292.99, "prediction_parsed_k_with_c_conv": 284.95, "prediction_parsed_k_strict": 284.95, "absolute_error_k_vs_c_conv_pred": 8.04000000000002, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.04000000000002, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.99", "prediction_raw_text": "The value is 284.95\u0642\u0650"}
{"id": "line_1400", "truth_parsed_k": 292.77, "prediction_parsed_k_with_c_conv": 287.66, "prediction_parsed_k_strict": 287.66, "absolute_error_k_vs_c_conv_pred": 5.109999999999957, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.109999999999957, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.77", "prediction_raw_text": "The value is 287.66\uf997"}
{"id": "line_1401", "truth_parsed_k": 292.07, "prediction_parsed_k_with_c_conv": 284.98, "prediction_parsed_k_strict": 284.98, "absolute_error_k_vs_c_conv_pred": 7.089999999999975, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.089999999999975, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.07", "prediction_raw_text": "The value is 284.98\u0e2d\u0e07\u0e04\u0e4c"}
{"id": "line_1402", "truth_parsed_k": 291.35, "prediction_parsed_k_with_c_conv": 285.55, "prediction_parsed_k_strict": 285.55, "absolute_error_k_vs_c_conv_pred": 5.800000000000011, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.800000000000011, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.35", "prediction_raw_text": "The value is 285.55"}
{"id": "line_1403", "truth_parsed_k": 290.45, "prediction_parsed_k_with_c_conv": 287.49, "prediction_parsed_k_strict": 287.49, "absolute_error_k_vs_c_conv_pred": 2.9599999999999795, "score_vs_c_conv_pred": 13.668211052588774, "absolute_error_k_vs_strict_pred": 2.9599999999999795, "score_vs_pred_strict": 13.668211052588774, "truth_raw_assistant_text": "The value is 290.45", "prediction_raw_text": "The value is 287.49\u0e40\u0e2b\u0e25\u0e48\u0e32"}
{"id": "line_1404", "truth_parsed_k": 290.1, "prediction_parsed_k_with_c_conv": 285.92, "prediction_parsed_k_strict": 285.92, "absolute_error_k_vs_c_conv_pred": 4.180000000000007, "score_vs_c_conv_pred": 4.69503173798671, "absolute_error_k_vs_strict_pred": 4.180000000000007, "score_vs_pred_strict": 4.69503173798671, "truth_raw_assistant_text": "The value is 290.10", "prediction_raw_text": "The value is 285.92."}
{"id": "line_1405", "truth_parsed_k": 289.71, "prediction_parsed_k_with_c_conv": 285.95, "prediction_parsed_k_strict": 285.95, "absolute_error_k_vs_c_conv_pred": 3.759999999999991, "score_vs_c_conv_pred": 7.4593285448395825, "absolute_error_k_vs_strict_pred": 3.759999999999991, "score_vs_pred_strict": 7.4593285448395825, "truth_raw_assistant_text": "The value is 289.71", "prediction_raw_text": "The value is 285.95\u0646\u064f"}
{"id": "line_1406", "truth_parsed_k": 289.86, "prediction_parsed_k_with_c_conv": 286.5, "prediction_parsed_k_strict": 286.5, "absolute_error_k_vs_c_conv_pred": 3.3600000000000136, "score_vs_c_conv_pred": 10.385216009938524, "absolute_error_k_vs_strict_pred": 3.3600000000000136, "score_vs_pred_strict": 10.385216009938524, "truth_raw_assistant_text": "The value is 289.86", "prediction_raw_text": "The value is 286.50\u0e22\u0e37\u0e19\u0e22\u0e31\u0e19"}
{"id": "line_1407", "truth_parsed_k": 290.37, "prediction_parsed_k_with_c_conv": 285.69, "prediction_parsed_k_strict": 285.69, "absolute_error_k_vs_c_conv_pred": 4.680000000000007, "score_vs_c_conv_pred": 1.7361581098770285, "absolute_error_k_vs_strict_pred": 4.680000000000007, "score_vs_pred_strict": 1.7361581098770285, "truth_raw_assistant_text": "The value is 290.37", "prediction_raw_text": "The value is 285.69\u0e17\u0e35\u0e48\u0e44\u0e21\u0e48"}
{"id": "line_1408", "truth_parsed_k": 291.21, "prediction_parsed_k_with_c_conv": 286.89, "prediction_parsed_k_strict": 286.89, "absolute_error_k_vs_c_conv_pred": 4.319999999999993, "score_vs_c_conv_pred": 3.8332541688072874, "absolute_error_k_vs_strict_pred": 4.319999999999993, "score_vs_pred_strict": 3.8332541688072874, "truth_raw_assistant_text": "The value is 291.21", "prediction_raw_text": "The value is 286.89\u0e23\u0e30\u0e2b\u0e27\u0e48\u0e32\u0e07"}
{"id": "line_1409", "truth_parsed_k": 291.93, "prediction_parsed_k_with_c_conv": 285.62, "prediction_parsed_k_strict": 285.62, "absolute_error_k_vs_c_conv_pred": 6.310000000000002, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.310000000000002, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.93", "prediction_raw_text": "The value is 285.62\uf9cd"}
{"id": "line_1410", "truth_parsed_k": 292.73, "prediction_parsed_k_with_c_conv": 286.98, "prediction_parsed_k_strict": 286.98, "absolute_error_k_vs_c_conv_pred": 5.75, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.75, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.73", "prediction_raw_text": "The value is 286.98\u0e1e\u0e38"}
{"id": "line_1411", "truth_parsed_k": 292.98, "prediction_parsed_k_with_c_conv": 287.31, "prediction_parsed_k_strict": 287.31, "absolute_error_k_vs_c_conv_pred": 5.670000000000016, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.670000000000016, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.98", "prediction_raw_text": "The value is 287.31"}
{"id": "line_1412", "truth_parsed_k": 292.73, "prediction_parsed_k_with_c_conv": 285.81, "prediction_parsed_k_strict": 285.81, "absolute_error_k_vs_c_conv_pred": 6.920000000000016, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.920000000000016, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.73", "prediction_raw_text": "The value is 285.81."}
{"id": "line_1413", "truth_parsed_k": 292.18, "prediction_parsed_k_with_c_conv": 285.81, "prediction_parsed_k_strict": 285.81, "absolute_error_k_vs_c_conv_pred": 6.3700000000000045, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.3700000000000045, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.18", "prediction_raw_text": "The value is 285.81\u0e21\u0e2b\u0e32\u0e27\u0e34\u0e17\u0e22\u0e32"}
{"id": "line_1414", "truth_parsed_k": 291.25, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 6.430000000000007, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.430000000000007, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.25", "prediction_raw_text": "The value is 284.82"}
{"id": "line_1415", "truth_parsed_k": 290.45, "prediction_parsed_k_with_c_conv": 285.45, "prediction_parsed_k_strict": 285.45, "absolute_error_k_vs_c_conv_pred": 5.0, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.0, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.45", "prediction_raw_text": "The value is 285.45."}
{"id": "line_1416", "truth_parsed_k": 289.93, "prediction_parsed_k_with_c_conv": 285.67, "prediction_parsed_k_strict": 285.67, "absolute_error_k_vs_c_conv_pred": 4.259999999999991, "score_vs_c_conv_pred": 4.199215644648069, "absolute_error_k_vs_strict_pred": 4.259999999999991, "score_vs_pred_strict": 4.199215644648069, "truth_raw_assistant_text": "The value is 289.93", "prediction_raw_text": "The value is 285.67\u0e41\u0e15\u0e01\u0e15\u0e48\u0e32\u0e07\u0e01\u0e31\u0e19"}
{"id": "line_1417", "truth_parsed_k": 289.66, "prediction_parsed_k_with_c_conv": 285.88, "prediction_parsed_k_strict": 285.88, "absolute_error_k_vs_c_conv_pred": 3.7800000000000296, "score_vs_c_conv_pred": 7.321057376815487, "absolute_error_k_vs_strict_pred": 3.7800000000000296, "score_vs_pred_strict": 7.321057376815487, "truth_raw_assistant_text": "The value is 289.66", "prediction_raw_text": "The value is 285.88."}
{"id": "line_1418", "truth_parsed_k": 289.54, "prediction_parsed_k_with_c_conv": 286.49, "prediction_parsed_k_strict": 286.49, "absolute_error_k_vs_c_conv_pred": 3.0500000000000114, "score_vs_c_conv_pred": 12.893862945946688, "absolute_error_k_vs_strict_pred": 3.0500000000000114, "score_vs_pred_strict": 12.893862945946688, "truth_raw_assistant_text": "The value is 289.54", "prediction_raw_text": "The value is 286.49\u0e2d\u0e48\u0e2d\u0e19"}
{"id": "line_1419", "truth_parsed_k": 290.18, "prediction_parsed_k_with_c_conv": 285.94, "prediction_parsed_k_strict": 285.94, "absolute_error_k_vs_c_conv_pred": 4.240000000000009, "score_vs_c_conv_pred": 4.322316434143003, "absolute_error_k_vs_strict_pred": 4.240000000000009, "score_vs_pred_strict": 4.322316434143003, "truth_raw_assistant_text": "The value is 290.18", "prediction_raw_text": "The value is 285.94\u0e40\u0e0a\u0e35\u0e22\u0e07\u0e43\u0e2b\u0e21\u0e48"}
{"id": "line_1420", "truth_parsed_k": 291.0, "prediction_parsed_k_with_c_conv": 288.29, "prediction_parsed_k_strict": 288.29, "absolute_error_k_vs_c_conv_pred": 2.7099999999999795, "score_vs_c_conv_pred": 15.943909993114158, "absolute_error_k_vs_strict_pred": 2.7099999999999795, "score_vs_pred_strict": 15.943909993114158, "truth_raw_assistant_text": "The value is 291.00", "prediction_raw_text": "The value is 288.29\u0e2e\u0e34"}
{"id": "line_1421", "truth_parsed_k": 291.75, "prediction_parsed_k_with_c_conv": 286.95, "prediction_parsed_k_strict": 286.95, "absolute_error_k_vs_c_conv_pred": 4.800000000000011, "score_vs_c_conv_pred": 1.071912694446353, "absolute_error_k_vs_strict_pred": 4.800000000000011, "score_vs_pred_strict": 1.071912694446353, "truth_raw_assistant_text": "The value is 291.75", "prediction_raw_text": "The value is 286.95\u0e40\u0e01\u0e48\u0e07"}
{"id": "line_1422", "truth_parsed_k": 292.4, "prediction_parsed_k_with_c_conv": 285.67, "prediction_parsed_k_strict": 285.67, "absolute_error_k_vs_c_conv_pred": 6.729999999999961, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.729999999999961, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.40", "prediction_raw_text": "The value is 285.67\u0e01\u0e23\u0e30\u0e40\u0e1b\u0e4b"}
{"id": "line_1423", "truth_parsed_k": 292.82, "prediction_parsed_k_with_c_conv": 285.97, "prediction_parsed_k_strict": 285.97, "absolute_error_k_vs_c_conv_pred": 6.849999999999966, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.849999999999966, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.82", "prediction_raw_text": "The value is 285.97\u0e2b\u0e19\u0e49\u0e32\u0e08\u0e2d"}
{"id": "line_1424", "truth_parsed_k": 292.53, "prediction_parsed_k_with_c_conv": 285.81, "prediction_parsed_k_strict": 285.81, "absolute_error_k_vs_c_conv_pred": 6.71999999999997, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.71999999999997, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.53", "prediction_raw_text": "The value is 285.81."}
{"id": "line_1425", "truth_parsed_k": 291.78, "prediction_parsed_k_with_c_conv": 285.98, "prediction_parsed_k_strict": 285.98, "absolute_error_k_vs_c_conv_pred": 5.7999999999999545, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.7999999999999545, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.78", "prediction_raw_text": "The value is 285.98\u0642\u064e"}
{"id": "line_1426", "truth_parsed_k": 290.93, "prediction_parsed_k_with_c_conv": 288.65, "prediction_parsed_k_strict": 288.65, "absolute_error_k_vs_c_conv_pred": 2.2800000000000296, "score_vs_c_conv_pred": 20.37339273014781, "absolute_error_k_vs_strict_pred": 2.2800000000000296, "score_vs_pred_strict": 20.37339273014781, "truth_raw_assistant_text": "The value is 290.93", "prediction_raw_text": "The value is 288.65"}
{"id": "line_1427", "truth_parsed_k": 290.06, "prediction_parsed_k_with_c_conv": 285.66, "prediction_parsed_k_strict": 285.66, "absolute_error_k_vs_c_conv_pred": 4.399999999999977, "score_vs_c_conv_pred": 3.352915888962116, "absolute_error_k_vs_strict_pred": 4.399999999999977, "score_vs_pred_strict": 3.352915888962116, "truth_raw_assistant_text": "The value is 290.06", "prediction_raw_text": "The value is 285.66."}
{"id": "line_1428", "truth_parsed_k": 289.51, "prediction_parsed_k_with_c_conv": 286.36, "prediction_parsed_k_strict": 286.36, "absolute_error_k_vs_c_conv_pred": 3.1499999999999773, "score_vs_c_conv_pred": 12.058811513376266, "absolute_error_k_vs_strict_pred": 3.1499999999999773, "score_vs_pred_strict": 12.058811513376266, "truth_raw_assistant_text": "The value is 289.51", "prediction_raw_text": "The value is 286.36."}
{"id": "line_1429", "truth_parsed_k": 289.37, "prediction_parsed_k_with_c_conv": 285.35, "prediction_parsed_k_strict": 285.35, "absolute_error_k_vs_c_conv_pred": 4.019999999999982, "score_vs_c_conv_pred": 5.714921714864262, "absolute_error_k_vs_strict_pred": 4.019999999999982, "score_vs_pred_strict": 5.714921714864262, "truth_raw_assistant_text": "The value is 289.37", "prediction_raw_text": "The value is 285.35."}
{"id": "line_1430", "truth_parsed_k": 289.58, "prediction_parsed_k_with_c_conv": 285.34, "prediction_parsed_k_strict": 285.34, "absolute_error_k_vs_c_conv_pred": 4.240000000000009, "score_vs_c_conv_pred": 4.322316434143003, "absolute_error_k_vs_strict_pred": 4.240000000000009, "score_vs_pred_strict": 4.322316434143003, "truth_raw_assistant_text": "The value is 289.58", "prediction_raw_text": "The value is 285.34\uf9b5"}
{"id": "line_1431", "truth_parsed_k": 290.01, "prediction_parsed_k_with_c_conv": 285.8, "prediction_parsed_k_strict": 285.8, "absolute_error_k_vs_c_conv_pred": 4.2099999999999795, "score_vs_c_conv_pred": 4.508029244751999, "absolute_error_k_vs_strict_pred": 4.2099999999999795, "score_vs_pred_strict": 4.508029244751999, "truth_raw_assistant_text": "The value is 290.01", "prediction_raw_text": "The value is 285.80\u0e1c\u0e25\u0e34\u0e15\u0e20\u0e31"}
{"id": "line_1432", "truth_parsed_k": 290.81, "prediction_parsed_k_with_c_conv": 286.58, "prediction_parsed_k_strict": 286.58, "absolute_error_k_vs_c_conv_pred": 4.230000000000018, "score_vs_c_conv_pred": 4.38407850435194, "absolute_error_k_vs_strict_pred": 4.230000000000018, "score_vs_pred_strict": 4.38407850435194, "truth_raw_assistant_text": "The value is 290.81", "prediction_raw_text": "The value is 286.58\u0e23\u0e39\u0e1b\u0e41\u0e1a\u0e1a"}
{"id": "line_1433", "truth_parsed_k": 291.63, "prediction_parsed_k_with_c_conv": 286.26, "prediction_parsed_k_strict": 286.26, "absolute_error_k_vs_c_conv_pred": 5.3700000000000045, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.3700000000000045, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.63", "prediction_raw_text": "The value is 286.26."}
{"id": "line_1434", "truth_parsed_k": 292.23, "prediction_parsed_k_with_c_conv": 285.98, "prediction_parsed_k_strict": 285.98, "absolute_error_k_vs_c_conv_pred": 6.25, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.25, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.23", "prediction_raw_text": "The value is 285.98 \u0e01\u0e31\u0e19\u0e22"}
{"id": "line_1435", "truth_parsed_k": 292.73, "prediction_parsed_k_with_c_conv": 285.94766682061, "prediction_parsed_k_strict": 285.94766682061, "absolute_error_k_vs_c_conv_pred": 6.7823331793900365, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.7823331793900365, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.73", "prediction_raw_text": "The value is 285.94766682061."}
{"id": "line_1436", "truth_parsed_k": 292.56, "prediction_parsed_k_with_c_conv": 284.96, "prediction_parsed_k_strict": 284.96, "absolute_error_k_vs_c_conv_pred": 7.600000000000023, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.600000000000023, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.56", "prediction_raw_text": "The value is 284.96\uf9d0"}
{"id": "line_1437", "truth_parsed_k": 291.74, "prediction_parsed_k_with_c_conv": 285.84, "prediction_parsed_k_strict": 285.84, "absolute_error_k_vs_c_conv_pred": 5.900000000000034, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.900000000000034, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.74", "prediction_raw_text": "The value is 285.84\u0e08\u0e31\u0e19\u0e17\u0e23\u0e4c"}
{"id": "line_1438", "truth_parsed_k": 291.0, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 6.029999999999973, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.029999999999973, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.00", "prediction_raw_text": "The value is 284.97\u0e2d\u0e38\u0e1b\u0e01\u0e23\u0e13\u0e4c"}
{"id": "line_1439", "truth_parsed_k": 290.27, "prediction_parsed_k_with_c_conv": 286.29, "prediction_parsed_k_strict": 286.29, "absolute_error_k_vs_c_conv_pred": 3.9799999999999613, "score_vs_c_conv_pred": 5.976046193094131, "absolute_error_k_vs_strict_pred": 3.9799999999999613, "score_vs_pred_strict": 5.976046193094131, "truth_raw_assistant_text": "The value is 290.27", "prediction_raw_text": "The value is 286.29\ufa56"}
{"id": "line_1440", "truth_parsed_k": 289.76, "prediction_parsed_k_with_c_conv": 286.27, "prediction_parsed_k_strict": 286.27, "absolute_error_k_vs_c_conv_pred": 3.490000000000009, "score_vs_c_conv_pred": 9.39900083111902, "absolute_error_k_vs_strict_pred": 3.490000000000009, "score_vs_pred_strict": 9.39900083111902, "truth_raw_assistant_text": "The value is 289.76", "prediction_raw_text": "The value is 286.27."}
{"id": "line_1441", "truth_parsed_k": 289.52, "prediction_parsed_k_with_c_conv": 284.73, "prediction_parsed_k_strict": 284.73, "absolute_error_k_vs_c_conv_pred": 4.789999999999964, "score_vs_c_conv_pred": 1.126644930375098, "absolute_error_k_vs_strict_pred": 4.789999999999964, "score_vs_pred_strict": 1.126644930375098, "truth_raw_assistant_text": "The value is 289.52", "prediction_raw_text": "The value is 284.73\u0e21\u0e30\u0e40\u0e23\u0e47"}
{"id": "line_1442", "truth_parsed_k": 289.59, "prediction_parsed_k_with_c_conv": 284.69, "prediction_parsed_k_strict": 284.69, "absolute_error_k_vs_c_conv_pred": 4.899999999999977, "score_vs_c_conv_pred": 0.5306230961831027, "absolute_error_k_vs_strict_pred": 4.899999999999977, "score_vs_pred_strict": 0.5306230961831027, "truth_raw_assistant_text": "The value is 289.59", "prediction_raw_text": "The value is 284.69\u0e2d\u0e31\u0e19\u0e15\u0e23\u0e32\u0e22"}
{"id": "line_1443", "truth_parsed_k": 290.13, "prediction_parsed_k_with_c_conv": 284.86, "prediction_parsed_k_strict": 284.86, "absolute_error_k_vs_c_conv_pred": 5.269999999999982, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.269999999999982, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.13", "prediction_raw_text": "The value is 284.86\u0e04\u0e27\u0e32\u0e21\u0e04\u0e34\u0e14"}
{"id": "line_1444", "truth_parsed_k": 290.98, "prediction_parsed_k_with_c_conv": 284.81, "prediction_parsed_k_strict": 284.81, "absolute_error_k_vs_c_conv_pred": 6.170000000000016, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.170000000000016, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.98", "prediction_raw_text": "The value is 284.81"}
{"id": "line_1445", "truth_parsed_k": 291.91, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 6.970000000000027, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.970000000000027, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.91", "prediction_raw_text": "The value is 284.94."}
{"id": "line_1446", "truth_parsed_k": 292.68, "prediction_parsed_k_with_c_conv": 284.76, "prediction_parsed_k_strict": 284.76, "absolute_error_k_vs_c_conv_pred": 7.920000000000016, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.920000000000016, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.68", "prediction_raw_text": "The value is 284.76\u0e40\u0e25\u0e37\u0e2d\u0e14"}
{"id": "line_1447", "truth_parsed_k": 293.05, "prediction_parsed_k_with_c_conv": 284.68, "prediction_parsed_k_strict": 284.68, "absolute_error_k_vs_c_conv_pred": 8.370000000000005, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.370000000000005, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.05", "prediction_raw_text": "The value is 284.68\u0e22\u0e31\u0e19"}
{"id": "line_1448", "truth_parsed_k": 292.92, "prediction_parsed_k_with_c_conv": 284.77, "prediction_parsed_k_strict": 284.77, "absolute_error_k_vs_c_conv_pred": 8.150000000000034, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.150000000000034, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.92", "prediction_raw_text": "The value is 284.77"}
{"id": "line_1449", "truth_parsed_k": 292.18, "prediction_parsed_k_with_c_conv": 285.06, "prediction_parsed_k_strict": 285.06, "absolute_error_k_vs_c_conv_pred": 7.1200000000000045, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.1200000000000045, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.18", "prediction_raw_text": "The value is 285.06"}
{"id": "line_1450", "truth_parsed_k": 291.47, "prediction_parsed_k_with_c_conv": 284.89, "prediction_parsed_k_strict": 284.89, "absolute_error_k_vs_c_conv_pred": 6.580000000000041, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.580000000000041, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.47", "prediction_raw_text": "The value is 284.89\u0e40\u0e2d\u0e47\u0e21"}
{"id": "line_1451", "truth_parsed_k": 290.57, "prediction_parsed_k_with_c_conv": 284.75, "prediction_parsed_k_strict": 284.75, "absolute_error_k_vs_c_conv_pred": 5.819999999999993, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.819999999999993, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.57", "prediction_raw_text": "The value is 284.75\uf961"}
{"id": "line_1452", "truth_parsed_k": 290.16, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 5.220000000000027, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.220000000000027, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.16", "prediction_raw_text": "The value is 284.94."}
{"id": "line_1453", "truth_parsed_k": 289.84, "prediction_parsed_k_with_c_conv": 286.8, "prediction_parsed_k_strict": 286.8, "absolute_error_k_vs_c_conv_pred": 3.0399999999999636, "score_vs_c_conv_pred": 12.978810179819277, "absolute_error_k_vs_strict_pred": 3.0399999999999636, "score_vs_pred_strict": 12.978810179819277, "truth_raw_assistant_text": "The value is 289.84", "prediction_raw_text": "The value is 286.80\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07"}
{"id": "line_1454", "truth_parsed_k": 289.95, "prediction_parsed_k_with_c_conv": 285.87, "prediction_parsed_k_strict": 285.87, "absolute_error_k_vs_c_conv_pred": 4.079999999999984, "score_vs_c_conv_pred": 5.3279222281299665, "absolute_error_k_vs_strict_pred": 4.079999999999984, "score_vs_pred_strict": 5.3279222281299665, "truth_raw_assistant_text": "The value is 289.95", "prediction_raw_text": "The value is 285.87\u0e01\u0e32\u0e23\u0e40\u0e21\u0e37\u0e2d\u0e07"}
{"id": "line_1455", "truth_parsed_k": 290.55, "prediction_parsed_k_with_c_conv": 286.87, "prediction_parsed_k_strict": 286.87, "absolute_error_k_vs_c_conv_pred": 3.680000000000007, "score_vs_c_conv_pred": 8.019624457567708, "absolute_error_k_vs_strict_pred": 3.680000000000007, "score_vs_pred_strict": 8.019624457567708, "truth_raw_assistant_text": "The value is 290.55", "prediction_raw_text": "The value is 286.87\u1fbe"}
{"id": "line_1456", "truth_parsed_k": 291.27, "prediction_parsed_k_with_c_conv": 286.96, "prediction_parsed_k_strict": 286.96, "absolute_error_k_vs_c_conv_pred": 4.310000000000002, "score_vs_c_conv_pred": 3.8939034079728985, "absolute_error_k_vs_strict_pred": 4.310000000000002, "score_vs_pred_strict": 3.8939034079728985, "truth_raw_assistant_text": "The value is 291.27", "prediction_raw_text": "The value is 286.96\u0e2a\u0e31\u0e21\u0e1c"}
{"id": "line_1457", "truth_parsed_k": 292.28, "prediction_parsed_k_with_c_conv": 284.76, "prediction_parsed_k_strict": 284.76, "absolute_error_k_vs_c_conv_pred": 7.519999999999982, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.519999999999982, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.28", "prediction_raw_text": "The value is 284.76\u0e2a\u0e2b\u0e23\u0e31"}
{"id": "line_1458", "truth_parsed_k": 293.11, "prediction_parsed_k_with_c_conv": 285.75, "prediction_parsed_k_strict": 285.75, "absolute_error_k_vs_c_conv_pred": 7.360000000000014, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.360000000000014, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.11", "prediction_raw_text": "The value is 285.75\u0e2d\u0e32\u0e23\u0e4c"}
{"id": "line_1459", "truth_parsed_k": 293.28, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 8.45999999999998, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.45999999999998, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.28", "prediction_raw_text": "The value is 284.82\u0e24\u0e14\u0e39"}
{"id": "line_1460", "truth_parsed_k": 293.31, "prediction_parsed_k_with_c_conv": 285.57, "prediction_parsed_k_strict": 285.57, "absolute_error_k_vs_c_conv_pred": 7.740000000000009, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.740000000000009, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.31", "prediction_raw_text": "The value is 285.57\u0e40\u0e2b\u0e25\u0e37\u0e2d"}
{"id": "line_1461", "truth_parsed_k": 292.5, "prediction_parsed_k_with_c_conv": 285.84, "prediction_parsed_k_strict": 285.84, "absolute_error_k_vs_c_conv_pred": 6.660000000000025, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.660000000000025, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.50", "prediction_raw_text": "The value is 285.84."}
{"id": "line_1462", "truth_parsed_k": 291.74, "prediction_parsed_k_with_c_conv": 285.94, "prediction_parsed_k_strict": 285.94, "absolute_error_k_vs_c_conv_pred": 5.800000000000011, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.800000000000011, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.74", "prediction_raw_text": "The value is 285.94\uf990"}
{"id": "line_1463", "truth_parsed_k": 290.9, "prediction_parsed_k_with_c_conv": 285.42, "prediction_parsed_k_strict": 285.42, "absolute_error_k_vs_c_conv_pred": 5.479999999999961, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.479999999999961, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.90", "prediction_raw_text": "The value is 285.42"}
{"id": "line_1464", "truth_parsed_k": 290.48, "prediction_parsed_k_with_c_conv": 285.32, "prediction_parsed_k_strict": 285.32, "absolute_error_k_vs_c_conv_pred": 5.160000000000025, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.160000000000025, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.48", "prediction_raw_text": "The value is 285.32"}
{"id": "line_1465", "truth_parsed_k": 290.18, "prediction_parsed_k_with_c_conv": 284.5, "prediction_parsed_k_strict": 284.5, "absolute_error_k_vs_c_conv_pred": 5.680000000000007, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.680000000000007, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.18", "prediction_raw_text": "The value is 284.50\u0e0d\u0e35\u0e48\u0e1b\u0e38\u0e48\u0e19"}
{"id": "line_1466", "truth_parsed_k": 290.51, "prediction_parsed_k_with_c_conv": 284.65, "prediction_parsed_k_strict": 284.65, "absolute_error_k_vs_c_conv_pred": 5.860000000000014, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.860000000000014, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.51", "prediction_raw_text": "The value is 284.65\u0e21\u0e19\u0e38\u0e29\u0e22\u0e4c"}
{"id": "line_1467", "truth_parsed_k": 290.96, "prediction_parsed_k_with_c_conv": 284.96, "prediction_parsed_k_strict": 284.96, "absolute_error_k_vs_c_conv_pred": 6.0, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.0, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.96", "prediction_raw_text": "The value is 284.96\u0e17\u0e23\u0e31\u0e1e\u0e22\u0e4c"}
{"id": "line_1468", "truth_parsed_k": 291.6, "prediction_parsed_k_with_c_conv": 286.34, "prediction_parsed_k_strict": 286.34, "absolute_error_k_vs_c_conv_pred": 5.260000000000048, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.260000000000048, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.60", "prediction_raw_text": "The value is 286.34."}
{"id": "line_1469", "truth_parsed_k": 292.36, "prediction_parsed_k_with_c_conv": 285.74, "prediction_parsed_k_strict": 285.74, "absolute_error_k_vs_c_conv_pred": 6.6200000000000045, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.6200000000000045, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.36", "prediction_raw_text": "The value is 285.74 \u0e18\u0e31\u0e19\u0e27\u0e32\u0e04\u0e21"}
{"id": "line_1470", "truth_parsed_k": 293.16, "prediction_parsed_k_with_c_conv": 285.05, "prediction_parsed_k_strict": 285.05, "absolute_error_k_vs_c_conv_pred": 8.110000000000014, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.110000000000014, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.16", "prediction_raw_text": "The value is 285.05\u0e1b\u0e25\u0e48\u0e2d\u0e22"}
{"id": "line_1471", "truth_parsed_k": 293.31, "prediction_parsed_k_with_c_conv": 284.78, "prediction_parsed_k_strict": 284.78, "absolute_error_k_vs_c_conv_pred": 8.53000000000003, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.53000000000003, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.31", "prediction_raw_text": "The value is 284.78."}
{"id": "line_1472", "truth_parsed_k": 293.11, "prediction_parsed_k_with_c_conv": 285.97, "prediction_parsed_k_strict": 285.97, "absolute_error_k_vs_c_conv_pred": 7.139999999999986, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.139999999999986, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.11", "prediction_raw_text": "The value is 285.97\u0e2a\u0e35\u0e48"}
{"id": "line_1473", "truth_parsed_k": 292.38, "prediction_parsed_k_with_c_conv": 284.96, "prediction_parsed_k_strict": 284.96, "absolute_error_k_vs_c_conv_pred": 7.420000000000016, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.420000000000016, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.38", "prediction_raw_text": "The value is 284.96."}
{"id": "line_1474", "truth_parsed_k": 291.58, "prediction_parsed_k_with_c_conv": 285.94, "prediction_parsed_k_strict": 285.94, "absolute_error_k_vs_c_conv_pred": 5.639999999999986, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.639999999999986, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.58", "prediction_raw_text": "The value is 285.94."}
{"id": "line_1475", "truth_parsed_k": 290.67, "prediction_parsed_k_with_c_conv": 285.84, "prediction_parsed_k_strict": 285.84, "absolute_error_k_vs_c_conv_pred": 4.830000000000041, "score_vs_c_conv_pred": 0.9083803043695826, "absolute_error_k_vs_strict_pred": 4.830000000000041, "score_vs_pred_strict": 0.9083803043695826, "truth_raw_assistant_text": "The value is 290.67", "prediction_raw_text": "The value is 285.84."}
{"id": "line_1476", "truth_parsed_k": 289.92, "prediction_parsed_k_with_c_conv": 285.42, "prediction_parsed_k_strict": 285.42, "absolute_error_k_vs_c_conv_pred": 4.5, "score_vs_c_conv_pred": 2.764296806852773, "absolute_error_k_vs_strict_pred": 4.5, "score_vs_pred_strict": 2.764296806852773, "truth_raw_assistant_text": "The value is 289.92", "prediction_raw_text": "The value is 285.42\u1fd3"}
{"id": "line_1477", "truth_parsed_k": 289.88, "prediction_parsed_k_with_c_conv": 284.67, "prediction_parsed_k_strict": 284.67, "absolute_error_k_vs_c_conv_pred": 5.2099999999999795, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.2099999999999795, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 289.88", "prediction_raw_text": "The value is 284.67\ufa04"}
{"id": "line_1478", "truth_parsed_k": 290.01, "prediction_parsed_k_with_c_conv": 285.98, "prediction_parsed_k_strict": 285.98, "absolute_error_k_vs_c_conv_pred": 4.029999999999973, "score_vs_c_conv_pred": 5.650034332126874, "absolute_error_k_vs_strict_pred": 4.029999999999973, "score_vs_pred_strict": 5.650034332126874, "truth_raw_assistant_text": "The value is 290.01", "prediction_raw_text": "The value is 285.98\uf951"}
{"id": "line_1479", "truth_parsed_k": 290.43, "prediction_parsed_k_with_c_conv": 286.75, "prediction_parsed_k_strict": 286.75, "absolute_error_k_vs_c_conv_pred": 3.680000000000007, "score_vs_c_conv_pred": 8.019624457567708, "absolute_error_k_vs_strict_pred": 3.680000000000007, "score_vs_pred_strict": 8.019624457567708, "truth_raw_assistant_text": "The value is 290.43", "prediction_raw_text": "The value is 286.75\ufa1c"}
{"id": "line_1480", "truth_parsed_k": 291.23, "prediction_parsed_k_with_c_conv": 284.77, "prediction_parsed_k_strict": 284.77, "absolute_error_k_vs_c_conv_pred": 6.460000000000036, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.460000000000036, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.23", "prediction_raw_text": "The value is 284.77\u0e44\u0e1b\u0e14\u0e49\u0e27\u0e22"}
{"id": "line_1481", "truth_parsed_k": 292.01, "prediction_parsed_k_with_c_conv": 284.71, "prediction_parsed_k_strict": 284.71, "absolute_error_k_vs_c_conv_pred": 7.300000000000011, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.300000000000011, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.01", "prediction_raw_text": "The value is 284.71"}
{"id": "line_1482", "truth_parsed_k": 292.66, "prediction_parsed_k_with_c_conv": 284.84, "prediction_parsed_k_strict": 284.84, "absolute_error_k_vs_c_conv_pred": 7.82000000000005, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.82000000000005, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.66", "prediction_raw_text": "The value is 284.84"}
{"id": "line_1483", "truth_parsed_k": 293.0, "prediction_parsed_k_with_c_conv": 285.84, "prediction_parsed_k_strict": 285.84, "absolute_error_k_vs_c_conv_pred": 7.160000000000025, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.160000000000025, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.00", "prediction_raw_text": "The value is 285.84\u0e40\u0e07\u0e37\u0e48\u0e2d\u0e19\u0e44\u0e02"}
{"id": "line_1484", "truth_parsed_k": 292.83, "prediction_parsed_k_with_c_conv": 285.92, "prediction_parsed_k_strict": 285.92, "absolute_error_k_vs_c_conv_pred": 6.909999999999968, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.909999999999968, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.83", "prediction_raw_text": "The value is 285.92."}
{"id": "line_1485", "truth_parsed_k": 292.09, "prediction_parsed_k_with_c_conv": 286.6, "prediction_parsed_k_strict": 286.6, "absolute_error_k_vs_c_conv_pred": 5.489999999999952, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.489999999999952, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.09", "prediction_raw_text": "The value is 286.60\u0e40\u0e0b\u0e47"}
{"id": "line_1486", "truth_parsed_k": 291.33, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 6.359999999999957, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.359999999999957, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.33", "prediction_raw_text": "The value is 284.97\uf949"}
{"id": "line_1487", "truth_parsed_k": 290.62, "prediction_parsed_k_with_c_conv": 284.54, "prediction_parsed_k_strict": 284.54, "absolute_error_k_vs_c_conv_pred": 6.079999999999984, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.079999999999984, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.62", "prediction_raw_text": "The value is 284.54\u0e40\u0e0a\u0e34\u0e0d"}
{"id": "line_1488", "truth_parsed_k": 289.94, "prediction_parsed_k_with_c_conv": 286.51, "prediction_parsed_k_strict": 286.51, "absolute_error_k_vs_c_conv_pred": 3.430000000000007, "score_vs_c_conv_pred": 9.849693373363877, "absolute_error_k_vs_strict_pred": 3.430000000000007, "score_vs_pred_strict": 9.849693373363877, "truth_raw_assistant_text": "The value is 289.94", "prediction_raw_text": "The value is 286.51\u0e32\u0e23\u0e4c\u0e14"}
{"id": "line_1489", "truth_parsed_k": 289.74, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 4.920000000000016, "score_vs_c_conv_pred": 0.42365870208784795, "absolute_error_k_vs_strict_pred": 4.920000000000016, "score_vs_pred_strict": 0.42365870208784795, "truth_raw_assistant_text": "The value is 289.74", "prediction_raw_text": "The value is 284.82\u0e01\u0e38"}
{"id": "line_1490", "truth_parsed_k": 290.02, "prediction_parsed_k_with_c_conv": 286.22, "prediction_parsed_k_strict": 286.22, "absolute_error_k_vs_c_conv_pred": 3.7999999999999545, "score_vs_c_conv_pred": 7.183492577992645, "absolute_error_k_vs_strict_pred": 3.7999999999999545, "score_vs_pred_strict": 7.183492577992645, "truth_raw_assistant_text": "The value is 290.02", "prediction_raw_text": "The value is 286.22\uf949"}
{"id": "line_1491", "truth_parsed_k": 290.46, "prediction_parsed_k_with_c_conv": 284.5, "prediction_parsed_k_strict": 284.5, "absolute_error_k_vs_c_conv_pred": 5.9599999999999795, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.9599999999999795, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.46", "prediction_raw_text": "The value is 284.50 \u0e01\u0e31\u0e19"}
{"id": "line_1492", "truth_parsed_k": 291.15, "prediction_parsed_k_with_c_conv": 285.74, "prediction_parsed_k_strict": 285.74, "absolute_error_k_vs_c_conv_pred": 5.409999999999968, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.409999999999968, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.15", "prediction_raw_text": "The value is 285.74\u0e25\u0e38\u0e01"}
{"id": "line_1493", "truth_parsed_k": 292.08, "prediction_parsed_k_with_c_conv": 285.99, "prediction_parsed_k_strict": 285.99, "absolute_error_k_vs_c_conv_pred": 6.089999999999975, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.089999999999975, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.08", "prediction_raw_text": "The value is 285.99\u0e2b\u0e31\u0e19"}
{"id": "line_1494", "truth_parsed_k": 292.67, "prediction_parsed_k_with_c_conv": 286.97, "prediction_parsed_k_strict": 286.97, "absolute_error_k_vs_c_conv_pred": 5.699999999999989, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.699999999999989, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.67", "prediction_raw_text": "The value is 286.97\u0e07\u0e04\u0e4c"}
{"id": "line_1495", "truth_parsed_k": 293.02, "prediction_parsed_k_with_c_conv": 286.7, "prediction_parsed_k_strict": 286.7, "absolute_error_k_vs_c_conv_pred": 6.319999999999993, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.319999999999993, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.02", "prediction_raw_text": "The value is 286.70"}
{"id": "line_1496", "truth_parsed_k": 292.76, "prediction_parsed_k_with_c_conv": 287.86, "prediction_parsed_k_strict": 287.86, "absolute_error_k_vs_c_conv_pred": 4.899999999999977, "score_vs_c_conv_pred": 0.5306230961831027, "absolute_error_k_vs_strict_pred": 4.899999999999977, "score_vs_pred_strict": 0.5306230961831027, "truth_raw_assistant_text": "The value is 292.76", "prediction_raw_text": "The value is 287.86"}
{"id": "line_1497", "truth_parsed_k": 292.08, "prediction_parsed_k_with_c_conv": 285.59, "prediction_parsed_k_strict": 285.59, "absolute_error_k_vs_c_conv_pred": 6.490000000000009, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.490000000000009, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.08", "prediction_raw_text": "The value is 285.59."}
{"id": "line_1498", "truth_parsed_k": 291.29, "prediction_parsed_k_with_c_conv": 285.72, "prediction_parsed_k_strict": 285.72, "absolute_error_k_vs_c_conv_pred": 5.569999999999993, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.569999999999993, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.29", "prediction_raw_text": "The value is 285.72\u0e0a\u0e31\u0e48\u0e27"}
{"id": "line_1499", "truth_parsed_k": 290.51, "prediction_parsed_k_with_c_conv": 284.77, "prediction_parsed_k_strict": 284.77, "absolute_error_k_vs_c_conv_pred": 5.740000000000009, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.740000000000009, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.51", "prediction_raw_text": "The value is 284.77\u0e27\u0e31\u0e2a\u0e14"}
{"id": "line_1500", "truth_parsed_k": 289.81, "prediction_parsed_k_with_c_conv": 285.97, "prediction_parsed_k_strict": 285.97, "absolute_error_k_vs_c_conv_pred": 3.839999999999975, "score_vs_c_conv_pred": 6.91045347520115, "absolute_error_k_vs_strict_pred": 3.839999999999975, "score_vs_pred_strict": 6.91045347520115, "truth_raw_assistant_text": "The value is 289.81", "prediction_raw_text": "The value is 285.97"}
{"id": "line_1501", "truth_parsed_k": 289.75, "prediction_parsed_k_with_c_conv": 285.92, "prediction_parsed_k_strict": 285.92, "absolute_error_k_vs_c_conv_pred": 3.829999999999984, "score_vs_c_conv_pred": 6.978454144686818, "absolute_error_k_vs_strict_pred": 3.829999999999984, "score_vs_pred_strict": 6.978454144686818, "truth_raw_assistant_text": "The value is 289.75", "prediction_raw_text": "The value is 285.92\u0e17\u0e38\u0e01\u0e04\u0e19"}
{"id": "line_1502", "truth_parsed_k": 289.91, "prediction_parsed_k_with_c_conv": 285.6, "prediction_parsed_k_strict": 285.6, "absolute_error_k_vs_c_conv_pred": 4.310000000000002, "score_vs_c_conv_pred": 3.8939034079728985, "absolute_error_k_vs_strict_pred": 4.310000000000002, "score_vs_pred_strict": 3.8939034079728985, "truth_raw_assistant_text": "The value is 289.91", "prediction_raw_text": "The value is 285.60"}
{"id": "line_1503", "truth_parsed_k": 290.4, "prediction_parsed_k_with_c_conv": 284.52, "prediction_parsed_k_strict": 284.52, "absolute_error_k_vs_c_conv_pred": 5.8799999999999955, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.8799999999999955, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.40", "prediction_raw_text": "The value is 284.52\u0e25\u0e31\u0e01\u0e29\u0e13\u0e30"}
{"id": "line_1504", "truth_parsed_k": 291.33, "prediction_parsed_k_with_c_conv": 284.78, "prediction_parsed_k_strict": 284.78, "absolute_error_k_vs_c_conv_pred": 6.550000000000011, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.550000000000011, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.33", "prediction_raw_text": "The value is 284.78\u0e40\u0e21\u0e37\u0e48\u0e2d\u0e27\u0e31\u0e19\u0e17\u0e35\u0e48"}
{"id": "line_1505", "truth_parsed_k": 292.13, "prediction_parsed_k_with_c_conv": 284.88, "prediction_parsed_k_strict": 284.88, "absolute_error_k_vs_c_conv_pred": 7.25, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.25, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.13", "prediction_raw_text": "The value is 284.88\u0e1d\u0e23\u0e31\u0e48\u0e07\u0e40\u0e28\u0e2a"}
{"id": "line_1506", "truth_parsed_k": 292.78, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 7.9599999999999795, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.9599999999999795, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.78", "prediction_raw_text": "The value is 284.82\uf978"}
{"id": "line_1507", "truth_parsed_k": 293.14, "prediction_parsed_k_with_c_conv": 286.38, "prediction_parsed_k_strict": 286.38, "absolute_error_k_vs_c_conv_pred": 6.759999999999991, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.759999999999991, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.14", "prediction_raw_text": "The value is 286.38\u0e1d\u0e23\u0e31\u0e48\u0e07\u0e40\u0e28"}
{"id": "line_1508", "truth_parsed_k": 293.08, "prediction_parsed_k_with_c_conv": 286.24, "prediction_parsed_k_strict": 286.24, "absolute_error_k_vs_c_conv_pred": 6.839999999999975, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.839999999999975, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.08", "prediction_raw_text": "The value is 286.24."}
{"id": "line_1509", "truth_parsed_k": 292.42, "prediction_parsed_k_with_c_conv": 285.82, "prediction_parsed_k_strict": 285.82, "absolute_error_k_vs_c_conv_pred": 6.600000000000023, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.600000000000023, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.42", "prediction_raw_text": "The value is 285.82\u0e40\u0e2b\u0e21\u0e37\u0e2d\u0e19\u0e01\u0e31\u0e19"}
{"id": "line_1510", "truth_parsed_k": 291.55, "prediction_parsed_k_with_c_conv": 284.84, "prediction_parsed_k_strict": 284.84, "absolute_error_k_vs_c_conv_pred": 6.710000000000036, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.710000000000036, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.55", "prediction_raw_text": "The value is 284.84."}
{"id": "line_1511", "truth_parsed_k": 290.72, "prediction_parsed_k_with_c_conv": 285.8, "prediction_parsed_k_strict": 285.8, "absolute_error_k_vs_c_conv_pred": 4.920000000000016, "score_vs_c_conv_pred": 0.42365870208784795, "absolute_error_k_vs_strict_pred": 4.920000000000016, "score_vs_pred_strict": 0.42365870208784795, "truth_raw_assistant_text": "The value is 290.72", "prediction_raw_text": "The value is 285.80\u0e2a\u0e21\u0e31"}
{"id": "line_1512", "truth_parsed_k": 290.16, "prediction_parsed_k_with_c_conv": 284.77, "prediction_parsed_k_strict": 284.77, "absolute_error_k_vs_c_conv_pred": 5.390000000000043, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.390000000000043, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.16", "prediction_raw_text": "The value is 284.77\u0e25\u0e31\u0e01\u0e29"}
{"id": "line_1513", "truth_parsed_k": 290.09, "prediction_parsed_k_with_c_conv": 288.05, "prediction_parsed_k_strict": 288.05, "absolute_error_k_vs_c_conv_pred": 2.0399999999999636, "score_vs_c_conv_pred": 23.20434740707158, "absolute_error_k_vs_strict_pred": 2.0399999999999636, "score_vs_pred_strict": 23.20434740707158, "truth_raw_assistant_text": "The value is 290.09", "prediction_raw_text": "The value is 288.05\u0e14\u0e49\u0e32\u0e19\u0e25"}
{"id": "line_1514", "truth_parsed_k": 290.29, "prediction_parsed_k_with_c_conv": 284.64, "prediction_parsed_k_strict": 284.64, "absolute_error_k_vs_c_conv_pred": 5.650000000000034, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.650000000000034, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.29", "prediction_raw_text": "The value is 284.64\u0e28\u0e34\u0e25\u0e1b"}
{"id": "line_1515", "truth_parsed_k": 290.53, "prediction_parsed_k_with_c_conv": 284.58, "prediction_parsed_k_strict": 284.58, "absolute_error_k_vs_c_conv_pred": 5.949999999999989, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.949999999999989, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.53", "prediction_raw_text": "The value is 284.58."}
{"id": "line_1516", "truth_parsed_k": 291.36, "prediction_parsed_k_with_c_conv": 285.52, "prediction_parsed_k_strict": 285.52, "absolute_error_k_vs_c_conv_pred": 5.840000000000032, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.840000000000032, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.36", "prediction_raw_text": "The value is 285.52\u0e21\u0e32\u0e41\u0e25\u0e49\u0e27"}
{"id": "line_1517", "truth_parsed_k": 292.19, "prediction_parsed_k_with_c_conv": 285.36, "prediction_parsed_k_strict": 285.36, "absolute_error_k_vs_c_conv_pred": 6.829999999999984, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.829999999999984, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.19", "prediction_raw_text": "The value is 285.36."}
{"id": "line_1518", "truth_parsed_k": 293.02, "prediction_parsed_k_with_c_conv": 285.5, "prediction_parsed_k_strict": 285.5, "absolute_error_k_vs_c_conv_pred": 7.519999999999982, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.519999999999982, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.02", "prediction_raw_text": "The value is 285.50\uf929"}
{"id": "line_1519", "truth_parsed_k": 293.23, "prediction_parsed_k_with_c_conv": 285.42, "prediction_parsed_k_strict": 285.42, "absolute_error_k_vs_c_conv_pred": 7.810000000000002, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.810000000000002, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.23", "prediction_raw_text": "The value is 285.42\u0e1c\u0e34\u0e27"}
{"id": "line_1520", "truth_parsed_k": 293.22, "prediction_parsed_k_with_c_conv": 286.87, "prediction_parsed_k_strict": 286.87, "absolute_error_k_vs_c_conv_pred": 6.350000000000023, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.350000000000023, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.22", "prediction_raw_text": "The value is 286.87\u0e44\u0e23\u0e48"}
{"id": "line_1521", "truth_parsed_k": 292.42, "prediction_parsed_k_with_c_conv": 285.7, "prediction_parsed_k_strict": 285.7, "absolute_error_k_vs_c_conv_pred": 6.720000000000027, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.720000000000027, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.42", "prediction_raw_text": "The value is 285.70\u0e24\u0e14\u0e39"}
{"id": "line_1522", "truth_parsed_k": 291.52, "prediction_parsed_k_with_c_conv": 285.5, "prediction_parsed_k_strict": 285.5, "absolute_error_k_vs_c_conv_pred": 6.019999999999982, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.019999999999982, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.52", "prediction_raw_text": "The value is 285.50\u0e27\u0e34\u0e14\u0e35\u0e42\u0e2d"}
{"id": "line_1523", "truth_parsed_k": 290.99, "prediction_parsed_k_with_c_conv": 285.59, "prediction_parsed_k_strict": 285.59, "absolute_error_k_vs_c_conv_pred": 5.400000000000034, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.400000000000034, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.99", "prediction_raw_text": "The value is 285.59."}
{"id": "line_1524", "truth_parsed_k": 290.46, "prediction_parsed_k_with_c_conv": 284.92, "prediction_parsed_k_strict": 284.92, "absolute_error_k_vs_c_conv_pred": 5.539999999999964, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.539999999999964, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.46", "prediction_raw_text": "The value is 284.92\u0e40\u0e0b\u0e2d\u0e23\u0e4c"}
{"id": "line_1525", "truth_parsed_k": 290.15, "prediction_parsed_k_with_c_conv": 288.06, "prediction_parsed_k_strict": 288.06, "absolute_error_k_vs_c_conv_pred": 2.089999999999975, "score_vs_c_conv_pred": 22.589520452523903, "absolute_error_k_vs_strict_pred": 2.089999999999975, "score_vs_pred_strict": 22.589520452523903, "truth_raw_assistant_text": "The value is 290.15", "prediction_raw_text": "The value is 288.06"}
{"id": "line_1526", "truth_parsed_k": 290.41, "prediction_parsed_k_with_c_conv": 285.9, "prediction_parsed_k_strict": 285.9, "absolute_error_k_vs_c_conv_pred": 4.510000000000048, "score_vs_c_conv_pred": 2.706136415330951, "absolute_error_k_vs_strict_pred": 4.510000000000048, "score_vs_pred_strict": 2.706136415330951, "truth_raw_assistant_text": "The value is 290.41", "prediction_raw_text": "The value is 285.90\u0e23\u0e13\u0e4c"}
{"id": "line_1527", "truth_parsed_k": 290.94, "prediction_parsed_k_with_c_conv": 284.81, "prediction_parsed_k_strict": 284.81, "absolute_error_k_vs_c_conv_pred": 6.1299999999999955, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.1299999999999955, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.94", "prediction_raw_text": "The value is 284.81\uf9b8"}
{"id": "line_1528", "truth_parsed_k": 291.55, "prediction_parsed_k_with_c_conv": 286.86, "prediction_parsed_k_strict": 286.86, "absolute_error_k_vs_c_conv_pred": 4.689999999999998, "score_vs_c_conv_pred": 1.6801742003018982, "absolute_error_k_vs_strict_pred": 4.689999999999998, "score_vs_pred_strict": 1.6801742003018982, "truth_raw_assistant_text": "The value is 291.55", "prediction_raw_text": "The value is 286.86"}
{"id": "line_1529", "truth_parsed_k": 292.38, "prediction_parsed_k_with_c_conv": 284.83, "prediction_parsed_k_strict": 284.83, "absolute_error_k_vs_c_conv_pred": 7.550000000000011, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.550000000000011, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.38", "prediction_raw_text": "The value is 284.83\u0e28\u0e34\u0e25\u0e1b"}
{"id": "line_1530", "truth_parsed_k": 293.29, "prediction_parsed_k_with_c_conv": 285.54, "prediction_parsed_k_strict": 285.54, "absolute_error_k_vs_c_conv_pred": 7.75, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.75, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.29", "prediction_raw_text": "The value is 285.54."}
{"id": "line_1531", "truth_parsed_k": 293.56, "prediction_parsed_k_with_c_conv": 284.76, "prediction_parsed_k_strict": 284.76, "absolute_error_k_vs_c_conv_pred": 8.800000000000011, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.800000000000011, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.56", "prediction_raw_text": "The value is 284.76\ufb1f"}
{"id": "line_1532", "truth_parsed_k": 293.38, "prediction_parsed_k_with_c_conv": 284.95, "prediction_parsed_k_strict": 284.95, "absolute_error_k_vs_c_conv_pred": 8.430000000000007, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.430000000000007, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.38", "prediction_raw_text": "The value is 284.95\u0e41\u0e25\u0e19\u0e14\u0e4c"}
{"id": "line_1533", "truth_parsed_k": 292.7, "prediction_parsed_k_with_c_conv": 284.96, "prediction_parsed_k_strict": 284.96, "absolute_error_k_vs_c_conv_pred": 7.740000000000009, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.740000000000009, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.70", "prediction_raw_text": "The value is 284.96\u0e40\u0e25\u0e34"}
{"id": "line_1534", "truth_parsed_k": 291.76, "prediction_parsed_k_with_c_conv": 285.94, "prediction_parsed_k_strict": 285.94, "absolute_error_k_vs_c_conv_pred": 5.819999999999993, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.819999999999993, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.76", "prediction_raw_text": "The value is 285.94."}
{"id": "line_1535", "truth_parsed_k": 290.96, "prediction_parsed_k_with_c_conv": 285.47, "prediction_parsed_k_strict": 285.47, "absolute_error_k_vs_c_conv_pred": 5.489999999999952, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.489999999999952, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.96", "prediction_raw_text": "The value is 285.47"}
{"id": "line_1536", "truth_parsed_k": 290.53, "prediction_parsed_k_with_c_conv": 284.62, "prediction_parsed_k_strict": 284.62, "absolute_error_k_vs_c_conv_pred": 5.909999999999968, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.909999999999968, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.53", "prediction_raw_text": "The value is 284.62\u0e40\u0e27\u0e2d\u0e23\u0e4c"}
{"id": "line_1537", "truth_parsed_k": 290.12, "prediction_parsed_k_with_c_conv": 285.62, "prediction_parsed_k_strict": 285.62, "absolute_error_k_vs_c_conv_pred": 4.5, "score_vs_c_conv_pred": 2.764296806852773, "absolute_error_k_vs_strict_pred": 4.5, "score_vs_pred_strict": 2.764296806852773, "truth_raw_assistant_text": "The value is 290.12", "prediction_raw_text": "The value is 285.62\u0e2d\u0e37\u0e48\u0e19\u0e46"}
{"id": "line_1538", "truth_parsed_k": 290.35, "prediction_parsed_k_with_c_conv": 284.91, "prediction_parsed_k_strict": 284.91, "absolute_error_k_vs_c_conv_pred": 5.439999999999998, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.439999999999998, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.35", "prediction_raw_text": "The value is 284.91 \u0e16\u0e49\u0e32"}
{"id": "line_1539", "truth_parsed_k": 290.91, "prediction_parsed_k_with_c_conv": 284.65, "prediction_parsed_k_strict": 284.65, "absolute_error_k_vs_c_conv_pred": 6.260000000000048, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.260000000000048, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.91", "prediction_raw_text": "The value is 284.65\u0e2d\u0e32\u0e08\u0e32\u0e23\u0e22\u0e4c"}
{"id": "line_1540", "truth_parsed_k": 291.62, "prediction_parsed_k_with_c_conv": 285.72, "prediction_parsed_k_strict": 285.72, "absolute_error_k_vs_c_conv_pred": 5.899999999999977, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.899999999999977, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.62", "prediction_raw_text": "The value is 285.72\u0e22\u0e31\u0e07\u0e44\u0e21\u0e48"}
{"id": "line_1541", "truth_parsed_k": 292.42, "prediction_parsed_k_with_c_conv": 285.72, "prediction_parsed_k_strict": 285.72, "absolute_error_k_vs_c_conv_pred": 6.699999999999989, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.699999999999989, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.42", "prediction_raw_text": "The value is 285.72\u0e04\u0e23\u0e34"}
{"id": "line_1542", "truth_parsed_k": 293.33, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 8.359999999999957, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.359999999999957, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.33", "prediction_raw_text": "The value is 284.97\u0e2a\u0e33\u0e40\u0e23\u0e47"}
{"id": "line_1543", "truth_parsed_k": 293.67, "prediction_parsed_k_with_c_conv": 284.89, "prediction_parsed_k_strict": 284.89, "absolute_error_k_vs_c_conv_pred": 8.78000000000003, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.78000000000003, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.67", "prediction_raw_text": "The value is 284.89."}
{"id": "line_1544", "truth_parsed_k": 293.51, "prediction_parsed_k_with_c_conv": 285.81, "prediction_parsed_k_strict": 285.81, "absolute_error_k_vs_c_conv_pred": 7.699999999999989, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.699999999999989, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.51", "prediction_raw_text": "The value is 285.81\u0e2d\u0e19\u0e38\u0e0d\u0e32"}
{"id": "line_1545", "truth_parsed_k": 292.92, "prediction_parsed_k_with_c_conv": 285.84, "prediction_parsed_k_strict": 285.84, "absolute_error_k_vs_c_conv_pred": 7.080000000000041, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.080000000000041, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.92", "prediction_raw_text": "The value is 285.84\u0e08\u0e30\u0e21\u0e35"}
{"id": "line_1546", "truth_parsed_k": 291.95, "prediction_parsed_k_with_c_conv": 284.95, "prediction_parsed_k_strict": 284.95, "absolute_error_k_vs_c_conv_pred": 7.0, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.0, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.95", "prediction_raw_text": "The value is 284.95\u0e1c\u0e48\u0e32\u0e19"}
{"id": "line_1547", "truth_parsed_k": 291.17, "prediction_parsed_k_with_c_conv": 285.68, "prediction_parsed_k_strict": 285.68, "absolute_error_k_vs_c_conv_pred": 5.490000000000009, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.490000000000009, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.17", "prediction_raw_text": "The value is 285.68."}
{"id": "line_1548", "truth_parsed_k": 290.57, "prediction_parsed_k_with_c_conv": 285.97, "prediction_parsed_k_strict": 285.97, "absolute_error_k_vs_c_conv_pred": 4.599999999999966, "score_vs_c_conv_pred": 2.1882694779878475, "absolute_error_k_vs_strict_pred": 4.599999999999966, "score_vs_pred_strict": 2.1882694779878475, "truth_raw_assistant_text": "The value is 290.57", "prediction_raw_text": "The value is 285.97\u0e1a\u0e34\u0e19"}
{"id": "line_1549", "truth_parsed_k": 290.33, "prediction_parsed_k_with_c_conv": 284.88, "prediction_parsed_k_strict": 284.88, "absolute_error_k_vs_c_conv_pred": 5.449999999999989, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.449999999999989, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.33", "prediction_raw_text": "The value is 284.88\u0e1b\u0e01\u0e15\u0e34"}
{"id": "line_1550", "truth_parsed_k": 290.39, "prediction_parsed_k_with_c_conv": 284.92, "prediction_parsed_k_strict": 284.92, "absolute_error_k_vs_c_conv_pred": 5.46999999999997, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.46999999999997, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.39", "prediction_raw_text": "The value is 284.92"}
{"id": "line_1551", "truth_parsed_k": 290.7, "prediction_parsed_k_with_c_conv": 285.77, "prediction_parsed_k_strict": 285.77, "absolute_error_k_vs_c_conv_pred": 4.930000000000007, "score_vs_c_conv_pred": 0.3703354157400529, "absolute_error_k_vs_strict_pred": 4.930000000000007, "score_vs_pred_strict": 0.3703354157400529, "truth_raw_assistant_text": "The value is 290.70", "prediction_raw_text": "The value is 285.77\u0e17\u0e38\u0e01\u0e04\u0e19"}
{"id": "line_1552", "truth_parsed_k": 291.49, "prediction_parsed_k_with_c_conv": 285.49, "prediction_parsed_k_strict": 285.49, "absolute_error_k_vs_c_conv_pred": 6.0, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.0, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.49", "prediction_raw_text": "The value is 285.49"}
{"id": "line_1553", "truth_parsed_k": 292.58, "prediction_parsed_k_with_c_conv": 284.9, "prediction_parsed_k_strict": 284.9, "absolute_error_k_vs_c_conv_pred": 7.680000000000007, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.680000000000007, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.58", "prediction_raw_text": "The value is 284.90\u0e22\u0e31\u0e07\u0e44"}
{"id": "line_1554", "truth_parsed_k": 293.31, "prediction_parsed_k_with_c_conv": 286.84, "prediction_parsed_k_strict": 286.84, "absolute_error_k_vs_c_conv_pred": 6.470000000000027, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.470000000000027, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.31", "prediction_raw_text": "The value is 286.84."}
{"id": "line_1555", "truth_parsed_k": 293.62, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 8.649999999999977, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.649999999999977, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.62", "prediction_raw_text": "The value is 284.97\uf9e7"}
{"id": "line_1556", "truth_parsed_k": 293.47, "prediction_parsed_k_with_c_conv": 284.83, "prediction_parsed_k_strict": 284.83, "absolute_error_k_vs_c_conv_pred": 8.640000000000043, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.640000000000043, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.47", "prediction_raw_text": "The value is 284.83\u0e2b\u0e19\u0e48\u0e27\u0e22"}
{"id": "line_1557", "truth_parsed_k": 292.68, "prediction_parsed_k_with_c_conv": 285.81, "prediction_parsed_k_strict": 285.81, "absolute_error_k_vs_c_conv_pred": 6.8700000000000045, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.8700000000000045, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.68", "prediction_raw_text": "The value is 285.81\u0e14\u0e49\u0e27\u0e22\u0e01\u0e31\u0e19"}
{"id": "line_1558", "truth_parsed_k": 291.82, "prediction_parsed_k_with_c_conv": 286.37, "prediction_parsed_k_strict": 286.37, "absolute_error_k_vs_c_conv_pred": 5.449999999999989, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.449999999999989, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.82", "prediction_raw_text": "The value is 286.37\u0e2a\u0e35\u0e48"}
{"id": "line_1559", "truth_parsed_k": 291.18, "prediction_parsed_k_with_c_conv": 285.78, "prediction_parsed_k_strict": 285.78, "absolute_error_k_vs_c_conv_pred": 5.400000000000034, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.400000000000034, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.18", "prediction_raw_text": "The value is 285.78\u0e43\u0e19\u0e0a\u0e48\u0e27\u0e07"}
{"id": "line_1560", "truth_parsed_k": 290.53, "prediction_parsed_k_with_c_conv": 285.64, "prediction_parsed_k_strict": 285.64, "absolute_error_k_vs_c_conv_pred": 4.889999999999986, "score_vs_c_conv_pred": 0.584265047798882, "absolute_error_k_vs_strict_pred": 4.889999999999986, "score_vs_pred_strict": 0.584265047798882, "truth_raw_assistant_text": "The value is 290.53", "prediction_raw_text": "The value is 285.64."}
{"id": "line_1561", "truth_parsed_k": 290.36, "prediction_parsed_k_with_c_conv": 285.8, "prediction_parsed_k_strict": 285.8, "absolute_error_k_vs_c_conv_pred": 4.560000000000002, "score_vs_c_conv_pred": 2.417203913690291, "absolute_error_k_vs_strict_pred": 4.560000000000002, "score_vs_pred_strict": 2.417203913690291, "truth_raw_assistant_text": "The value is 290.36", "prediction_raw_text": "The value is 285.80\uf9d1"}
{"id": "line_1562", "truth_parsed_k": 290.46, "prediction_parsed_k_with_c_conv": 285.87, "prediction_parsed_k_strict": 285.87, "absolute_error_k_vs_c_conv_pred": 4.589999999999975, "score_vs_c_conv_pred": 2.2453208781553435, "absolute_error_k_vs_strict_pred": 4.589999999999975, "score_vs_pred_strict": 2.2453208781553435, "truth_raw_assistant_text": "The value is 290.46", "prediction_raw_text": "The value is 285.87."}
{"id": "line_1563", "truth_parsed_k": 290.79, "prediction_parsed_k_with_c_conv": 286.67, "prediction_parsed_k_strict": 286.67, "absolute_error_k_vs_c_conv_pred": 4.1200000000000045, "score_vs_c_conv_pred": 5.072978304372855, "absolute_error_k_vs_strict_pred": 4.1200000000000045, "score_vs_pred_strict": 5.072978304372855, "truth_raw_assistant_text": "The value is 290.79", "prediction_raw_text": "The value is 286.67\ufb3c"}
{"id": "line_1564", "truth_parsed_k": 291.69, "prediction_parsed_k_with_c_conv": 284.89, "prediction_parsed_k_strict": 284.89, "absolute_error_k_vs_c_conv_pred": 6.800000000000011, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.800000000000011, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.69", "prediction_raw_text": "The value is 284.89."}
{"id": "line_1565", "truth_parsed_k": 292.5, "prediction_parsed_k_with_c_conv": 285.24, "prediction_parsed_k_strict": 285.24, "absolute_error_k_vs_c_conv_pred": 7.259999999999991, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.259999999999991, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.50", "prediction_raw_text": "The value is 285.24\uf9df"}
{"id": "line_1566", "truth_parsed_k": 293.22, "prediction_parsed_k_with_c_conv": 284.64, "prediction_parsed_k_strict": 284.64, "absolute_error_k_vs_c_conv_pred": 8.580000000000041, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.580000000000041, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.22", "prediction_raw_text": "The value is 284.64 \u062c\u062f\u0627\u064b"}
{"id": "line_1567", "truth_parsed_k": 293.43, "prediction_parsed_k_with_c_conv": 286.39, "prediction_parsed_k_strict": 286.39, "absolute_error_k_vs_c_conv_pred": 7.0400000000000205, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.0400000000000205, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.43", "prediction_raw_text": "The value is 286.39\u0e40\u0e23\u0e35\u0e22\u0e1a"}
{"id": "line_1568", "truth_parsed_k": 293.25, "prediction_parsed_k_with_c_conv": 285.5, "prediction_parsed_k_strict": 285.5, "absolute_error_k_vs_c_conv_pred": 7.75, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.75, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.25", "prediction_raw_text": "The value is 285.50\u0e1e\u0e38"}
{"id": "line_1569", "truth_parsed_k": 292.58, "prediction_parsed_k_with_c_conv": 285.43, "prediction_parsed_k_strict": 285.43, "absolute_error_k_vs_c_conv_pred": 7.149999999999977, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.149999999999977, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.58", "prediction_raw_text": "The value is 285.43\u0e41\u0e2b\u0e25\u0e48\u0e07"}
{"id": "line_1570", "truth_parsed_k": 291.64, "prediction_parsed_k_with_c_conv": 284.74, "prediction_parsed_k_strict": 284.74, "absolute_error_k_vs_c_conv_pred": 6.899999999999977, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.899999999999977, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.64", "prediction_raw_text": "The value is 284.74\u0e40\u0e1b\u0e34\u0e14\u0e15\u0e31\u0e27"}
{"id": "line_1571", "truth_parsed_k": 290.66, "prediction_parsed_k_with_c_conv": 284.96, "prediction_parsed_k_strict": 284.96, "absolute_error_k_vs_c_conv_pred": 5.7000000000000455, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.7000000000000455, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.66", "prediction_raw_text": "The value is 284.96\ufa01"}
{"id": "line_1572", "truth_parsed_k": 290.2, "prediction_parsed_k_with_c_conv": 287.88, "prediction_parsed_k_strict": 287.88, "absolute_error_k_vs_c_conv_pred": 2.319999999999993, "score_vs_c_conv_pred": 19.92920532586956, "absolute_error_k_vs_strict_pred": 2.319999999999993, "score_vs_pred_strict": 19.92920532586956, "truth_raw_assistant_text": "The value is 290.20", "prediction_raw_text": "The value is 287.88\u0e40\u0e01\u0e47\u0e1a"}
{"id": "line_1573", "truth_parsed_k": 290.01, "prediction_parsed_k_with_c_conv": 284.42, "prediction_parsed_k_strict": 284.42, "absolute_error_k_vs_c_conv_pred": 5.589999999999975, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.589999999999975, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.01", "prediction_raw_text": "The value is 284.42"}
{"id": "line_1574", "truth_parsed_k": 290.27, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 5.449999999999989, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.449999999999989, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.27", "prediction_raw_text": "The value is 284.82"}
{"id": "line_1575", "truth_parsed_k": 290.58, "prediction_parsed_k_with_c_conv": 285.86, "prediction_parsed_k_strict": 285.86, "absolute_error_k_vs_c_conv_pred": 4.71999999999997, "score_vs_c_conv_pred": 1.512917446398443, "absolute_error_k_vs_strict_pred": 4.71999999999997, "score_vs_pred_strict": 1.512917446398443, "truth_raw_assistant_text": "The value is 290.58", "prediction_raw_text": "The value is 285.86."}
{"id": "line_1576", "truth_parsed_k": 291.47, "prediction_parsed_k_with_c_conv": 285.87, "prediction_parsed_k_strict": 285.87, "absolute_error_k_vs_c_conv_pred": 5.600000000000023, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.600000000000023, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.47", "prediction_raw_text": "The value is 285.87\ufb1f"}
{"id": "line_1577", "truth_parsed_k": 292.19, "prediction_parsed_k_with_c_conv": 285.82, "prediction_parsed_k_strict": 285.82, "absolute_error_k_vs_c_conv_pred": 6.3700000000000045, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.3700000000000045, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.19", "prediction_raw_text": "The value is 285.82\u0e2a\u0e37"}
{"id": "line_1578", "truth_parsed_k": 292.98, "prediction_parsed_k_with_c_conv": 284.99, "prediction_parsed_k_strict": 284.99, "absolute_error_k_vs_c_conv_pred": 7.990000000000009, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.990000000000009, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.98", "prediction_raw_text": "The value is 284.99<|fim_suffix|>"}
{"id": "line_1579", "truth_parsed_k": 293.35, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 8.379999999999995, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.379999999999995, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.35", "prediction_raw_text": "The value is 284.97\u0e25\u0e49\u0e2d"}
{"id": "line_1580", "truth_parsed_k": 293.14, "prediction_parsed_k_with_c_conv": 285.97, "prediction_parsed_k_strict": 285.97, "absolute_error_k_vs_c_conv_pred": 7.169999999999959, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.169999999999959, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.14", "prediction_raw_text": "The value is 285.97\u0643\u064f\u0645\u0652"}
{"id": "line_1581", "truth_parsed_k": 292.47, "prediction_parsed_k_with_c_conv": 285.61, "prediction_parsed_k_strict": 285.61, "absolute_error_k_vs_c_conv_pred": 6.860000000000014, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.860000000000014, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.47", "prediction_raw_text": "The value is 285.61\ufa2d"}
{"id": "line_1582", "truth_parsed_k": 291.58, "prediction_parsed_k_with_c_conv": 286.69, "prediction_parsed_k_strict": 286.69, "absolute_error_k_vs_c_conv_pred": 4.889999999999986, "score_vs_c_conv_pred": 0.584265047798882, "absolute_error_k_vs_strict_pred": 4.889999999999986, "score_vs_pred_strict": 0.584265047798882, "truth_raw_assistant_text": "The value is 291.58", "prediction_raw_text": "The value is 286.69\u0e40\u0e2d\u0e32\u0e44\u0e27\u0e49"}
{"id": "line_1583", "truth_parsed_k": 290.79, "prediction_parsed_k_with_c_conv": 285.8, "prediction_parsed_k_strict": 285.8, "absolute_error_k_vs_c_conv_pred": 4.990000000000009, "score_vs_c_conv_pred": 0.052594256575544396, "absolute_error_k_vs_strict_pred": 4.990000000000009, "score_vs_pred_strict": 0.052594256575544396, "truth_raw_assistant_text": "The value is 290.79", "prediction_raw_text": "The value is 285.80"}
{"id": "line_1584", "truth_parsed_k": 290.22, "prediction_parsed_k_with_c_conv": 286.5, "prediction_parsed_k_strict": 286.5, "absolute_error_k_vs_c_conv_pred": 3.7200000000000273, "score_vs_c_conv_pred": 7.738019268382712, "absolute_error_k_vs_strict_pred": 3.7200000000000273, "score_vs_pred_strict": 7.738019268382712, "truth_raw_assistant_text": "The value is 290.22", "prediction_raw_text": "The value is 286.50"}
{"id": "line_1585", "truth_parsed_k": 290.11, "prediction_parsed_k_with_c_conv": 285.38, "prediction_parsed_k_strict": 285.38, "absolute_error_k_vs_c_conv_pred": 4.730000000000018, "score_vs_c_conv_pred": 1.457395260117067, "absolute_error_k_vs_strict_pred": 4.730000000000018, "score_vs_pred_strict": 1.457395260117067, "truth_raw_assistant_text": "The value is 290.11", "prediction_raw_text": "The value is 285.38\u0e40\u0e17\u0e04\u0e19\u0e34\u0e04"}
{"id": "line_1586", "truth_parsed_k": 290.26, "prediction_parsed_k_with_c_conv": 284.89, "prediction_parsed_k_strict": 284.89, "absolute_error_k_vs_c_conv_pred": 5.3700000000000045, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.3700000000000045, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.26", "prediction_raw_text": "The value is 284.89."}
{"id": "line_1587", "truth_parsed_k": 290.85, "prediction_parsed_k_with_c_conv": 285.78, "prediction_parsed_k_strict": 285.78, "absolute_error_k_vs_c_conv_pred": 5.07000000000005, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.07000000000005, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.85", "prediction_raw_text": "The value is 285.78\u0e1e\u0e34\u0e18\u0e35"}
{"id": "line_1588", "truth_parsed_k": 291.63, "prediction_parsed_k_with_c_conv": 285.81, "prediction_parsed_k_strict": 285.81, "absolute_error_k_vs_c_conv_pred": 5.819999999999993, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.819999999999993, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.63", "prediction_raw_text": "The value is 285.81\u1f71"}
{"id": "line_1589", "truth_parsed_k": 292.42, "prediction_parsed_k_with_c_conv": 285.89, "prediction_parsed_k_strict": 285.89, "absolute_error_k_vs_c_conv_pred": 6.53000000000003, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.53000000000003, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.42", "prediction_raw_text": "The value is 285.89\u0e1a\u0e38\u0e04"}
{"id": "line_1590", "truth_parsed_k": 293.2, "prediction_parsed_k_with_c_conv": 286.37, "prediction_parsed_k_strict": 286.37, "absolute_error_k_vs_c_conv_pred": 6.829999999999984, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.829999999999984, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.20", "prediction_raw_text": "The value is 286.37\u0e40\u0e17\u0e48\u0e32\u0e44"}
{"id": "line_1591", "truth_parsed_k": 293.63, "prediction_parsed_k_with_c_conv": 285.68, "prediction_parsed_k_strict": 285.68, "absolute_error_k_vs_c_conv_pred": 7.949999999999989, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.949999999999989, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.63", "prediction_raw_text": "The value is 285.68\u0e2a\u0e21\u0e40\u0e14\u0e47"}
{"id": "line_1592", "truth_parsed_k": 293.54, "prediction_parsed_k_with_c_conv": 284.56, "prediction_parsed_k_strict": 284.56, "absolute_error_k_vs_c_conv_pred": 8.980000000000018, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.980000000000018, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.54", "prediction_raw_text": "The value is 284.56\u0e04\u0e34"}
{"id": "line_1593", "truth_parsed_k": 292.82, "prediction_parsed_k_with_c_conv": 284.78, "prediction_parsed_k_strict": 284.78, "absolute_error_k_vs_c_conv_pred": 8.04000000000002, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.04000000000002, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.82", "prediction_raw_text": "The value is 284.78\u0e20\u0e31"}
{"id": "line_1594", "truth_parsed_k": 291.85, "prediction_parsed_k_with_c_conv": 286.46, "prediction_parsed_k_strict": 286.46, "absolute_error_k_vs_c_conv_pred": 5.390000000000043, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.390000000000043, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.85", "prediction_raw_text": "The value is 286.46."}
{"id": "line_1595", "truth_parsed_k": 291.02, "prediction_parsed_k_with_c_conv": 284.88, "prediction_parsed_k_strict": 284.88, "absolute_error_k_vs_c_conv_pred": 6.139999999999986, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.139999999999986, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.02", "prediction_raw_text": "The value is 284.88\u0e40\u0e19\u0e37\u0e49\u0e2d\u0e2b\u0e32"}
{"id": "line_1596", "truth_parsed_k": 290.63, "prediction_parsed_k_with_c_conv": 286.67, "prediction_parsed_k_strict": 286.67, "absolute_error_k_vs_c_conv_pred": 3.9599999999999795, "score_vs_c_conv_pred": 6.1075641629811495, "absolute_error_k_vs_strict_pred": 3.9599999999999795, "score_vs_pred_strict": 6.1075641629811495, "truth_raw_assistant_text": "The value is 290.63", "prediction_raw_text": "The value is 286.67"}
{"id": "line_1597", "truth_parsed_k": 290.28, "prediction_parsed_k_with_c_conv": 284.72, "prediction_parsed_k_strict": 284.72, "absolute_error_k_vs_c_conv_pred": 5.559999999999945, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.559999999999945, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.28", "prediction_raw_text": "The value is 284.72"}
{"id": "line_1598", "truth_parsed_k": 290.37, "prediction_parsed_k_with_c_conv": 285.77, "prediction_parsed_k_strict": 285.77, "absolute_error_k_vs_c_conv_pred": 4.600000000000023, "score_vs_c_conv_pred": 2.1882694779875256, "absolute_error_k_vs_strict_pred": 4.600000000000023, "score_vs_pred_strict": 2.1882694779875256, "truth_raw_assistant_text": "The value is 290.37", "prediction_raw_text": "The value is 285.77\u0e23\u0e49\u0e32\u0e19\u0e04\u0e49\u0e32"}
{"id": "line_1599", "truth_parsed_k": 290.93, "prediction_parsed_k_with_c_conv": 285.78, "prediction_parsed_k_strict": 285.78, "absolute_error_k_vs_c_conv_pred": 5.150000000000034, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.150000000000034, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.93", "prediction_raw_text": "The value is 285.78"}
{"id": "line_1600", "truth_parsed_k": 291.77, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 6.829999999999984, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.829999999999984, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.77", "prediction_raw_text": "The value is 284.94."}
{"id": "line_1601", "truth_parsed_k": 292.66, "prediction_parsed_k_with_c_conv": 284.5, "prediction_parsed_k_strict": 284.5, "absolute_error_k_vs_c_conv_pred": 8.160000000000025, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.160000000000025, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.66", "prediction_raw_text": "The value is 284.50\ufa5b"}
{"id": "line_1602", "truth_parsed_k": 293.41, "prediction_parsed_k_with_c_conv": 284.77, "prediction_parsed_k_strict": 284.77, "absolute_error_k_vs_c_conv_pred": 8.640000000000043, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.640000000000043, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.41", "prediction_raw_text": "The value is 284.77\u0e1b\u0e23\u0e36\u0e01"}
{"id": "line_1603", "truth_parsed_k": 293.82, "prediction_parsed_k_with_c_conv": 285.54, "prediction_parsed_k_strict": 285.54, "absolute_error_k_vs_c_conv_pred": 8.279999999999973, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.279999999999973, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.82", "prediction_raw_text": "The value is 285.54."}
{"id": "line_1604", "truth_parsed_k": 293.54, "prediction_parsed_k_with_c_conv": 285.37, "prediction_parsed_k_strict": 285.37, "absolute_error_k_vs_c_conv_pred": 8.170000000000016, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.170000000000016, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.54", "prediction_raw_text": "The value is 285.37rawid\u0142ow"}
{"id": "line_1605", "truth_parsed_k": 292.78, "prediction_parsed_k_with_c_conv": 285.77, "prediction_parsed_k_strict": 285.77, "absolute_error_k_vs_c_conv_pred": 7.009999999999991, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.009999999999991, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.78", "prediction_raw_text": "The value is 285.77\u0e2a\u0e21\u0e31"}
{"id": "line_1606", "truth_parsed_k": 292.04, "prediction_parsed_k_with_c_conv": 285.8, "prediction_parsed_k_strict": 285.8, "absolute_error_k_vs_c_conv_pred": 6.240000000000009, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.240000000000009, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.04", "prediction_raw_text": "The value is 285.80\ufb4e"}
{"id": "line_1607", "truth_parsed_k": 291.32, "prediction_parsed_k_with_c_conv": 285.94, "prediction_parsed_k_strict": 285.94, "absolute_error_k_vs_c_conv_pred": 5.3799999999999955, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.3799999999999955, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.32", "prediction_raw_text": "The value is 285.94"}
{"id": "line_1608", "truth_parsed_k": 290.77, "prediction_parsed_k_with_c_conv": 285.74, "prediction_parsed_k_strict": 285.74, "absolute_error_k_vs_c_conv_pred": 5.029999999999973, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.029999999999973, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.77", "prediction_raw_text": "The value is 285.74."}
{"id": "line_1609", "truth_parsed_k": 290.6, "prediction_parsed_k_with_c_conv": 284.43, "prediction_parsed_k_strict": 284.43, "absolute_error_k_vs_c_conv_pred": 6.170000000000016, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.170000000000016, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.60", "prediction_raw_text": "The value is 284.43\u0e2b\u0e19\u0e35"}
{"id": "line_1610", "truth_parsed_k": 290.73, "prediction_parsed_k_with_c_conv": 286.64, "prediction_parsed_k_strict": 286.64, "absolute_error_k_vs_c_conv_pred": 4.090000000000032, "score_vs_c_conv_pred": 5.2639596070672035, "absolute_error_k_vs_strict_pred": 4.090000000000032, "score_vs_pred_strict": 5.2639596070672035, "truth_raw_assistant_text": "The value is 290.73", "prediction_raw_text": "The value is 286.64\u0e01\u0e32\u0e23\u0e25\u0e07\u0e17\u0e38\u0e19"}
{"id": "line_1611", "truth_parsed_k": 291.25, "prediction_parsed_k_with_c_conv": 285.71, "prediction_parsed_k_strict": 285.71, "absolute_error_k_vs_c_conv_pred": 5.5400000000000205, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.5400000000000205, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.25", "prediction_raw_text": "The value is 285.71"}
{"id": "line_1612", "truth_parsed_k": 291.89, "prediction_parsed_k_with_c_conv": 285.5, "prediction_parsed_k_strict": 285.5, "absolute_error_k_vs_c_conv_pred": 6.389999999999986, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.389999999999986, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.89", "prediction_raw_text": "The value is 285.50\u0e17\u0e35\u0e48\u0e40\u0e23\u0e32"}
{"id": "line_1613", "truth_parsed_k": 292.78, "prediction_parsed_k_with_c_conv": 285.69, "prediction_parsed_k_strict": 285.69, "absolute_error_k_vs_c_conv_pred": 7.089999999999975, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.089999999999975, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.78", "prediction_raw_text": "The value is 285.69"}
{"id": "line_1614", "truth_parsed_k": 293.57, "prediction_parsed_k_with_c_conv": 285.55, "prediction_parsed_k_strict": 285.55, "absolute_error_k_vs_c_conv_pred": 8.019999999999982, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.019999999999982, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.57", "prediction_raw_text": "The value is 285.55\u05e9\u05c1"}
{"id": "line_1615", "truth_parsed_k": 293.97, "prediction_parsed_k_with_c_conv": 284.73, "prediction_parsed_k_strict": 284.73, "absolute_error_k_vs_c_conv_pred": 9.240000000000009, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 9.240000000000009, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.97", "prediction_raw_text": "The value is 284.73\u0e21\u0e39\u0e25\u0e04\u0e48\u0e32"}
{"id": "line_1616", "truth_parsed_k": 293.63, "prediction_parsed_k_with_c_conv": 286.22, "prediction_parsed_k_strict": 286.22, "absolute_error_k_vs_c_conv_pred": 7.409999999999968, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.409999999999968, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.63", "prediction_raw_text": "The value is 286.22\uf941"}
{"id": "line_1617", "truth_parsed_k": 292.87, "prediction_parsed_k_with_c_conv": 284.78, "prediction_parsed_k_strict": 284.78, "absolute_error_k_vs_c_conv_pred": 8.090000000000032, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.090000000000032, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.87", "prediction_raw_text": "The value is 284.78\u0e01\u0e49\u0e32\u0e27"}
{"id": "line_1618", "truth_parsed_k": 292.18, "prediction_parsed_k_with_c_conv": 284.75, "prediction_parsed_k_strict": 284.75, "absolute_error_k_vs_c_conv_pred": 7.430000000000007, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.430000000000007, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.18", "prediction_raw_text": "The value is 284.75"}
{"id": "line_1619", "truth_parsed_k": 291.18, "prediction_parsed_k_with_c_conv": 285.88, "prediction_parsed_k_strict": 285.88, "absolute_error_k_vs_c_conv_pred": 5.300000000000011, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.300000000000011, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.18", "prediction_raw_text": "The value is 285.88"}
{"id": "line_1620", "truth_parsed_k": 290.78, "prediction_parsed_k_with_c_conv": 285.91, "prediction_parsed_k_strict": 285.91, "absolute_error_k_vs_c_conv_pred": 4.869999999999948, "score_vs_c_conv_pred": 0.6918705888147136, "absolute_error_k_vs_strict_pred": 4.869999999999948, "score_vs_pred_strict": 0.6918705888147136, "truth_raw_assistant_text": "The value is 290.78", "prediction_raw_text": "The value is 285.91\u0e2d\u0e48\u0e32\u0e19"}
{"id": "line_1621", "truth_parsed_k": 290.45, "prediction_parsed_k_with_c_conv": 284.72, "prediction_parsed_k_strict": 284.72, "absolute_error_k_vs_c_conv_pred": 5.729999999999961, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.729999999999961, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.45", "prediction_raw_text": "The value is 284.72\ufa68"}
{"id": "line_1622", "truth_parsed_k": 290.73, "prediction_parsed_k_with_c_conv": 284.59, "prediction_parsed_k_strict": 284.59, "absolute_error_k_vs_c_conv_pred": 6.140000000000043, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.140000000000043, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.73", "prediction_raw_text": "The value is 284.59\u0e04\u0e25\u0e49\u0e32\u0e22"}
{"id": "line_1623", "truth_parsed_k": 291.19, "prediction_parsed_k_with_c_conv": 284.99, "prediction_parsed_k_strict": 284.99, "absolute_error_k_vs_c_conv_pred": 6.199999999999989, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.199999999999989, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.19", "prediction_raw_text": "The value is 284.99\u0e17\u0e35\u0e48\u0e14\u0e35"}
{"id": "line_1624", "truth_parsed_k": 291.85, "prediction_parsed_k_with_c_conv": 288.27, "prediction_parsed_k_strict": 288.27, "absolute_error_k_vs_c_conv_pred": 3.580000000000041, "score_vs_c_conv_pred": 8.736797359754721, "absolute_error_k_vs_strict_pred": 3.580000000000041, "score_vs_pred_strict": 8.736797359754721, "truth_raw_assistant_text": "The value is 291.85", "prediction_raw_text": "The value is 288.27."}
{"id": "line_1625", "truth_parsed_k": 292.72, "prediction_parsed_k_with_c_conv": 286.27, "prediction_parsed_k_strict": 286.27, "absolute_error_k_vs_c_conv_pred": 6.4500000000000455, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.4500000000000455, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.72", "prediction_raw_text": "The value is 286.27\u0e1b\u0e23\u0e30\u0e27\u0e31\u0e15\u0e34"}
{"id": "line_1626", "truth_parsed_k": 293.46, "prediction_parsed_k_with_c_conv": 284.91, "prediction_parsed_k_strict": 284.91, "absolute_error_k_vs_c_conv_pred": 8.549999999999955, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.549999999999955, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.46", "prediction_raw_text": "The value is 284.91."}
{"id": "line_1627", "truth_parsed_k": 293.73, "prediction_parsed_k_with_c_conv": 285.95, "prediction_parsed_k_strict": 285.95, "absolute_error_k_vs_c_conv_pred": 7.78000000000003, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.78000000000003, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.73", "prediction_raw_text": "The value is 285.95\u0e2d\u0e31\u0e19\u0e14\u0e31\u0e1a"}
{"id": "line_1628", "truth_parsed_k": 293.55, "prediction_parsed_k_with_c_conv": 286.61, "prediction_parsed_k_strict": 286.61, "absolute_error_k_vs_c_conv_pred": 6.939999999999998, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.939999999999998, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.55", "prediction_raw_text": "The value is 286.61\u0e25\u0e49\u0e33"}
{"id": "line_1629", "truth_parsed_k": 292.77, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 7.949999999999989, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.949999999999989, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.77", "prediction_raw_text": "The value is 284.82\u0642\u064e"}
{"id": "line_1630", "truth_parsed_k": 291.88, "prediction_parsed_k_with_c_conv": 285.46, "prediction_parsed_k_strict": 285.46, "absolute_error_k_vs_c_conv_pred": 6.420000000000016, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.420000000000016, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.88", "prediction_raw_text": "The value is 285.46\u0e2b\u0e19\u0e48\u0e27\u0e22\u0e07\u0e32\u0e19"}
{"id": "line_1631", "truth_parsed_k": 291.04, "prediction_parsed_k_with_c_conv": 284.96, "prediction_parsed_k_strict": 284.96, "absolute_error_k_vs_c_conv_pred": 6.080000000000041, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.080000000000041, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.04", "prediction_raw_text": "The value is 284.96\u0631\u064e"}
{"id": "line_1632", "truth_parsed_k": 290.42, "prediction_parsed_k_with_c_conv": 286.57, "prediction_parsed_k_strict": 286.57, "absolute_error_k_vs_c_conv_pred": 3.8500000000000227, "score_vs_c_conv_pred": 6.842624092217575, "absolute_error_k_vs_strict_pred": 3.8500000000000227, "score_vs_pred_strict": 6.842624092217575, "truth_raw_assistant_text": "The value is 290.42", "prediction_raw_text": "The value is 286.57."}
{"id": "line_1633", "truth_parsed_k": 290.29, "prediction_parsed_k_with_c_conv": 287.32, "prediction_parsed_k_strict": 287.32, "absolute_error_k_vs_c_conv_pred": 2.9700000000000273, "score_vs_c_conv_pred": 13.58106453378981, "absolute_error_k_vs_strict_pred": 2.9700000000000273, "score_vs_pred_strict": 13.58106453378981, "truth_raw_assistant_text": "The value is 290.29", "prediction_raw_text": "The value is 287.32\u0e0b\u0e38"}
{"id": "line_1634", "truth_parsed_k": 290.52, "prediction_parsed_k_with_c_conv": 285.95, "prediction_parsed_k_strict": 285.95, "absolute_error_k_vs_c_conv_pred": 4.569999999999993, "score_vs_c_conv_pred": 2.359787578917749, "absolute_error_k_vs_strict_pred": 4.569999999999993, "score_vs_pred_strict": 2.359787578917749, "truth_raw_assistant_text": "The value is 290.52", "prediction_raw_text": "The value is 285.95"}
{"id": "line_1635", "truth_parsed_k": 290.84, "prediction_parsed_k_with_c_conv": 284.66, "prediction_parsed_k_strict": 284.66, "absolute_error_k_vs_c_conv_pred": 6.17999999999995, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.17999999999995, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.84", "prediction_raw_text": "The value is 284.66."}
{"id": "line_1636", "truth_parsed_k": 291.79, "prediction_parsed_k_with_c_conv": 285.87, "prediction_parsed_k_strict": 285.87, "absolute_error_k_vs_c_conv_pred": 5.920000000000016, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.920000000000016, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.79", "prediction_raw_text": "The value is 285.87."}
{"id": "line_1637", "truth_parsed_k": 292.61, "prediction_parsed_k_with_c_conv": 284.91, "prediction_parsed_k_strict": 284.91, "absolute_error_k_vs_c_conv_pred": 7.699999999999989, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.699999999999989, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.61", "prediction_raw_text": "The value is 284.91\u0e40\u0e21\u0e37\u0e48\u0e2d\u0e27\u0e31\u0e19\u0e17\u0e35\u0e48"}
{"id": "line_1638", "truth_parsed_k": 293.41, "prediction_parsed_k_with_c_conv": 284.64575999675, "prediction_parsed_k_strict": 284.64575999675, "absolute_error_k_vs_c_conv_pred": 8.764240003250052, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.764240003250052, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.41", "prediction_raw_text": "The value is 284.64575999675."}
{"id": "line_1639", "truth_parsed_k": 293.7, "prediction_parsed_k_with_c_conv": 288.5, "prediction_parsed_k_strict": 288.5, "absolute_error_k_vs_c_conv_pred": 5.199999999999989, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.199999999999989, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.70", "prediction_raw_text": "The value is 288.50\u0e40\u0e02\u0e49\u0e32\u0e44\u0e1b"}
{"id": "line_1640", "truth_parsed_k": 293.55, "prediction_parsed_k_with_c_conv": 285.94, "prediction_parsed_k_strict": 285.94, "absolute_error_k_vs_c_conv_pred": 7.610000000000014, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.610000000000014, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.55", "prediction_raw_text": "The value is 285.94\u0e04\u0e49\u0e19"}
{"id": "line_1641", "truth_parsed_k": 292.75, "prediction_parsed_k_with_c_conv": 284.78, "prediction_parsed_k_strict": 284.78, "absolute_error_k_vs_c_conv_pred": 7.970000000000027, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.970000000000027, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.75", "prediction_raw_text": "The value is 284.78\u0e0b\u0e49\u0e32\u0e22"}
{"id": "line_1642", "truth_parsed_k": 291.89, "prediction_parsed_k_with_c_conv": 284.75, "prediction_parsed_k_strict": 284.75, "absolute_error_k_vs_c_conv_pred": 7.139999999999986, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.139999999999986, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.89", "prediction_raw_text": "The value is 284.75"}
{"id": "line_1643", "truth_parsed_k": 290.97, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 6.0, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.0, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.97", "prediction_raw_text": "The value is 284.97\u0e40\u0e23\u0e35\u0e22\u0e1a\u0e23"}
{"id": "line_1644", "truth_parsed_k": 290.53, "prediction_parsed_k_with_c_conv": 285.8, "prediction_parsed_k_strict": 285.8, "absolute_error_k_vs_c_conv_pred": 4.729999999999961, "score_vs_c_conv_pred": 1.4573952601173779, "absolute_error_k_vs_strict_pred": 4.729999999999961, "score_vs_pred_strict": 1.4573952601173779, "truth_raw_assistant_text": "The value is 290.53", "prediction_raw_text": "The value is 285.80\u0e44\u0e21\u0e48\u0e21\u0e35"}
{"id": "line_1645", "truth_parsed_k": 290.24, "prediction_parsed_k_with_c_conv": 284.89, "prediction_parsed_k_strict": 284.89, "absolute_error_k_vs_c_conv_pred": 5.350000000000023, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.350000000000023, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.24", "prediction_raw_text": "The value is 284.89\u0e40\u0e2b\u0e25\u0e37\u0e2d"}
{"id": "line_1646", "truth_parsed_k": 290.6, "prediction_parsed_k_with_c_conv": 284.87, "prediction_parsed_k_strict": 284.87, "absolute_error_k_vs_c_conv_pred": 5.730000000000018, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.730000000000018, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.60", "prediction_raw_text": "The value is 284.87\u0e40\u0e04\u0e23\u0e37\u0e48\u0e2d\u0e07\u0e21\u0e37\u0e2d"}
{"id": "line_1647", "truth_parsed_k": 290.98, "prediction_parsed_k_with_c_conv": 284.86, "prediction_parsed_k_strict": 284.86, "absolute_error_k_vs_c_conv_pred": 6.1200000000000045, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.1200000000000045, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.98", "prediction_raw_text": "The value is 284.86\u0e2d\u0e19\u0e38"}
{"id": "line_1648", "truth_parsed_k": 291.64, "prediction_parsed_k_with_c_conv": 285.95, "prediction_parsed_k_strict": 285.95, "absolute_error_k_vs_c_conv_pred": 5.689999999999998, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.689999999999998, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.64", "prediction_raw_text": "The value is 285.95."}
{"id": "line_1649", "truth_parsed_k": 292.64, "prediction_parsed_k_with_c_conv": 285.8, "prediction_parsed_k_strict": 285.8, "absolute_error_k_vs_c_conv_pred": 6.839999999999975, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.839999999999975, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.64", "prediction_raw_text": "The value is 285.80\u0e19\u0e34\u0e27"}
{"id": "line_1650", "truth_parsed_k": 293.52, "prediction_parsed_k_with_c_conv": 285.52, "prediction_parsed_k_strict": 285.52, "absolute_error_k_vs_c_conv_pred": 8.0, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.0, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.52", "prediction_raw_text": "The value is 285.52 \u064a\u064e"}
{"id": "line_1651", "truth_parsed_k": 293.87, "prediction_parsed_k_with_c_conv": 285.72, "prediction_parsed_k_strict": 285.72, "absolute_error_k_vs_c_conv_pred": 8.149999999999977, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.149999999999977, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.87", "prediction_raw_text": "The value is 285.72\u0e40\u0e22\u0e35\u0e48\u0e22"}
{"id": "line_1652", "truth_parsed_k": 293.68, "prediction_parsed_k_with_c_conv": 285.66, "prediction_parsed_k_strict": 285.66, "absolute_error_k_vs_c_conv_pred": 8.019999999999982, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.019999999999982, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.68", "prediction_raw_text": "The value is 285.66."}
{"id": "line_1653", "truth_parsed_k": 293.13, "prediction_parsed_k_with_c_conv": 287.73, "prediction_parsed_k_strict": 287.73, "absolute_error_k_vs_c_conv_pred": 5.399999999999977, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.399999999999977, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.13", "prediction_raw_text": "The value is 287.73."}
{"id": "line_1654", "truth_parsed_k": 292.13, "prediction_parsed_k_with_c_conv": 284.77, "prediction_parsed_k_strict": 284.77, "absolute_error_k_vs_c_conv_pred": 7.360000000000014, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.360000000000014, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.13", "prediction_raw_text": "The value is 284.77\u05e9\u05c1"}
{"id": "line_1655", "truth_parsed_k": 291.34, "prediction_parsed_k_with_c_conv": 284.64, "prediction_parsed_k_strict": 284.64, "absolute_error_k_vs_c_conv_pred": 6.699999999999989, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.699999999999989, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.34", "prediction_raw_text": "The value is 284.64."}
{"id": "line_1656", "truth_parsed_k": 290.77, "prediction_parsed_k_with_c_conv": 286.87, "prediction_parsed_k_strict": 286.87, "absolute_error_k_vs_c_conv_pred": 3.8999999999999773, "score_vs_c_conv_pred": 6.50601657418135, "absolute_error_k_vs_strict_pred": 3.8999999999999773, "score_vs_pred_strict": 6.50601657418135, "truth_raw_assistant_text": "The value is 290.77", "prediction_raw_text": "The value is 286.87"}
{"id": "line_1657", "truth_parsed_k": 290.5, "prediction_parsed_k_with_c_conv": 285.62, "prediction_parsed_k_strict": 285.62, "absolute_error_k_vs_c_conv_pred": 4.8799999999999955, "score_vs_c_conv_pred": 0.638014069249393, "absolute_error_k_vs_strict_pred": 4.8799999999999955, "score_vs_pred_strict": 0.638014069249393, "truth_raw_assistant_text": "The value is 290.50", "prediction_raw_text": "The value is 285.62 \u0641\u0650\u064a"}
{"id": "line_1658", "truth_parsed_k": 290.78, "prediction_parsed_k_with_c_conv": 286.88, "prediction_parsed_k_strict": 286.88, "absolute_error_k_vs_c_conv_pred": 3.8999999999999773, "score_vs_c_conv_pred": 6.50601657418135, "absolute_error_k_vs_strict_pred": 3.8999999999999773, "score_vs_pred_strict": 6.50601657418135, "truth_raw_assistant_text": "The value is 290.78", "prediction_raw_text": "The value is 286.88\uf9dc"}
{"id": "line_1659", "truth_parsed_k": 291.25, "prediction_parsed_k_with_c_conv": 287.75, "prediction_parsed_k_strict": 287.75, "absolute_error_k_vs_c_conv_pred": 3.5, "score_vs_c_conv_pred": 9.324613350647704, "absolute_error_k_vs_strict_pred": 3.5, "score_vs_pred_strict": 9.324613350647704, "truth_raw_assistant_text": "The value is 291.25", "prediction_raw_text": "The value is 287.75\u0e21\u0e31\u0e49\u0e22"}
{"id": "line_1660", "truth_parsed_k": 292.05, "prediction_parsed_k_with_c_conv": 285.6, "prediction_parsed_k_strict": 285.6, "absolute_error_k_vs_c_conv_pred": 6.449999999999989, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.449999999999989, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.05", "prediction_raw_text": "The value is 285.60\u0e40\u0e07\u0e37\u0e48\u0e2d\u0e19"}
{"id": "line_1661", "truth_parsed_k": 293.0, "prediction_parsed_k_with_c_conv": 286.67, "prediction_parsed_k_strict": 286.67, "absolute_error_k_vs_c_conv_pred": 6.329999999999984, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.329999999999984, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.00", "prediction_raw_text": "The value is 286.67."}
{"id": "line_1662", "truth_parsed_k": 293.71, "prediction_parsed_k_with_c_conv": 286.4, "prediction_parsed_k_strict": 286.4, "absolute_error_k_vs_c_conv_pred": 7.310000000000002, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.310000000000002, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.71", "prediction_raw_text": "The value is 286.40\u0e04\u0e25\u0e49\u0e32\u0e22"}
{"id": "line_1663", "truth_parsed_k": 294.04, "prediction_parsed_k_with_c_conv": 285.94, "prediction_parsed_k_strict": 285.94, "absolute_error_k_vs_c_conv_pred": 8.100000000000023, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.100000000000023, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 294.04", "prediction_raw_text": "The value is 285.94\u0e23\u0e31\u0e01\u0e29\u0e32"}
{"id": "line_1664", "truth_parsed_k": 293.85, "prediction_parsed_k_with_c_conv": 286.28, "prediction_parsed_k_strict": 286.28, "absolute_error_k_vs_c_conv_pred": 7.57000000000005, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.57000000000005, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.85", "prediction_raw_text": "The value is 286.28."}
{"id": "line_1665", "truth_parsed_k": 293.22, "prediction_parsed_k_with_c_conv": 285.47, "prediction_parsed_k_strict": 285.47, "absolute_error_k_vs_c_conv_pred": 7.75, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.75, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.22", "prediction_raw_text": "The value is 285.47\u0e17\u0e35\u0e48\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16"}
{"id": "line_1666", "truth_parsed_k": 292.38, "prediction_parsed_k_with_c_conv": 285.33, "prediction_parsed_k_strict": 285.33, "absolute_error_k_vs_c_conv_pred": 7.050000000000011, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.050000000000011, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.38", "prediction_raw_text": "The value is 285.33\u062c\u064e"}
{"id": "line_1667", "truth_parsed_k": 291.62, "prediction_parsed_k_with_c_conv": 285.56, "prediction_parsed_k_strict": 285.56, "absolute_error_k_vs_c_conv_pred": 6.060000000000002, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.060000000000002, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.62", "prediction_raw_text": "The value is 285.56\u0e08\u0e38\u0e14"}
{"id": "line_1668", "truth_parsed_k": 290.96, "prediction_parsed_k_with_c_conv": 287.54, "prediction_parsed_k_strict": 287.54, "absolute_error_k_vs_c_conv_pred": 3.419999999999959, "score_vs_c_conv_pred": 9.92554765114625, "absolute_error_k_vs_strict_pred": 3.419999999999959, "score_vs_pred_strict": 9.92554765114625, "truth_raw_assistant_text": "The value is 290.96", "prediction_raw_text": "The value is 287.54."}
{"id": "line_1669", "truth_parsed_k": 290.72, "prediction_parsed_k_with_c_conv": 285.86, "prediction_parsed_k_strict": 285.86, "absolute_error_k_vs_c_conv_pred": 4.860000000000014, "score_vs_c_conv_pred": 0.7458350373484968, "absolute_error_k_vs_strict_pred": 4.860000000000014, "score_vs_pred_strict": 0.7458350373484968, "truth_raw_assistant_text": "The value is 290.72", "prediction_raw_text": "The value is 285.86\u0e32\u0e23\u0e4c\u0e14"}
{"id": "line_1670", "truth_parsed_k": 290.8, "prediction_parsed_k_with_c_conv": 285.94, "prediction_parsed_k_strict": 285.94, "absolute_error_k_vs_c_conv_pred": 4.860000000000014, "score_vs_c_conv_pred": 0.7458350373484968, "absolute_error_k_vs_strict_pred": 4.860000000000014, "score_vs_pred_strict": 0.7458350373484968, "truth_raw_assistant_text": "The value is 290.80", "prediction_raw_text": "The value is 285.94\u0e01\u0e48\u0e2d\u0e2a\u0e23\u0e49\u0e32\u0e07"}
{"id": "line_1671", "truth_parsed_k": 291.11, "prediction_parsed_k_with_c_conv": 284.86, "prediction_parsed_k_strict": 284.86, "absolute_error_k_vs_c_conv_pred": 6.25, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.25, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.11", "prediction_raw_text": "The value is 284.86"}
{"id": "line_1672", "truth_parsed_k": 292.02, "prediction_parsed_k_with_c_conv": 284.62, "prediction_parsed_k_strict": 284.62, "absolute_error_k_vs_c_conv_pred": 7.399999999999977, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.399999999999977, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.02", "prediction_raw_text": "The value is 284.62."}
{"id": "line_1673", "truth_parsed_k": 292.87, "prediction_parsed_k_with_c_conv": 285.88, "prediction_parsed_k_strict": 285.88, "absolute_error_k_vs_c_conv_pred": 6.990000000000009, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.990000000000009, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.87", "prediction_raw_text": "The value is 285.88\uf9fd"}
{"id": "line_1674", "truth_parsed_k": 293.71, "prediction_parsed_k_with_c_conv": 285.69, "prediction_parsed_k_strict": 285.69, "absolute_error_k_vs_c_conv_pred": 8.019999999999982, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.019999999999982, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.71", "prediction_raw_text": "The value is 285.69\u0e08\u0e49\u0e32\u0e07"}
{"id": "line_1675", "truth_parsed_k": 293.99, "prediction_parsed_k_with_c_conv": 285.37, "prediction_parsed_k_strict": 285.37, "absolute_error_k_vs_c_conv_pred": 8.620000000000005, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.620000000000005, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.99", "prediction_raw_text": "The value is 285.37\u0e19\u0e34\u0e22\u0e21"}
{"id": "line_1676", "truth_parsed_k": 293.81, "prediction_parsed_k_with_c_conv": 285.85, "prediction_parsed_k_strict": 285.85, "absolute_error_k_vs_c_conv_pred": 7.9599999999999795, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.9599999999999795, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.81", "prediction_raw_text": "The value is 285.85"}
{"id": "line_1677", "truth_parsed_k": 293.18, "prediction_parsed_k_with_c_conv": 284.84, "prediction_parsed_k_strict": 284.84, "absolute_error_k_vs_c_conv_pred": 8.340000000000032, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.340000000000032, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.18", "prediction_raw_text": "The value is 284.84."}
{"id": "line_1678", "truth_parsed_k": 292.32, "prediction_parsed_k_with_c_conv": 284.7, "prediction_parsed_k_strict": 284.7, "absolute_error_k_vs_c_conv_pred": 7.6200000000000045, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.6200000000000045, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.32", "prediction_raw_text": "The value is 284.70"}
{"id": "line_1679", "truth_parsed_k": 291.44, "prediction_parsed_k_with_c_conv": 284.79, "prediction_parsed_k_strict": 284.79, "absolute_error_k_vs_c_conv_pred": 6.649999999999977, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.649999999999977, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.44", "prediction_raw_text": "The value is 284.79\u0e02\u0e31\u0e49\u0e19\u0e15\u0e2d\u0e19"}
{"id": "line_1680", "truth_parsed_k": 290.92, "prediction_parsed_k_with_c_conv": 285.67, "prediction_parsed_k_strict": 285.67, "absolute_error_k_vs_c_conv_pred": 5.25, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.25, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.92", "prediction_raw_text": "The value is 285.67\u0e27\u0e31\u0e22"}
{"id": "line_1681", "truth_parsed_k": 290.57, "prediction_parsed_k_with_c_conv": 285.87, "prediction_parsed_k_strict": 285.87, "absolute_error_k_vs_c_conv_pred": 4.699999999999989, "score_vs_c_conv_pred": 1.6243064400001561, "absolute_error_k_vs_strict_pred": 4.699999999999989, "score_vs_pred_strict": 1.6243064400001561, "truth_raw_assistant_text": "The value is 290.57", "prediction_raw_text": "The value is 285.87\u064a\u0627\u064b"}
{"id": "line_1682", "truth_parsed_k": 290.81, "prediction_parsed_k_with_c_conv": 285.43, "prediction_parsed_k_strict": 285.43, "absolute_error_k_vs_c_conv_pred": 5.3799999999999955, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.3799999999999955, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.81", "prediction_raw_text": "The value is 285.43\u0e02\u0e49\u0e32"}
{"id": "line_1683", "truth_parsed_k": 291.38, "prediction_parsed_k_with_c_conv": 285.5, "prediction_parsed_k_strict": 285.5, "absolute_error_k_vs_c_conv_pred": 5.8799999999999955, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.8799999999999955, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.38", "prediction_raw_text": "The value is 285.50\u0e25\u0e31\u0e01\u0e29\u0e13\u0e30"}
{"id": "line_1684", "truth_parsed_k": 292.05, "prediction_parsed_k_with_c_conv": 285.7, "prediction_parsed_k_strict": 285.7, "absolute_error_k_vs_c_conv_pred": 6.350000000000023, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.350000000000023, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.05", "prediction_raw_text": "The value is 285.70\u0e14\u0e38"}
{"id": "line_1685", "truth_parsed_k": 292.89, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 8.069999999999993, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.069999999999993, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.89", "prediction_raw_text": "The value is 284.82\u0e20\u0e32\u0e22\u0e43\u0e15\u0e49"}
{"id": "line_1686", "truth_parsed_k": 293.72, "prediction_parsed_k_with_c_conv": 286.57, "prediction_parsed_k_strict": 286.57, "absolute_error_k_vs_c_conv_pred": 7.150000000000034, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.150000000000034, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.72", "prediction_raw_text": "The value is 286.57."}
{"id": "line_1687", "truth_parsed_k": 294.09, "prediction_parsed_k_with_c_conv": 284.89, "prediction_parsed_k_strict": 284.89, "absolute_error_k_vs_c_conv_pred": 9.199999999999989, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 9.199999999999989, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 294.09", "prediction_raw_text": "The value is 284.89\u0e2a\u0e19\u0e31\u0e1a"}
{"id": "line_1688", "truth_parsed_k": 293.84, "prediction_parsed_k_with_c_conv": 285.95, "prediction_parsed_k_strict": 285.95, "absolute_error_k_vs_c_conv_pred": 7.889999999999986, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.889999999999986, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.84", "prediction_raw_text": "The value is 285.95."}
{"id": "line_1689", "truth_parsed_k": 293.05, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 8.230000000000018, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.230000000000018, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.05", "prediction_raw_text": "The value is 284.82\u0e19\u0e49\u0e2d\u0e22"}
{"id": "line_1690", "truth_parsed_k": 292.2, "prediction_parsed_k_with_c_conv": 284.77, "prediction_parsed_k_strict": 284.77, "absolute_error_k_vs_c_conv_pred": 7.430000000000007, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.430000000000007, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.20", "prediction_raw_text": "The value is 284.77"}
{"id": "line_1691", "truth_parsed_k": 291.3, "prediction_parsed_k_with_c_conv": 284.94114799447, "prediction_parsed_k_strict": 284.94114799447, "absolute_error_k_vs_c_conv_pred": 6.358852005530025, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.358852005530025, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.30", "prediction_raw_text": "The value is 284.94114799447."}
{"id": "line_1692", "truth_parsed_k": 290.65, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 5.829999999999984, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.829999999999984, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.65", "prediction_raw_text": "The value is 284.82\u0e2d\u0e35\u0e01\u0e14\u0e49\u0e27\u0e22"}
{"id": "line_1693", "truth_parsed_k": 290.41, "prediction_parsed_k_with_c_conv": 284.98, "prediction_parsed_k_strict": 284.98, "absolute_error_k_vs_c_conv_pred": 5.430000000000007, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.430000000000007, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.41", "prediction_raw_text": "The value is 284.98\u0e04\u0e14\u0e35"}
{"id": "line_1694", "truth_parsed_k": 290.84, "prediction_parsed_k_with_c_conv": 285.43, "prediction_parsed_k_strict": 285.43, "absolute_error_k_vs_c_conv_pred": 5.409999999999968, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.409999999999968, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.84", "prediction_raw_text": "The value is 285.43\u0e40\u0e1e\u0e35\u0e22\u0e07\u0e41\u0e04\u0e48"}
{"id": "line_1695", "truth_parsed_k": 291.29, "prediction_parsed_k_with_c_conv": 285.35, "prediction_parsed_k_strict": 285.35, "absolute_error_k_vs_c_conv_pred": 5.939999999999998, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.939999999999998, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.29", "prediction_raw_text": "The value is 285.35"}
{"id": "line_1696", "truth_parsed_k": 292.06, "prediction_parsed_k_with_c_conv": 285.7, "prediction_parsed_k_strict": 285.7, "absolute_error_k_vs_c_conv_pred": 6.360000000000014, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.360000000000014, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.06", "prediction_raw_text": "The value is 285.70\u0e08\u0e31\u0e14\u0e01\u0e32\u0e23"}
{"id": "line_1697", "truth_parsed_k": 293.09, "prediction_parsed_k_with_c_conv": 284.81, "prediction_parsed_k_strict": 284.81, "absolute_error_k_vs_c_conv_pred": 8.279999999999973, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.279999999999973, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.09", "prediction_raw_text": "The value is 284.81\u0e40\u0e01\u0e37\u0e2d"}
{"id": "line_1698", "truth_parsed_k": 293.75, "prediction_parsed_k_with_c_conv": 284.84, "prediction_parsed_k_strict": 284.84, "absolute_error_k_vs_c_conv_pred": 8.910000000000025, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.910000000000025, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.75", "prediction_raw_text": "The value is 284.84\u0e2d\u0e22\u0e39\u0e48\u0e43\u0e19"}
{"id": "line_1699", "truth_parsed_k": 294.14, "prediction_parsed_k_with_c_conv": 287.99, "prediction_parsed_k_strict": 287.99, "absolute_error_k_vs_c_conv_pred": 6.149999999999977, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.149999999999977, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 294.14", "prediction_raw_text": "The value is 287.99\u0e1e\u0e23\u0e30\u0e1e\u0e38\u0e17\u0e18"}
{"id": "line_1700", "truth_parsed_k": 293.75, "prediction_parsed_k_with_c_conv": 286.59, "prediction_parsed_k_strict": 286.59, "absolute_error_k_vs_c_conv_pred": 7.160000000000025, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.160000000000025, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.75", "prediction_raw_text": "The value is 286.59\ufa08"}
{"id": "line_1701", "truth_parsed_k": 293.16, "prediction_parsed_k_with_c_conv": 285.62, "prediction_parsed_k_strict": 285.62, "absolute_error_k_vs_c_conv_pred": 7.5400000000000205, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.5400000000000205, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.16", "prediction_raw_text": "The value is 285.62\u0e01\u0e25\u0e34\u0e48\u0e19"}
{"id": "line_1702", "truth_parsed_k": 292.34, "prediction_parsed_k_with_c_conv": 284.87, "prediction_parsed_k_strict": 284.87, "absolute_error_k_vs_c_conv_pred": 7.46999999999997, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.46999999999997, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.34", "prediction_raw_text": "The value is 284.87"}
{"id": "line_1703", "truth_parsed_k": 291.43, "prediction_parsed_k_with_c_conv": 285.86, "prediction_parsed_k_strict": 285.86, "absolute_error_k_vs_c_conv_pred": 5.569999999999993, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.569999999999993, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.43", "prediction_raw_text": "The value is 285.86\u0e2a\u0e21\u0e1a\u0e39\u0e23\u0e13\u0e4c"}
{"id": "line_1704", "truth_parsed_k": 290.9, "prediction_parsed_k_with_c_conv": 284.67, "prediction_parsed_k_strict": 284.67, "absolute_error_k_vs_c_conv_pred": 6.229999999999961, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.229999999999961, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.90", "prediction_raw_text": "The value is 284.67"}
{"id": "line_1705", "truth_parsed_k": 290.53, "prediction_parsed_k_with_c_conv": 285.6, "prediction_parsed_k_strict": 285.6, "absolute_error_k_vs_c_conv_pred": 4.92999999999995, "score_vs_c_conv_pred": 0.37033541574035267, "absolute_error_k_vs_strict_pred": 4.92999999999995, "score_vs_pred_strict": 0.37033541574035267, "truth_raw_assistant_text": "The value is 290.53", "prediction_raw_text": "The value is 285.60"}
{"id": "line_1706", "truth_parsed_k": 290.87, "prediction_parsed_k_with_c_conv": 285.95, "prediction_parsed_k_strict": 285.95, "absolute_error_k_vs_c_conv_pred": 4.920000000000016, "score_vs_c_conv_pred": 0.42365870208784795, "absolute_error_k_vs_strict_pred": 4.920000000000016, "score_vs_pred_strict": 0.42365870208784795, "truth_raw_assistant_text": "The value is 290.87", "prediction_raw_text": "The value is 285.95"}
{"id": "line_1707", "truth_parsed_k": 291.26, "prediction_parsed_k_with_c_conv": 286.65, "prediction_parsed_k_strict": 286.65, "absolute_error_k_vs_c_conv_pred": 4.610000000000014, "score_vs_c_conv_pred": 2.131338693978002, "absolute_error_k_vs_strict_pred": 4.610000000000014, "score_vs_pred_strict": 2.131338693978002, "truth_raw_assistant_text": "The value is 291.26", "prediction_raw_text": "The value is 286.65."}
{"id": "line_1708", "truth_parsed_k": 292.03, "prediction_parsed_k_with_c_conv": 285.86, "prediction_parsed_k_strict": 285.86, "absolute_error_k_vs_c_conv_pred": 6.169999999999959, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.169999999999959, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.03", "prediction_raw_text": "The value is 285.86\uf9c9"}
{"id": "line_1709", "truth_parsed_k": 292.91, "prediction_parsed_k_with_c_conv": 284.67, "prediction_parsed_k_strict": 284.67, "absolute_error_k_vs_c_conv_pred": 8.240000000000009, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.240000000000009, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.91", "prediction_raw_text": "The value is 284.67"}
{"id": "line_1710", "truth_parsed_k": 293.71, "prediction_parsed_k_with_c_conv": 286.99, "prediction_parsed_k_strict": 286.99, "absolute_error_k_vs_c_conv_pred": 6.71999999999997, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.71999999999997, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.71", "prediction_raw_text": "The value is 286.99\u0e2d\u0e31\u0e1e"}
{"id": "line_1711", "truth_parsed_k": 294.14, "prediction_parsed_k_with_c_conv": 285.54, "prediction_parsed_k_strict": 285.54, "absolute_error_k_vs_c_conv_pred": 8.599999999999966, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.599999999999966, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 294.14", "prediction_raw_text": "The value is 285.54."}
{"id": "line_1712", "truth_parsed_k": 293.95, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 8.979999999999961, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.979999999999961, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.95", "prediction_raw_text": "The value is 284.97"}
{"id": "line_1713", "truth_parsed_k": 293.23, "prediction_parsed_k_with_c_conv": 284.95, "prediction_parsed_k_strict": 284.95, "absolute_error_k_vs_c_conv_pred": 8.28000000000003, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.28000000000003, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.23", "prediction_raw_text": "The value is 284.95\u0e40\u0e23\u0e37\u0e2d\u0e19"}
{"id": "line_1714", "truth_parsed_k": 292.28, "prediction_parsed_k_with_c_conv": 284.93, "prediction_parsed_k_strict": 284.93, "absolute_error_k_vs_c_conv_pred": 7.349999999999966, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.349999999999966, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.28", "prediction_raw_text": "The value is 284.93\u0e08\u0e33\u0e40\u0e1b\u0e47\u0e19\u0e15\u0e49\u0e2d\u0e07"}
{"id": "line_1715", "truth_parsed_k": 291.52, "prediction_parsed_k_with_c_conv": 284.74, "prediction_parsed_k_strict": 284.74, "absolute_error_k_vs_c_conv_pred": 6.779999999999973, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.779999999999973, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.52", "prediction_raw_text": "The value is 284.74\u0e40\u0e14\u0e34\u0e19\u0e17\u0e32\u0e07"}
{"id": "line_1716", "truth_parsed_k": 290.96, "prediction_parsed_k_with_c_conv": 285.42, "prediction_parsed_k_strict": 285.42, "absolute_error_k_vs_c_conv_pred": 5.539999999999964, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.539999999999964, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.96", "prediction_raw_text": "The value is 285.42\u0e27\u0e48\u0e32\u0e40\u0e1b\u0e47\u0e19"}
{"id": "line_1717", "truth_parsed_k": 290.59, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 5.619999999999948, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.619999999999948, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.59", "prediction_raw_text": "The value is 284.97\u0e14\u0e35\u0e01\u0e27\u0e48\u0e32"}
{"id": "line_1718", "truth_parsed_k": 290.81, "prediction_parsed_k_with_c_conv": 285.9, "prediction_parsed_k_strict": 285.9, "absolute_error_k_vs_c_conv_pred": 4.910000000000025, "score_vs_c_conv_pred": 0.4770877886763336, "absolute_error_k_vs_strict_pred": 4.910000000000025, "score_vs_pred_strict": 0.4770877886763336, "truth_raw_assistant_text": "The value is 290.81", "prediction_raw_text": "The value is 285.90\uf952"}
{"id": "line_1719", "truth_parsed_k": 291.31, "prediction_parsed_k_with_c_conv": 284.86, "prediction_parsed_k_strict": 284.86, "absolute_error_k_vs_c_conv_pred": 6.449999999999989, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.449999999999989, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.31", "prediction_raw_text": "The value is 284.86\u0e43\u0e19\u0e40\u0e23\u0e37\u0e48\u0e2d\u0e07"}
{"id": "line_1720", "truth_parsed_k": 292.04, "prediction_parsed_k_with_c_conv": 284.74, "prediction_parsed_k_strict": 284.74, "absolute_error_k_vs_c_conv_pred": 7.300000000000011, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.300000000000011, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.04", "prediction_raw_text": "The value is 284.74 \u0e1c\u0e39\u0e49"}
{"id": "line_1721", "truth_parsed_k": 292.95, "prediction_parsed_k_with_c_conv": 285.86, "prediction_parsed_k_strict": 285.86, "absolute_error_k_vs_c_conv_pred": 7.089999999999975, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.089999999999975, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.95", "prediction_raw_text": "The value is 285.86\ufa26"}
{"id": "line_1722", "truth_parsed_k": 293.86, "prediction_parsed_k_with_c_conv": 284.81, "prediction_parsed_k_strict": 284.81, "absolute_error_k_vs_c_conv_pred": 9.050000000000011, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 9.050000000000011, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.86", "prediction_raw_text": "The value is 284.81"}
{"id": "line_1723", "truth_parsed_k": 294.3, "prediction_parsed_k_with_c_conv": 285.45, "prediction_parsed_k_strict": 285.45, "absolute_error_k_vs_c_conv_pred": 8.850000000000023, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.850000000000023, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 294.30", "prediction_raw_text": "The value is 285.45."}
{"id": "line_1724", "truth_parsed_k": 294.1, "prediction_parsed_k_with_c_conv": 284.79, "prediction_parsed_k_strict": 284.79, "absolute_error_k_vs_c_conv_pred": 9.310000000000002, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 9.310000000000002, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 294.10", "prediction_raw_text": "The value is 284.79\u0e2d\u0e48\u0e32\u0e19"}
{"id": "line_1725", "truth_parsed_k": 293.49, "prediction_parsed_k_with_c_conv": 284.56, "prediction_parsed_k_strict": 284.56, "absolute_error_k_vs_c_conv_pred": 8.930000000000007, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.930000000000007, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.49", "prediction_raw_text": "The value is 284.56\u0e07\u0e48\u0e32\u0e22\u0e46"}
{"id": "line_1726", "truth_parsed_k": 292.59, "prediction_parsed_k_with_c_conv": 286.41, "prediction_parsed_k_strict": 286.41, "absolute_error_k_vs_c_conv_pred": 6.17999999999995, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.17999999999995, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.59", "prediction_raw_text": "The value is 286.41\u2126"}
{"id": "line_1727", "truth_parsed_k": 291.64, "prediction_parsed_k_with_c_conv": 286.08, "prediction_parsed_k_strict": 286.08, "absolute_error_k_vs_c_conv_pred": 5.560000000000002, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.560000000000002, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.64", "prediction_raw_text": "The value is 286.08"}
{"id": "line_1728", "truth_parsed_k": 290.99, "prediction_parsed_k_with_c_conv": 285.24, "prediction_parsed_k_strict": 285.24, "absolute_error_k_vs_c_conv_pred": 5.75, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.75, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.99", "prediction_raw_text": "The value is 285.24\u0e04\u0e31\u0e19"}
{"id": "line_1729", "truth_parsed_k": 290.93, "prediction_parsed_k_with_c_conv": 285.6, "prediction_parsed_k_strict": 285.6, "absolute_error_k_vs_c_conv_pred": 5.329999999999984, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.329999999999984, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.93", "prediction_raw_text": "The value is 285.60\u0e1a\u0e38\u0e0d"}
{"id": "line_1730", "truth_parsed_k": 290.83, "prediction_parsed_k_with_c_conv": 284.96, "prediction_parsed_k_strict": 284.96, "absolute_error_k_vs_c_conv_pred": 5.8700000000000045, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.8700000000000045, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.83", "prediction_raw_text": "The value is 284.96"}
{"id": "line_1731", "truth_parsed_k": 291.45, "prediction_parsed_k_with_c_conv": 286.42, "prediction_parsed_k_strict": 286.42, "absolute_error_k_vs_c_conv_pred": 5.029999999999973, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.029999999999973, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.45", "prediction_raw_text": "The value is 286.42\u0e17\u0e49\u0e2d\u0e07\u0e16\u0e34\u0e48\u0e19"}
{"id": "line_1732", "truth_parsed_k": 292.19, "prediction_parsed_k_with_c_conv": 285.48, "prediction_parsed_k_strict": 285.48, "absolute_error_k_vs_c_conv_pred": 6.7099999999999795, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.7099999999999795, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.19", "prediction_raw_text": "The value is 285.48\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19\u0e41\u0e1b\u0e25\u0e07"}
{"id": "line_1733", "truth_parsed_k": 292.96, "prediction_parsed_k_with_c_conv": 284.58, "prediction_parsed_k_strict": 284.58, "absolute_error_k_vs_c_conv_pred": 8.379999999999995, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.379999999999995, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.96", "prediction_raw_text": "The value is 284.58"}
{"id": "line_1734", "truth_parsed_k": 293.78, "prediction_parsed_k_with_c_conv": 285.5, "prediction_parsed_k_strict": 285.5, "absolute_error_k_vs_c_conv_pred": 8.279999999999973, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.279999999999973, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.78", "prediction_raw_text": "The value is 285.50."}
{"id": "line_1735", "truth_parsed_k": 294.27, "prediction_parsed_k_with_c_conv": 285.35, "prediction_parsed_k_strict": 285.35, "absolute_error_k_vs_c_conv_pred": 8.919999999999959, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.919999999999959, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 294.27", "prediction_raw_text": "The value is 285.35\u0e22\u0e2d\u0e14\u0e40\u0e22\u0e35\u0e48\u0e22\u0e21"}
{"id": "line_1736", "truth_parsed_k": 293.98, "prediction_parsed_k_with_c_conv": 284.63, "prediction_parsed_k_strict": 284.63, "absolute_error_k_vs_c_conv_pred": 9.350000000000023, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 9.350000000000023, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.98", "prediction_raw_text": "The value is 284.63"}
{"id": "line_1737", "truth_parsed_k": 293.34, "prediction_parsed_k_with_c_conv": 285.98, "prediction_parsed_k_strict": 285.98, "absolute_error_k_vs_c_conv_pred": 7.359999999999957, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.359999999999957, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.34", "prediction_raw_text": "The value is 285.98\u0e04\u0e38\u0e13\u0e2a\u0e21\u0e1a\u0e31\u0e15\u0e34"}
{"id": "line_1738", "truth_parsed_k": 292.33, "prediction_parsed_k_with_c_conv": 284.76, "prediction_parsed_k_strict": 284.76, "absolute_error_k_vs_c_conv_pred": 7.569999999999993, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.569999999999993, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.33", "prediction_raw_text": "The value is 284.76\ufb34"}
{"id": "line_1739", "truth_parsed_k": 291.53, "prediction_parsed_k_with_c_conv": 284.97, "prediction_parsed_k_strict": 284.97, "absolute_error_k_vs_c_conv_pred": 6.559999999999945, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.559999999999945, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.53", "prediction_raw_text": "The value is 284.97\u0e1f\u0e2d\u0e23\u0e4c\u0e21"}
{"id": "line_1740", "truth_parsed_k": 290.9, "prediction_parsed_k_with_c_conv": 284.78, "prediction_parsed_k_strict": 284.78, "absolute_error_k_vs_c_conv_pred": 6.1200000000000045, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.1200000000000045, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.90", "prediction_raw_text": "The value is 284.78\u0e0a\u0e34"}
{"id": "line_1741", "truth_parsed_k": 290.81, "prediction_parsed_k_with_c_conv": 285.66, "prediction_parsed_k_strict": 285.66, "absolute_error_k_vs_c_conv_pred": 5.149999999999977, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.149999999999977, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.81", "prediction_raw_text": "The value is 285.66\u0e04\u0e27\u0e49\u0e32"}
{"id": "line_1742", "truth_parsed_k": 290.91, "prediction_parsed_k_with_c_conv": 284.73, "prediction_parsed_k_strict": 284.73, "absolute_error_k_vs_c_conv_pred": 6.180000000000007, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.180000000000007, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.91", "prediction_raw_text": "The value is 284.73."}
{"id": "line_1743", "truth_parsed_k": 291.61, "prediction_parsed_k_with_c_conv": 286.66, "prediction_parsed_k_strict": 286.66, "absolute_error_k_vs_c_conv_pred": 4.949999999999989, "score_vs_c_conv_pred": 0.26400457350821416, "absolute_error_k_vs_strict_pred": 4.949999999999989, "score_vs_pred_strict": 0.26400457350821416, "truth_raw_assistant_text": "The value is 291.61", "prediction_raw_text": "The value is 286.66\u0e0a\u0e34"}
{"id": "line_1744", "truth_parsed_k": 292.25, "prediction_parsed_k_with_c_conv": 284.83, "prediction_parsed_k_strict": 284.83, "absolute_error_k_vs_c_conv_pred": 7.420000000000016, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.420000000000016, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.25", "prediction_raw_text": "The value is 284.83\u0e14\u0e49\u0e32\u0e19\u0e25\u0e48\u0e32\u0e07"}
{"id": "line_1745", "truth_parsed_k": 293.04, "prediction_parsed_k_with_c_conv": 284.84, "prediction_parsed_k_strict": 284.84, "absolute_error_k_vs_c_conv_pred": 8.200000000000045, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.200000000000045, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.04", "prediction_raw_text": "The value is 284.84\u0e40\u0e04\u0e25\u0e47\u0e14"}
{"id": "line_1746", "truth_parsed_k": 293.96, "prediction_parsed_k_with_c_conv": 285.98, "prediction_parsed_k_strict": 285.98, "absolute_error_k_vs_c_conv_pred": 7.979999999999961, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.979999999999961, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.96", "prediction_raw_text": "The value is 285.98"}
{"id": "line_1747", "truth_parsed_k": 294.24, "prediction_parsed_k_with_c_conv": 284.75, "prediction_parsed_k_strict": 284.75, "absolute_error_k_vs_c_conv_pred": 9.490000000000009, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 9.490000000000009, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 294.24", "prediction_raw_text": "The value is 284.75"}
{"id": "line_1748", "truth_parsed_k": 294.1, "prediction_parsed_k_with_c_conv": 285.74, "prediction_parsed_k_strict": 285.74, "absolute_error_k_vs_c_conv_pred": 8.360000000000014, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.360000000000014, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 294.10", "prediction_raw_text": "The value is 285.74\u0e40\u0e15\u0e23\u0e35\u0e22\u0e21"}
{"id": "line_1749", "truth_parsed_k": 293.31, "prediction_parsed_k_with_c_conv": 284.85, "prediction_parsed_k_strict": 284.85, "absolute_error_k_vs_c_conv_pred": 8.45999999999998, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.45999999999998, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.31", "prediction_raw_text": "The value is 284.85"}
{"id": "line_1750", "truth_parsed_k": 292.4, "prediction_parsed_k_with_c_conv": 284.76, "prediction_parsed_k_strict": 284.76, "absolute_error_k_vs_c_conv_pred": 7.639999999999986, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.639999999999986, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.40", "prediction_raw_text": "The value is 284.76\u0e1e\u0e37\u0e49\u0e19\u0e10\u0e32\u0e19"}
{"id": "line_1751", "truth_parsed_k": 291.7, "prediction_parsed_k_with_c_conv": 284.82, "prediction_parsed_k_strict": 284.82, "absolute_error_k_vs_c_conv_pred": 6.8799999999999955, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.8799999999999955, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.70", "prediction_raw_text": "The value is 284.82\u0e40\u0e23\u0e37\u0e48\u0e2d\u0e22\u0e46"}
{"id": "line_1752", "truth_parsed_k": 291.15, "prediction_parsed_k_with_c_conv": 286.68, "prediction_parsed_k_strict": 286.68, "absolute_error_k_vs_c_conv_pred": 4.46999999999997, "score_vs_c_conv_pred": 2.939535499945378, "absolute_error_k_vs_strict_pred": 4.46999999999997, "score_vs_pred_strict": 2.939535499945378, "truth_raw_assistant_text": "The value is 291.15", "prediction_raw_text": "The value is 286.68"}
{"id": "line_1753", "truth_parsed_k": 290.84, "prediction_parsed_k_with_c_conv": 285.89, "prediction_parsed_k_strict": 285.89, "absolute_error_k_vs_c_conv_pred": 4.949999999999989, "score_vs_c_conv_pred": 0.26400457350821416, "absolute_error_k_vs_strict_pred": 4.949999999999989, "score_vs_pred_strict": 0.26400457350821416, "truth_raw_assistant_text": "The value is 290.84", "prediction_raw_text": "The value is 285.89\u0e04\u0e14\u0e35"}
{"id": "line_1754", "truth_parsed_k": 291.11, "prediction_parsed_k_with_c_conv": 285.71, "prediction_parsed_k_strict": 285.71, "absolute_error_k_vs_c_conv_pred": 5.400000000000034, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.400000000000034, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.11", "prediction_raw_text": "The value is 285.71\u0e2a\u0e07\u0e2a\u0e31\u0e22"}
{"id": "line_1755", "truth_parsed_k": 291.42, "prediction_parsed_k_with_c_conv": 287.44, "prediction_parsed_k_strict": 287.44, "absolute_error_k_vs_c_conv_pred": 3.980000000000018, "score_vs_c_conv_pred": 5.976046193093754, "absolute_error_k_vs_strict_pred": 3.980000000000018, "score_vs_pred_strict": 5.976046193093754, "truth_raw_assistant_text": "The value is 291.42", "prediction_raw_text": "The value is 287.44\u0e22\u0e36\u0e14"}
{"id": "line_1756", "truth_parsed_k": 292.32, "prediction_parsed_k_with_c_conv": 284.87, "prediction_parsed_k_strict": 284.87, "absolute_error_k_vs_c_conv_pred": 7.449999999999989, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.449999999999989, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.32", "prediction_raw_text": "The value is 284.87\u0e04\u0e25\u0e34"}
{"id": "line_1757", "truth_parsed_k": 293.13, "prediction_parsed_k_with_c_conv": 285.99, "prediction_parsed_k_strict": 285.99, "absolute_error_k_vs_c_conv_pred": 7.139999999999986, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.139999999999986, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.13", "prediction_raw_text": "The value is 285.99\u0e40\u0e2a\u0e35\u0e22\u0e0a\u0e35\u0e27\u0e34\u0e15"}
{"id": "line_1758", "truth_parsed_k": 293.94, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 9.0, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 9.0, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.94", "prediction_raw_text": "The value is 284.94\u0e02\u0e27\u0e31"}
{"id": "line_1759", "truth_parsed_k": 294.37, "prediction_parsed_k_with_c_conv": 286.25, "prediction_parsed_k_strict": 286.25, "absolute_error_k_vs_c_conv_pred": 8.120000000000005, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.120000000000005, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 294.37", "prediction_raw_text": "The value is 286.25"}
{"id": "line_1760", "truth_parsed_k": 294.21, "prediction_parsed_k_with_c_conv": 285.7, "prediction_parsed_k_strict": 285.7, "absolute_error_k_vs_c_conv_pred": 8.509999999999991, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.509999999999991, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 294.21", "prediction_raw_text": "The value is 285.70\u0e01\u0e31\u0e07\u0e27\u0e25"}
{"id": "line_1761", "truth_parsed_k": 293.58, "prediction_parsed_k_with_c_conv": 284.62, "prediction_parsed_k_strict": 284.62, "absolute_error_k_vs_c_conv_pred": 8.95999999999998, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.95999999999998, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.58", "prediction_raw_text": "The value is 284.62"}
{"id": "line_1762", "truth_parsed_k": 292.78, "prediction_parsed_k_with_c_conv": 286.84, "prediction_parsed_k_strict": 286.84, "absolute_error_k_vs_c_conv_pred": 5.939999999999998, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.939999999999998, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.78", "prediction_raw_text": "The value is 286.84\u0625\u0650"}
{"id": "line_1763", "truth_parsed_k": 292.0, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 7.060000000000002, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.060000000000002, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.00", "prediction_raw_text": "The value is 284.94\u0e17\u0e48\u0e2d\u0e07"}
{"id": "line_1764", "truth_parsed_k": 291.42, "prediction_parsed_k_with_c_conv": 285.8, "prediction_parsed_k_strict": 285.8, "absolute_error_k_vs_c_conv_pred": 5.6200000000000045, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.6200000000000045, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.42", "prediction_raw_text": "The value is 285.80\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22"}
{"id": "line_1765", "truth_parsed_k": 291.1, "prediction_parsed_k_with_c_conv": 287.84, "prediction_parsed_k_strict": 287.84, "absolute_error_k_vs_c_conv_pred": 3.2600000000000477, "score_vs_c_conv_pred": 11.169208815929144, "absolute_error_k_vs_strict_pred": 3.2600000000000477, "score_vs_pred_strict": 11.169208815929144, "truth_raw_assistant_text": "The value is 291.10", "prediction_raw_text": "The value is 287.84\u45f4"}
{"id": "line_1766", "truth_parsed_k": 291.54, "prediction_parsed_k_with_c_conv": 284.86, "prediction_parsed_k_strict": 284.86, "absolute_error_k_vs_c_conv_pred": 6.680000000000007, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.680000000000007, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.54", "prediction_raw_text": "The value is 284.86\u0e2a\u0e31\u0e07\u0e40\u0e01\u0e15"}
{"id": "line_1767", "truth_parsed_k": 291.82, "prediction_parsed_k_with_c_conv": 285.37, "prediction_parsed_k_strict": 285.37, "absolute_error_k_vs_c_conv_pred": 6.449999999999989, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.449999999999989, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.82", "prediction_raw_text": "The value is 285.37\u0e20\u0e32\u0e29\u0e32\u0e2d\u0e31\u0e07\u0e01\u0e24\u0e29"}
{"id": "line_1768", "truth_parsed_k": 292.48, "prediction_parsed_k_with_c_conv": 287.44, "prediction_parsed_k_strict": 287.44, "absolute_error_k_vs_c_conv_pred": 5.0400000000000205, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.0400000000000205, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.48", "prediction_raw_text": "The value is 287.44."}
{"id": "line_1769", "truth_parsed_k": 293.44, "prediction_parsed_k_with_c_conv": 284.52, "prediction_parsed_k_strict": 284.52, "absolute_error_k_vs_c_conv_pred": 8.920000000000016, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.920000000000016, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.44", "prediction_raw_text": "The value is 284.52\u0e2d\u0e31\u0e1e"}
{"id": "line_1770", "truth_parsed_k": 294.15, "prediction_parsed_k_with_c_conv": 285.42, "prediction_parsed_k_strict": 285.42, "absolute_error_k_vs_c_conv_pred": 8.729999999999961, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.729999999999961, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 294.15", "prediction_raw_text": "The value is 285.42\u0e22\u0e31\u0e19"}
{"id": "line_1771", "truth_parsed_k": 294.52, "prediction_parsed_k_with_c_conv": 284.92, "prediction_parsed_k_strict": 284.92, "absolute_error_k_vs_c_conv_pred": 9.599999999999966, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 9.599999999999966, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 294.52", "prediction_raw_text": "The value is 284.92\u0e2b\u0e21\u0e39"}
{"id": "line_1772", "truth_parsed_k": 294.27, "prediction_parsed_k_with_c_conv": 285.5, "prediction_parsed_k_strict": 285.5, "absolute_error_k_vs_c_conv_pred": 8.769999999999982, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.769999999999982, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 294.27", "prediction_raw_text": "The value is 285.50\u0e1e\u0e31\u0e12"}
{"id": "line_1773", "truth_parsed_k": 293.47, "prediction_parsed_k_with_c_conv": 285.54, "prediction_parsed_k_strict": 285.54, "absolute_error_k_vs_c_conv_pred": 7.930000000000007, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.930000000000007, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.47", "prediction_raw_text": "The value is 285.54\u0e19\u0e31\u0e01\u0e17\u0e48\u0e2d\u0e07\u0e40\u0e17\u0e35\u0e48\u0e22\u0e27"}
{"id": "line_1774", "truth_parsed_k": 292.67, "prediction_parsed_k_with_c_conv": 285.86, "prediction_parsed_k_strict": 285.86, "absolute_error_k_vs_c_conv_pred": 6.810000000000002, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.810000000000002, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.67", "prediction_raw_text": "The value is 285.86\u0646\u064f"}
{"id": "line_1775", "truth_parsed_k": 291.77, "prediction_parsed_k_with_c_conv": 284.92, "prediction_parsed_k_strict": 284.92, "absolute_error_k_vs_c_conv_pred": 6.849999999999966, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.849999999999966, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.77", "prediction_raw_text": "The value is 284.92\uf9fe"}
{"id": "line_1776", "truth_parsed_k": 291.01, "prediction_parsed_k_with_c_conv": 284.88, "prediction_parsed_k_strict": 284.88, "absolute_error_k_vs_c_conv_pred": 6.1299999999999955, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.1299999999999955, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.01", "prediction_raw_text": "The value is 284.88\u0e27\u0e34\u0e14\u0e35\u0e42\u0e2d"}
{"id": "line_1777", "truth_parsed_k": 290.74, "prediction_parsed_k_with_c_conv": 287.52, "prediction_parsed_k_strict": 287.52, "absolute_error_k_vs_c_conv_pred": 3.2200000000000273, "score_vs_c_conv_pred": 11.489310663167618, "absolute_error_k_vs_strict_pred": 3.2200000000000273, "score_vs_pred_strict": 11.489310663167618, "truth_raw_assistant_text": "The value is 290.74", "prediction_raw_text": "The value is 287.52\u0e40\u0e17\u0e48\u0e32\u0e44"}
{"id": "line_1778", "truth_parsed_k": 290.85, "prediction_parsed_k_with_c_conv": 285.76, "prediction_parsed_k_strict": 285.76, "absolute_error_k_vs_c_conv_pred": 5.090000000000032, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.090000000000032, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.85", "prediction_raw_text": "The value is 285.76\u0e25\u0e30\u0e40\u0e2d\u0e35\u0e22\u0e14"}
{"id": "line_1779", "truth_parsed_k": 291.51, "prediction_parsed_k_with_c_conv": 284.85, "prediction_parsed_k_strict": 284.85, "absolute_error_k_vs_c_conv_pred": 6.659999999999968, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.659999999999968, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.51", "prediction_raw_text": "The value is 284.85."}
{"id": "line_1780", "truth_parsed_k": 292.17, "prediction_parsed_k_with_c_conv": 285.37, "prediction_parsed_k_strict": 285.37, "absolute_error_k_vs_c_conv_pred": 6.800000000000011, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.800000000000011, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.17", "prediction_raw_text": "The value is 285.37\u0e2a\u0e31\u0e21\u0e1c"}
{"id": "line_1781", "truth_parsed_k": 293.11, "prediction_parsed_k_with_c_conv": 285.8, "prediction_parsed_k_strict": 285.8, "absolute_error_k_vs_c_conv_pred": 7.310000000000002, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.310000000000002, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.11", "prediction_raw_text": "The value is 285.80\uf95f"}
{"id": "line_1782", "truth_parsed_k": 293.85, "prediction_parsed_k_with_c_conv": 285.6, "prediction_parsed_k_strict": 285.6, "absolute_error_k_vs_c_conv_pred": 8.25, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.25, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.85", "prediction_raw_text": "The value is 285.60"}
{"id": "line_1783", "truth_parsed_k": 294.14, "prediction_parsed_k_with_c_conv": 286.36, "prediction_parsed_k_strict": 286.36, "absolute_error_k_vs_c_conv_pred": 7.779999999999973, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.779999999999973, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 294.14", "prediction_raw_text": "The value is 286.36\ufa52"}
{"id": "line_1784", "truth_parsed_k": 293.93, "prediction_parsed_k_with_c_conv": 285.9, "prediction_parsed_k_strict": 285.9, "absolute_error_k_vs_c_conv_pred": 8.03000000000003, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.03000000000003, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.93", "prediction_raw_text": "The value is 285.90"}
{"id": "line_1785", "truth_parsed_k": 293.23, "prediction_parsed_k_with_c_conv": 285.43, "prediction_parsed_k_strict": 285.43, "absolute_error_k_vs_c_conv_pred": 7.800000000000011, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.800000000000011, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.23", "prediction_raw_text": "The value is 285.43\u0e02\u0e48\u0e32\u0e27"}
{"id": "line_1786", "truth_parsed_k": 292.44, "prediction_parsed_k_with_c_conv": 284.62, "prediction_parsed_k_strict": 284.62, "absolute_error_k_vs_c_conv_pred": 7.819999999999993, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.819999999999993, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.44", "prediction_raw_text": "The value is 284.62 \u0e2d\u0e22\u0e48\u0e32\u0e07\u0e44\u0e23\u0e01\u0e47\u0e15\u0e32\u0e21"}
{"id": "line_1787", "truth_parsed_k": 291.68, "prediction_parsed_k_with_c_conv": 285.72, "prediction_parsed_k_strict": 285.72, "absolute_error_k_vs_c_conv_pred": 5.9599999999999795, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.9599999999999795, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.68", "prediction_raw_text": "The value is 285.72\u0e44\u0e14\u0e49\u0e23\u0e31\u0e1a\u0e01\u0e32\u0e23"}
{"id": "line_1788", "truth_parsed_k": 291.16, "prediction_parsed_k_with_c_conv": 284.98, "prediction_parsed_k_strict": 284.98, "absolute_error_k_vs_c_conv_pred": 6.180000000000007, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.180000000000007, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.16", "prediction_raw_text": "The value is 284.98"}
{"id": "line_1789", "truth_parsed_k": 290.87, "prediction_parsed_k_with_c_conv": 284.58, "prediction_parsed_k_strict": 284.58, "absolute_error_k_vs_c_conv_pred": 6.2900000000000205, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.2900000000000205, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 290.87", "prediction_raw_text": "The value is 284.58\u0e41\u0e01\u0e48"}
{"id": "line_1790", "truth_parsed_k": 290.75, "prediction_parsed_k_with_c_conv": 285.9, "prediction_parsed_k_strict": 285.9, "absolute_error_k_vs_c_conv_pred": 4.850000000000023, "score_vs_c_conv_pred": 0.7999078483018063, "absolute_error_k_vs_strict_pred": 4.850000000000023, "score_vs_pred_strict": 0.7999078483018063, "truth_raw_assistant_text": "The value is 290.75", "prediction_raw_text": "The value is 285.90\u0e25\u0e48\u0e30"}
{"id": "line_1791", "truth_parsed_k": 291.48, "prediction_parsed_k_with_c_conv": 284.81, "prediction_parsed_k_strict": 284.81, "absolute_error_k_vs_c_conv_pred": 6.670000000000016, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.670000000000016, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.48", "prediction_raw_text": "The value is 284.81\u0e22\u0e34\u0e49\u0e21"}
{"id": "line_1792", "truth_parsed_k": 292.36, "prediction_parsed_k_with_c_conv": 285.97, "prediction_parsed_k_strict": 285.97, "absolute_error_k_vs_c_conv_pred": 6.389999999999986, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.389999999999986, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.36", "prediction_raw_text": "The value is 285.97\u0e2b\u0e19\u0e36\u0e48\u0e07"}
{"id": "line_1793", "truth_parsed_k": 293.36, "prediction_parsed_k_with_c_conv": 285.81, "prediction_parsed_k_strict": 285.81, "absolute_error_k_vs_c_conv_pred": 7.550000000000011, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.550000000000011, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.36", "prediction_raw_text": "The value is 285.81\u0e1e\u0e48\u0e2d"}
{"id": "line_1794", "truth_parsed_k": 293.9, "prediction_parsed_k_with_c_conv": 284.93, "prediction_parsed_k_strict": 284.93, "absolute_error_k_vs_c_conv_pred": 8.96999999999997, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 8.96999999999997, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.90", "prediction_raw_text": "The value is 284.93\u0e08\u0e30\u0e44\u0e21\u0e48"}
{"id": "line_1795", "truth_parsed_k": 294.35, "prediction_parsed_k_with_c_conv": 284.73, "prediction_parsed_k_strict": 284.73, "absolute_error_k_vs_c_conv_pred": 9.620000000000005, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 9.620000000000005, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 294.35", "prediction_raw_text": "The value is 284.73\u0e1b\u0e23\u0e30\u0e27\u0e31"}
{"id": "line_1796", "truth_parsed_k": 294.26, "prediction_parsed_k_with_c_conv": 284.92, "prediction_parsed_k_strict": 284.92, "absolute_error_k_vs_c_conv_pred": 9.339999999999975, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 9.339999999999975, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 294.26", "prediction_raw_text": "The value is 284.92\u0e1b\u0e23\u0e2a\u0e34\u0e15"}
{"id": "line_1797", "truth_parsed_k": 293.42, "prediction_parsed_k_with_c_conv": 285.96, "prediction_parsed_k_strict": 285.96, "absolute_error_k_vs_c_conv_pred": 7.460000000000036, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 7.460000000000036, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 293.42", "prediction_raw_text": "The value is 285.96\u0e1b\u0e23\u0e30\u0e15\u0e39"}
{"id": "line_1798", "truth_parsed_k": 292.47, "prediction_parsed_k_with_c_conv": 285.81, "prediction_parsed_k_strict": 285.81, "absolute_error_k_vs_c_conv_pred": 6.660000000000025, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.660000000000025, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 292.47", "prediction_raw_text": "The value is 285.81\u0e1e\u0e19\u0e31\u0e19"}
{"id": "line_1799", "truth_parsed_k": 291.76, "prediction_parsed_k_with_c_conv": 284.94, "prediction_parsed_k_strict": 284.94, "absolute_error_k_vs_c_conv_pred": 6.819999999999993, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 6.819999999999993, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.76", "prediction_raw_text": "The value is 284.94."}
{"id": "line_1800", "truth_parsed_k": 291.25, "prediction_parsed_k_with_c_conv": 285.79, "prediction_parsed_k_strict": 285.79, "absolute_error_k_vs_c_conv_pred": 5.4599999999999795, "score_vs_c_conv_pred": 0.0, "absolute_error_k_vs_strict_pred": 5.4599999999999795, "score_vs_pred_strict": 0.0, "truth_raw_assistant_text": "The value is 291.25", "prediction_raw_text": "The value is 285.79\u0e01\u0e25\u0e48\u0e2d\u0e07"}
